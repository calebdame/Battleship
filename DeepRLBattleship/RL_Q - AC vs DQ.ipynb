{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Coding-Battleship\" data-toc-modified-id=\"Coding-Battleship-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Coding Battleship</a></span></li><li><span><a href=\"#Callback-and-Plotting\" data-toc-modified-id=\"Callback-and-Plotting-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Callback and Plotting</a></span></li><li><span><a href=\"#Playing-with-One-Ship-on-a-5x5-board\" data-toc-modified-id=\"Playing-with-One-Ship-on-a-5x5-board-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Playing with One Ship on a 5x5 board</a></span></li><li><span><a href=\"#Playing-with-One-Ship-on-a-Bigger-Board\" data-toc-modified-id=\"Playing-with-One-Ship-on-a-Bigger-Board-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Playing with One Ship on a Bigger Board</a></span></li><li><span><a href=\"#Visualizing-How-the-Agent-Plays\" data-toc-modified-id=\"Visualizing-How-the-Agent-Plays-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Visualizing How the Agent Plays</a></span></li><li><span><a href=\"#Optimizing-The-Algorithm-Parameters-with-Hyperopt\" data-toc-modified-id=\"Optimizing-The-Algorithm-Parameters-with-Hyperopt-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Optimizing The Algorithm Parameters with Hyperopt</a></span></li><li><span><a href=\"#Links\" data-toc-modified-id=\"Links-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Links</a></span></li><li><span><a href=\"#Reward-scheme\" data-toc-modified-id=\"Reward-scheme-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Reward scheme</a></span></li><li><span><a href=\"#Skeleton-Battleship-Environmnt\" data-toc-modified-id=\"Skeleton-Battleship-Environmnt-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Skeleton Battleship Environmnt</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Battleship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from BattleshipStats import Battleship\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import DQN, A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from stable_baselines3 import DQN, A2C\n",
    "from stable_baselines3.ppo import PPO \n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import os\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BattleshipEnv(gym.Env):\n",
    "    \n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    \"\"\"see https://github.com/openai/gym/blob/master/gym/core.py\"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human']} \n",
    "\n",
    "\n",
    "    def __init__(self, battleship, rewards):\n",
    "        \n",
    "        super(BattleshipEnv, self).__init__()\n",
    "        self.rewards = rewards\n",
    "        self.battleship = battleship\n",
    "        # board size\n",
    "        self.dim = battleship.dim \n",
    "        # cell state encoding (empty, hit, miss)\n",
    "        self.cell = {'E': 0, 'X': 1, 'O': -1} \n",
    "        # boards, actions, rewards\n",
    "        self.board = self.cell['E']*np.ones((self.dim, self.dim), dtype='int')\n",
    "        # set enemy board\n",
    "        self.enemyBoard = self.battleship.randomBoardFlatIndeces()\n",
    "        print(self.enemyBoard)\n",
    "        self.hits = set()\n",
    "        self.misses = set()\n",
    "\n",
    "        # legal (empty) cells available for moves\n",
    "        self.legal_actions = list(range(self.dim*self.dim)) \n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # In our case the action space is discrete: index of action\n",
    "        self.action_space = spaces.Discrete(self.dim * self.dim)\n",
    "        # The observation will be the state or configuration of the board\n",
    "        self.observation_space = spaces.Box(low=-1, high=1,shape=(self.dim, self.dim), \n",
    "                                            dtype=\"float32\")\n",
    "\n",
    "    def step(self, action):       \n",
    "        #print(action)\n",
    "        temp = -1\n",
    "\n",
    "        # assign a penalty for each illegal action used instead of a legal one\n",
    "        if (action in self.misses or action in self.hits):\n",
    "\n",
    "            reward = self.rewards[0]\n",
    "            temp = action\n",
    "            action = np.random.choice(self.legal_actions)\n",
    "            return self.board, reward, False , {}\n",
    "        # set new state after performing action (scoring board is updated)\n",
    "        self.set_state(action)\n",
    "\n",
    "        # new state on scoring board - this includes last action\n",
    "        next_state = self.board\n",
    "\n",
    "        # game completed?\n",
    "        done = bool(len(self.hits) == np.sum(self.battleship.ships))\n",
    "        # reward for a hit\n",
    "        if action in self.enemyBoard and temp < 0: \n",
    "            reward = (self.rewards[1] if done else self.rewards[2])\n",
    "        elif temp < 0:\n",
    "            reward = self.rewards[3]\n",
    "      \n",
    "        reward = float(reward)\n",
    "        info = {}\n",
    "        \n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array) \n",
    "        \"\"\"\n",
    "        \n",
    "        self.board = self.cell['E']*np.ones((self.dim, self.dim), dtype='int')\n",
    "        self.hits = set()\n",
    "        self.misses = set()\n",
    "        \n",
    "        self.legal_actions = list(range(self.dim*self.dim)) \n",
    "               \n",
    "        # generate a random board again if it was set randomly before\n",
    "\n",
    "        self.enemyBoard = self.battleship.randomBoardFlatIndeces()\n",
    "\n",
    "        return self.board\n",
    "    \n",
    "    # Render the environment to the screen\n",
    "    # board (i,j)\n",
    "    ## ------------>j\n",
    "    ## | (0,0) | (0,1) | (0,2) | |\n",
    "    ## | (1,0) | (1,1) | (1,2) | |\n",
    "    ##                           v i\n",
    "    def render(self, mode='human'):\n",
    "        for i in range(self.dim):\n",
    "            print(\"-\"*(4*self.dim+2))\n",
    "            for j in range(self.dim):\n",
    "                current_state_value = self.board[i,j]\n",
    "                current_state = list(self.cell.keys())[list(self.cell.values()).index(current_state_value)]\n",
    "                current_state = (current_state if current_state!='E' else ' ')\n",
    "                print(\" | \", end=\"\")\n",
    "                print(current_state, end='')\n",
    "            print(' |')\n",
    "        print(\"-\"*(4*self.dim+2))\n",
    "        \n",
    "    \n",
    "    # set board configuration and state value after player action\n",
    "    def set_state(self, action):\n",
    "        if action in self.enemyBoard:\n",
    "            self.hits.add(action)\n",
    "            \n",
    "            self.board[int(np.floor(action/self.dim)), action % self.dim] = self.cell[\"X\"]\n",
    "            \n",
    "        else:\n",
    "            self.misses.add(action)\n",
    "            self.board[int(np.floor(action/self.dim)), action % self.dim] = self.cell[\"O\"]\n",
    "#         print(action)\n",
    "#         print(int(np.floor(action/self.dim)), action % self.dim)\n",
    "        self.legal_actions.remove(action)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{64, 32, 2, 40, 74, 43, 12, 44, 42, 16, 50, 53, 54, 22, 26, 60, 63}\n"
     ]
    }
   ],
   "source": [
    "dim = 10\n",
    "ships = [2,3,3,4,5]\n",
    "rewards = [-10,25,5,-1]\n",
    "\n",
    "battleship = Battleship(dim, ships, True)\n",
    "\n",
    "env = BattleshipEnv(battleship,rewards)\n",
    "\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This callback function is legacy and needs to be replaced with object oriented functions\n",
    "## to work with all policies. See next callback function\n",
    "\n",
    "def callback(_locals, _globals):\n",
    "    \"\"\"\n",
    "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "    :param _locals: (dict)\n",
    "    :param _globals: (dict)\n",
    "    \"\"\"\n",
    "    global n_steps, best_mean_reward\n",
    "    # Print stats every step_interval calls\n",
    "    if (n_steps + 1) % step_interval == 0:\n",
    "        # Evaluate policy training performance\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            # NOTE: when done is True, timesteps are counted and reported to the log_dir\n",
    "            mean_reward = np.mean(y[-episode_interval:]) # mean reward over previous episode_interval episodes\n",
    "            mean_moves = np.mean(np.diff(x[-episode_interval:])) # mean moves over previous episode_interval episodes\n",
    "            print(x[-1], 'timesteps') # closest to step_interval step number\n",
    "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f} - Last mean moves per episode: {:.2f}\".format(best_mean_reward, \n",
    "                                                                                           mean_reward, mean_moves))\n",
    "\n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                # Example for saving best model\n",
    "                print(\"Saving new best model\")\n",
    "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
    "    n_steps += 1\n",
    "    # Returning False will stop training early\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, episode_interval: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.episode_interval = episode_interval\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model.pkl')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Evaluate policy training performance\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # NOTE: when done is True, timesteps are counted and reported to the log_dir\n",
    "                mean_reward = np.mean(y[-self.episode_interval:]) # mean reward over previous episode_interval episodes\n",
    "                mean_moves = np.mean(np.diff(x[-self.episode_interval:])) # mean moves over previous 100 episodes\n",
    "                if self.verbose > 0:\n",
    "                    print(x[-1], 'timesteps') # closest to step_interval step number\n",
    "                    print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f} - Last mean moves per episode: {:.2f}\".format(self.best_mean_reward, \n",
    "                                                                                                   mean_reward, mean_moves))\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Saving new best model\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "\n",
    "def plot_results(log_folder, window = 100, title='Learning Curve'):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    y = moving_average(y, window=window)\n",
    "    y_moves = moving_average(np.diff(x), window = window) \n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y):]\n",
    "    x_moves = x[len(x) - len(y_moves):]\n",
    "\n",
    "    title = 'Smoothed Learning Curve of Rewards (every ' + str(window) +' steps)'\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    title = 'Smoothed Learning Curve of Moves (every ' + str(window) +' steps)'\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x_moves, y_moves)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Moves')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class CustomCNN(BaseFeaturesExtractor):\n",
    "#     \"\"\"\n",
    "#     :param observation_space: (gym.Space)\n",
    "#     :param features_dim: (int) Number of features extracted.\n",
    "#         This corresponds to the number of unit for the last layer.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "#         super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "#         # We assume CxHxW images (channels first)\n",
    "#         # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "#         self.layer1 = nn.Linear()\n",
    "#         print(self.layer1)\n",
    "        \n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Linear(10,200),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Linear(200,400),\n",
    "#             nn.Sigmoid(),\n",
    "#             #nn.Conv2d(1, 1, kernel_size=40, stride=4, padding=1),\n",
    "#             #nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "#         )\n",
    "#         #print(th.as_tensor(observation_space.sample()[None]).float())\n",
    "#         # Compute shape by doing one forward pass\n",
    "#         with th.no_grad():\n",
    "#             n_flatten = self.cnn(\n",
    "#                 th.as_tensor(observation_space.sample().reshape(1,dim,dim)[None]).float()\n",
    "#             ).shape[1]\n",
    "#         print(n_flatten)\n",
    "#         self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "#     def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "#         return self.linear(self.cnn(observations.reshape(1,1,dim,dim)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with One Ship {3} : 5x5 board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Battleship' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1d900e4ae319>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mbattleship\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBattleship\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mships\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBattleshipEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbattleship\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Battleship' is not defined"
     ]
    }
   ],
   "source": [
    "dim = 5\n",
    "ships = [3]\n",
    "rewards = [-10, 25, 5, -0.1]\n",
    "lr = 0.0001\n",
    "num_timesteps = 500000000 # this is number of moves and not number of episodes\n",
    "log_dir = \"./gym_full/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "policy_kwargs = dict(\n",
    "                     activation_fn=th.nn.Tanh, \n",
    "                     net_arch=[200,dict(pi= [300,200,100], vf= [300,200,100])]\n",
    ")\n",
    "\n",
    "\n",
    "battleship = Battleship(dim, ships, True)\n",
    "\n",
    "env = BattleshipEnv(battleship, rewards)\n",
    "env = Monitor(\n",
    "    env, \n",
    "    filename=log_dir, \n",
    "    allow_early_resets=True\n",
    ")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "best_mean_reward, n_steps, step_interval, episode_interval = -np.inf, 0, 10000, 10000\n",
    "\n",
    "model = A2C(\n",
    "    'MlpPolicy', \n",
    "    env,\n",
    "    learning_rate=lr,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0, \n",
    "    gamma=1\n",
    ").learn(\n",
    "    total_timesteps=num_timesteps, \n",
    "    callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = A2C.load('./gym_full/best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fa809d993697>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m model_best.learn(\n\u001b[0;32m      3\u001b[0m     \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\caleb\\anaconda3\\envs\\deep36\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         )\n",
      "\u001b[1;32mc:\\users\\caleb\\anaconda3\\envs\\deep36\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         total_timesteps, callback = self._setup_learn(\n\u001b[1;32m--> 230\u001b[1;33m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_log_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         )\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\caleb\\anaconda3\\envs\\deep36\\lib\\site-packages\\stable_baselines3\\common\\base_class.py\u001b[0m in \u001b[0;36m_setup_learn\u001b[1;34m(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;31m# Avoid resetting the environment when calling ``.learn()`` consecutive times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_obs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pytype: disable=annotation-type-mismatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_episode_starts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;31m# Retrieve unnormalized observation for saving into the buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "num_timesteps = 495000000 # this is number of moves and not number of episodes\n",
    "model_best.learn(\n",
    "    total_timesteps=num_timesteps, \n",
    "    callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with Two Ships {3,3} : 6x6 board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5, 11, 13, 14, 15, 17}\n",
      "9959 timesteps\n",
      "Best mean reward: -inf - Last mean reward per episode: -2157.36 - Last mean moves per episode: 79.11\n",
      "Saving new best model\n",
      "19984 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2286.07 - Last mean moves per episode: 81.95\n",
      "29982 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2269.04 - Last mean moves per episode: 81.50\n",
      "39870 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2373.16 - Last mean moves per episode: 83.97\n",
      "49952 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2374.91 - Last mean moves per episode: 83.98\n",
      "59968 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2367.04 - Last mean moves per episode: 83.77\n",
      "69958 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2369.57 - Last mean moves per episode: 83.80\n",
      "79956 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2400.36 - Last mean moves per episode: 84.45\n",
      "89976 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2392.13 - Last mean moves per episode: 84.26\n",
      "99973 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2389.63 - Last mean moves per episode: 84.24\n",
      "109968 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2374.49 - Last mean moves per episode: 83.89\n",
      "119996 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2349.36 - Last mean moves per episode: 83.22\n",
      "129909 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2348.55 - Last mean moves per episode: 83.23\n",
      "139990 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2348.78 - Last mean moves per episode: 83.24\n",
      "149915 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2350.27 - Last mean moves per episode: 83.29\n",
      "159919 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2330.36 - Last mean moves per episode: 82.82\n",
      "169965 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2324.18 - Last mean moves per episode: 82.67\n",
      "179997 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2336.82 - Last mean moves per episode: 82.99\n",
      "189999 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2326.89 - Last mean moves per episode: 82.76\n",
      "199989 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2309.33 - Last mean moves per episode: 82.34\n",
      "209966 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2294.90 - Last mean moves per episode: 81.99\n",
      "219971 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2267.77 - Last mean moves per episode: 81.32\n",
      "229960 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2252.34 - Last mean moves per episode: 80.95\n",
      "239971 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2238.95 - Last mean moves per episode: 80.61\n",
      "249929 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2237.91 - Last mean moves per episode: 80.60\n",
      "259900 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2220.01 - Last mean moves per episode: 80.17\n",
      "269994 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2214.90 - Last mean moves per episode: 80.02\n",
      "279997 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2208.51 - Last mean moves per episode: 79.87\n",
      "289924 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2200.50 - Last mean moves per episode: 79.67\n",
      "299967 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2197.81 - Last mean moves per episode: 79.61\n",
      "309956 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2187.11 - Last mean moves per episode: 79.36\n",
      "319937 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2176.64 - Last mean moves per episode: 79.10\n",
      "329984 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2169.00 - Last mean moves per episode: 78.91\n",
      "339942 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2162.97 - Last mean moves per episode: 78.75\n",
      "349964 timesteps\n",
      "Best mean reward: -2157.36 - Last mean reward per episode: -2152.29 - Last mean moves per episode: 78.49\n",
      "Saving new best model\n",
      "359968 timesteps\n",
      "Best mean reward: -2152.29 - Last mean reward per episode: -2141.06 - Last mean moves per episode: 78.20\n",
      "Saving new best model\n",
      "369954 timesteps\n",
      "Best mean reward: -2141.06 - Last mean reward per episode: -2134.14 - Last mean moves per episode: 78.03\n",
      "Saving new best model\n",
      "379912 timesteps\n",
      "Best mean reward: -2134.14 - Last mean reward per episode: -2117.30 - Last mean moves per episode: 77.61\n",
      "Saving new best model\n",
      "389964 timesteps\n",
      "Best mean reward: -2117.30 - Last mean reward per episode: -2106.89 - Last mean moves per episode: 77.36\n",
      "Saving new best model\n",
      "399981 timesteps\n",
      "Best mean reward: -2106.89 - Last mean reward per episode: -2099.33 - Last mean moves per episode: 77.17\n",
      "Saving new best model\n",
      "409994 timesteps\n",
      "Best mean reward: -2099.33 - Last mean reward per episode: -2084.08 - Last mean moves per episode: 76.79\n",
      "Saving new best model\n",
      "419973 timesteps\n",
      "Best mean reward: -2084.08 - Last mean reward per episode: -2076.27 - Last mean moves per episode: 76.60\n",
      "Saving new best model\n",
      "429877 timesteps\n",
      "Best mean reward: -2076.27 - Last mean reward per episode: -2071.73 - Last mean moves per episode: 76.48\n",
      "Saving new best model\n",
      "439944 timesteps\n",
      "Best mean reward: -2071.73 - Last mean reward per episode: -2062.67 - Last mean moves per episode: 76.25\n",
      "Saving new best model\n",
      "449964 timesteps\n",
      "Best mean reward: -2062.67 - Last mean reward per episode: -2055.92 - Last mean moves per episode: 76.09\n",
      "Saving new best model\n",
      "459959 timesteps\n",
      "Best mean reward: -2055.92 - Last mean reward per episode: -2049.01 - Last mean moves per episode: 75.90\n",
      "Saving new best model\n",
      "469921 timesteps\n",
      "Best mean reward: -2049.01 - Last mean reward per episode: -2035.88 - Last mean moves per episode: 75.59\n",
      "Saving new best model\n",
      "479834 timesteps\n",
      "Best mean reward: -2035.88 - Last mean reward per episode: -2030.63 - Last mean moves per episode: 75.45\n",
      "Saving new best model\n",
      "489949 timesteps\n",
      "Best mean reward: -2030.63 - Last mean reward per episode: -2023.10 - Last mean moves per episode: 75.25\n",
      "Saving new best model\n",
      "499922 timesteps\n",
      "Best mean reward: -2023.10 - Last mean reward per episode: -2015.58 - Last mean moves per episode: 75.06\n",
      "Saving new best model\n",
      "509903 timesteps\n",
      "Best mean reward: -2015.58 - Last mean reward per episode: -2008.05 - Last mean moves per episode: 74.87\n",
      "Saving new best model\n",
      "519958 timesteps\n",
      "Best mean reward: -2008.05 - Last mean reward per episode: -1994.91 - Last mean moves per episode: 74.54\n",
      "Saving new best model\n",
      "529931 timesteps\n",
      "Best mean reward: -1994.91 - Last mean reward per episode: -1986.29 - Last mean moves per episode: 74.31\n",
      "Saving new best model\n",
      "539928 timesteps\n",
      "Best mean reward: -1986.29 - Last mean reward per episode: -1976.70 - Last mean moves per episode: 74.07\n",
      "Saving new best model\n",
      "549940 timesteps\n",
      "Best mean reward: -1976.70 - Last mean reward per episode: -1971.94 - Last mean moves per episode: 73.96\n",
      "Saving new best model\n",
      "559910 timesteps\n",
      "Best mean reward: -1971.94 - Last mean reward per episode: -1964.87 - Last mean moves per episode: 73.77\n",
      "Saving new best model\n",
      "569984 timesteps\n",
      "Best mean reward: -1964.87 - Last mean reward per episode: -1957.07 - Last mean moves per episode: 73.57\n",
      "Saving new best model\n",
      "579913 timesteps\n",
      "Best mean reward: -1957.07 - Last mean reward per episode: -1949.80 - Last mean moves per episode: 73.38\n",
      "Saving new best model\n",
      "589969 timesteps\n",
      "Best mean reward: -1949.80 - Last mean reward per episode: -1943.88 - Last mean moves per episode: 73.22\n",
      "Saving new best model\n",
      "599940 timesteps\n",
      "Best mean reward: -1943.88 - Last mean reward per episode: -1933.26 - Last mean moves per episode: 72.95\n",
      "Saving new best model\n",
      "609965 timesteps\n",
      "Best mean reward: -1933.26 - Last mean reward per episode: -1928.45 - Last mean moves per episode: 72.82\n",
      "Saving new best model\n",
      "619987 timesteps\n",
      "Best mean reward: -1928.45 - Last mean reward per episode: -1924.16 - Last mean moves per episode: 72.71\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629966 timesteps\n",
      "Best mean reward: -1924.16 - Last mean reward per episode: -1920.86 - Last mean moves per episode: 72.63\n",
      "Saving new best model\n",
      "639984 timesteps\n",
      "Best mean reward: -1920.86 - Last mean reward per episode: -1913.54 - Last mean moves per episode: 72.44\n",
      "Saving new best model\n",
      "649982 timesteps\n",
      "Best mean reward: -1913.54 - Last mean reward per episode: -1906.20 - Last mean moves per episode: 72.24\n",
      "Saving new best model\n",
      "659939 timesteps\n",
      "Best mean reward: -1906.20 - Last mean reward per episode: -1899.60 - Last mean moves per episode: 72.07\n",
      "Saving new best model\n",
      "669911 timesteps\n",
      "Best mean reward: -1899.60 - Last mean reward per episode: -1892.20 - Last mean moves per episode: 71.87\n",
      "Saving new best model\n",
      "679981 timesteps\n",
      "Best mean reward: -1892.20 - Last mean reward per episode: -1888.48 - Last mean moves per episode: 71.77\n",
      "Saving new best model\n",
      "689965 timesteps\n",
      "Best mean reward: -1888.48 - Last mean reward per episode: -1879.32 - Last mean moves per episode: 71.54\n",
      "Saving new best model\n",
      "699976 timesteps\n",
      "Best mean reward: -1879.32 - Last mean reward per episode: -1876.22 - Last mean moves per episode: 71.46\n",
      "Saving new best model\n",
      "709940 timesteps\n",
      "Best mean reward: -1876.22 - Last mean reward per episode: -1872.17 - Last mean moves per episode: 71.34\n",
      "Saving new best model\n",
      "719985 timesteps\n",
      "Best mean reward: -1872.17 - Last mean reward per episode: -1864.02 - Last mean moves per episode: 71.13\n",
      "Saving new best model\n",
      "729950 timesteps\n",
      "Best mean reward: -1864.02 - Last mean reward per episode: -1847.42 - Last mean moves per episode: 70.69\n",
      "Saving new best model\n",
      "739893 timesteps\n",
      "Best mean reward: -1847.42 - Last mean reward per episode: -1831.40 - Last mean moves per episode: 70.29\n",
      "Saving new best model\n",
      "749994 timesteps\n",
      "Best mean reward: -1831.40 - Last mean reward per episode: -1819.97 - Last mean moves per episode: 70.00\n",
      "Saving new best model\n",
      "759857 timesteps\n",
      "Best mean reward: -1819.97 - Last mean reward per episode: -1807.40 - Last mean moves per episode: 69.68\n",
      "Saving new best model\n",
      "770000 timesteps\n",
      "Best mean reward: -1807.40 - Last mean reward per episode: -1794.10 - Last mean moves per episode: 69.34\n",
      "Saving new best model\n",
      "779953 timesteps\n",
      "Best mean reward: -1794.10 - Last mean reward per episode: -1780.65 - Last mean moves per episode: 68.99\n",
      "Saving new best model\n",
      "790000 timesteps\n",
      "Best mean reward: -1780.65 - Last mean reward per episode: -1763.98 - Last mean moves per episode: 68.55\n",
      "Saving new best model\n",
      "799987 timesteps\n",
      "Best mean reward: -1763.98 - Last mean reward per episode: -1752.41 - Last mean moves per episode: 68.25\n",
      "Saving new best model\n",
      "809948 timesteps\n",
      "Best mean reward: -1752.41 - Last mean reward per episode: -1740.68 - Last mean moves per episode: 67.94\n",
      "Saving new best model\n",
      "819989 timesteps\n",
      "Best mean reward: -1740.68 - Last mean reward per episode: -1728.52 - Last mean moves per episode: 67.63\n",
      "Saving new best model\n",
      "829971 timesteps\n",
      "Best mean reward: -1728.52 - Last mean reward per episode: -1719.74 - Last mean moves per episode: 67.39\n",
      "Saving new best model\n",
      "839972 timesteps\n",
      "Best mean reward: -1719.74 - Last mean reward per episode: -1712.50 - Last mean moves per episode: 67.19\n",
      "Saving new best model\n",
      "849958 timesteps\n",
      "Best mean reward: -1712.50 - Last mean reward per episode: -1695.95 - Last mean moves per episode: 66.75\n",
      "Saving new best model\n",
      "859974 timesteps\n",
      "Best mean reward: -1695.95 - Last mean reward per episode: -1686.47 - Last mean moves per episode: 66.49\n",
      "Saving new best model\n",
      "869982 timesteps\n",
      "Best mean reward: -1686.47 - Last mean reward per episode: -1676.42 - Last mean moves per episode: 66.21\n",
      "Saving new best model\n",
      "879970 timesteps\n",
      "Best mean reward: -1676.42 - Last mean reward per episode: -1670.27 - Last mean moves per episode: 66.04\n",
      "Saving new best model\n",
      "889986 timesteps\n",
      "Best mean reward: -1670.27 - Last mean reward per episode: -1661.58 - Last mean moves per episode: 65.80\n",
      "Saving new best model\n",
      "899985 timesteps\n",
      "Best mean reward: -1661.58 - Last mean reward per episode: -1655.32 - Last mean moves per episode: 65.62\n",
      "Saving new best model\n",
      "909982 timesteps\n",
      "Best mean reward: -1655.32 - Last mean reward per episode: -1646.01 - Last mean moves per episode: 65.36\n",
      "Saving new best model\n",
      "919923 timesteps\n",
      "Best mean reward: -1646.01 - Last mean reward per episode: -1638.52 - Last mean moves per episode: 65.17\n",
      "Saving new best model\n",
      "929929 timesteps\n",
      "Best mean reward: -1638.52 - Last mean reward per episode: -1626.40 - Last mean moves per episode: 64.84\n",
      "Saving new best model\n",
      "939946 timesteps\n",
      "Best mean reward: -1626.40 - Last mean reward per episode: -1617.05 - Last mean moves per episode: 64.58\n",
      "Saving new best model\n",
      "949950 timesteps\n",
      "Best mean reward: -1617.05 - Last mean reward per episode: -1608.53 - Last mean moves per episode: 64.35\n",
      "Saving new best model\n",
      "959945 timesteps\n",
      "Best mean reward: -1608.53 - Last mean reward per episode: -1603.50 - Last mean moves per episode: 64.20\n",
      "Saving new best model\n",
      "969907 timesteps\n",
      "Best mean reward: -1603.50 - Last mean reward per episode: -1596.24 - Last mean moves per episode: 64.01\n",
      "Saving new best model\n",
      "979980 timesteps\n",
      "Best mean reward: -1596.24 - Last mean reward per episode: -1588.18 - Last mean moves per episode: 63.79\n",
      "Saving new best model\n",
      "990000 timesteps\n",
      "Best mean reward: -1588.18 - Last mean reward per episode: -1582.20 - Last mean moves per episode: 63.62\n",
      "Saving new best model\n",
      "999993 timesteps\n",
      "Best mean reward: -1582.20 - Last mean reward per episode: -1576.94 - Last mean moves per episode: 63.48\n",
      "Saving new best model\n",
      "1009931 timesteps\n",
      "Best mean reward: -1576.94 - Last mean reward per episode: -1571.68 - Last mean moves per episode: 63.32\n",
      "Saving new best model\n",
      "1019986 timesteps\n",
      "Best mean reward: -1571.68 - Last mean reward per episode: -1565.50 - Last mean moves per episode: 63.15\n",
      "Saving new best model\n",
      "1029954 timesteps\n",
      "Best mean reward: -1565.50 - Last mean reward per episode: -1562.11 - Last mean moves per episode: 63.04\n",
      "Saving new best model\n",
      "1039999 timesteps\n",
      "Best mean reward: -1562.11 - Last mean reward per episode: -1559.13 - Last mean moves per episode: 62.94\n",
      "Saving new best model\n",
      "1049997 timesteps\n",
      "Best mean reward: -1559.13 - Last mean reward per episode: -1554.67 - Last mean moves per episode: 62.82\n",
      "Saving new best model\n",
      "1059934 timesteps\n",
      "Best mean reward: -1554.67 - Last mean reward per episode: -1550.07 - Last mean moves per episode: 62.68\n",
      "Saving new best model\n",
      "1069918 timesteps\n",
      "Best mean reward: -1550.07 - Last mean reward per episode: -1545.80 - Last mean moves per episode: 62.55\n",
      "Saving new best model\n",
      "1079911 timesteps\n",
      "Best mean reward: -1545.80 - Last mean reward per episode: -1540.23 - Last mean moves per episode: 62.40\n",
      "Saving new best model\n",
      "1089958 timesteps\n",
      "Best mean reward: -1540.23 - Last mean reward per episode: -1537.45 - Last mean moves per episode: 62.30\n",
      "Saving new best model\n",
      "1099977 timesteps\n",
      "Best mean reward: -1537.45 - Last mean reward per episode: -1535.31 - Last mean moves per episode: 62.23\n",
      "Saving new best model\n",
      "1109974 timesteps\n",
      "Best mean reward: -1535.31 - Last mean reward per episode: -1528.45 - Last mean moves per episode: 62.04\n",
      "Saving new best model\n",
      "1119961 timesteps\n",
      "Best mean reward: -1528.45 - Last mean reward per episode: -1525.97 - Last mean moves per episode: 61.96\n",
      "Saving new best model\n",
      "1129996 timesteps\n",
      "Best mean reward: -1525.97 - Last mean reward per episode: -1523.27 - Last mean moves per episode: 61.87\n",
      "Saving new best model\n",
      "1139990 timesteps\n",
      "Best mean reward: -1523.27 - Last mean reward per episode: -1523.93 - Last mean moves per episode: 61.87\n",
      "1149965 timesteps\n",
      "Best mean reward: -1523.27 - Last mean reward per episode: -1520.81 - Last mean moves per episode: 61.77\n",
      "Saving new best model\n",
      "1159963 timesteps\n",
      "Best mean reward: -1520.81 - Last mean reward per episode: -1521.34 - Last mean moves per episode: 61.76\n",
      "1169950 timesteps\n",
      "Best mean reward: -1520.81 - Last mean reward per episode: -1517.80 - Last mean moves per episode: 61.64\n",
      "Saving new best model\n",
      "1179899 timesteps\n",
      "Best mean reward: -1517.80 - Last mean reward per episode: -1515.18 - Last mean moves per episode: 61.56\n",
      "Saving new best model\n",
      "1189977 timesteps\n",
      "Best mean reward: -1515.18 - Last mean reward per episode: -1510.82 - Last mean moves per episode: 61.42\n",
      "Saving new best model\n",
      "1199975 timesteps\n",
      "Best mean reward: -1510.82 - Last mean reward per episode: -1508.00 - Last mean moves per episode: 61.34\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209922 timesteps\n",
      "Best mean reward: -1508.00 - Last mean reward per episode: -1505.04 - Last mean moves per episode: 61.24\n",
      "Saving new best model\n",
      "1219993 timesteps\n",
      "Best mean reward: -1505.04 - Last mean reward per episode: -1505.46 - Last mean moves per episode: 61.23\n",
      "1229989 timesteps\n",
      "Best mean reward: -1505.04 - Last mean reward per episode: -1503.13 - Last mean moves per episode: 61.14\n",
      "Saving new best model\n",
      "1239993 timesteps\n",
      "Best mean reward: -1503.13 - Last mean reward per episode: -1497.86 - Last mean moves per episode: 60.99\n",
      "Saving new best model\n",
      "1249901 timesteps\n",
      "Best mean reward: -1497.86 - Last mean reward per episode: -1499.30 - Last mean moves per episode: 61.00\n",
      "1259966 timesteps\n",
      "Best mean reward: -1497.86 - Last mean reward per episode: -1497.05 - Last mean moves per episode: 60.93\n",
      "Saving new best model\n",
      "1269984 timesteps\n",
      "Best mean reward: -1497.05 - Last mean reward per episode: -1494.08 - Last mean moves per episode: 60.84\n",
      "Saving new best model\n",
      "1279947 timesteps\n",
      "Best mean reward: -1494.08 - Last mean reward per episode: -1494.54 - Last mean moves per episode: 60.84\n",
      "1289999 timesteps\n",
      "Best mean reward: -1494.08 - Last mean reward per episode: -1489.66 - Last mean moves per episode: 60.70\n",
      "Saving new best model\n",
      "1299964 timesteps\n",
      "Best mean reward: -1489.66 - Last mean reward per episode: -1488.85 - Last mean moves per episode: 60.66\n",
      "Saving new best model\n",
      "1309970 timesteps\n",
      "Best mean reward: -1488.85 - Last mean reward per episode: -1483.85 - Last mean moves per episode: 60.51\n",
      "Saving new best model\n",
      "1319971 timesteps\n",
      "Best mean reward: -1483.85 - Last mean reward per episode: -1480.08 - Last mean moves per episode: 60.40\n",
      "Saving new best model\n",
      "1329998 timesteps\n",
      "Best mean reward: -1480.08 - Last mean reward per episode: -1479.13 - Last mean moves per episode: 60.38\n",
      "Saving new best model\n",
      "1339965 timesteps\n",
      "Best mean reward: -1479.13 - Last mean reward per episode: -1479.23 - Last mean moves per episode: 60.35\n",
      "1349988 timesteps\n",
      "Best mean reward: -1479.13 - Last mean reward per episode: -1475.14 - Last mean moves per episode: 60.23\n",
      "Saving new best model\n",
      "1359960 timesteps\n",
      "Best mean reward: -1475.14 - Last mean reward per episode: -1470.62 - Last mean moves per episode: 60.10\n",
      "Saving new best model\n",
      "1369984 timesteps\n",
      "Best mean reward: -1470.62 - Last mean reward per episode: -1467.60 - Last mean moves per episode: 60.02\n",
      "Saving new best model\n",
      "1379953 timesteps\n",
      "Best mean reward: -1467.60 - Last mean reward per episode: -1468.40 - Last mean moves per episode: 60.02\n",
      "1389945 timesteps\n",
      "Best mean reward: -1467.60 - Last mean reward per episode: -1471.07 - Last mean moves per episode: 60.08\n",
      "1399903 timesteps\n",
      "Best mean reward: -1467.60 - Last mean reward per episode: -1472.07 - Last mean moves per episode: 60.08\n",
      "1409968 timesteps\n",
      "Best mean reward: -1467.60 - Last mean reward per episode: -1471.56 - Last mean moves per episode: 60.06\n",
      "1419894 timesteps\n",
      "Best mean reward: -1467.60 - Last mean reward per episode: -1469.43 - Last mean moves per episode: 59.99\n",
      "1429968 timesteps\n",
      "Best mean reward: -1467.60 - Last mean reward per episode: -1464.29 - Last mean moves per episode: 59.85\n",
      "Saving new best model\n",
      "1439951 timesteps\n",
      "Best mean reward: -1464.29 - Last mean reward per episode: -1465.09 - Last mean moves per episode: 59.87\n",
      "1449980 timesteps\n",
      "Best mean reward: -1464.29 - Last mean reward per episode: -1464.10 - Last mean moves per episode: 59.83\n",
      "Saving new best model\n",
      "1459999 timesteps\n",
      "Best mean reward: -1464.10 - Last mean reward per episode: -1463.30 - Last mean moves per episode: 59.81\n",
      "Saving new best model\n",
      "1469969 timesteps\n",
      "Best mean reward: -1463.30 - Last mean reward per episode: -1465.28 - Last mean moves per episode: 59.86\n",
      "1479994 timesteps\n",
      "Best mean reward: -1463.30 - Last mean reward per episode: -1464.39 - Last mean moves per episode: 59.83\n",
      "1489997 timesteps\n",
      "Best mean reward: -1463.30 - Last mean reward per episode: -1460.89 - Last mean moves per episode: 59.74\n",
      "Saving new best model\n",
      "1499976 timesteps\n",
      "Best mean reward: -1460.89 - Last mean reward per episode: -1459.15 - Last mean moves per episode: 59.69\n",
      "Saving new best model\n",
      "1509996 timesteps\n",
      "Best mean reward: -1459.15 - Last mean reward per episode: -1458.37 - Last mean moves per episode: 59.66\n",
      "Saving new best model\n",
      "1519983 timesteps\n",
      "Best mean reward: -1458.37 - Last mean reward per episode: -1457.49 - Last mean moves per episode: 59.64\n",
      "Saving new best model\n",
      "1529997 timesteps\n",
      "Best mean reward: -1457.49 - Last mean reward per episode: -1456.29 - Last mean moves per episode: 59.60\n",
      "Saving new best model\n",
      "1539964 timesteps\n",
      "Best mean reward: -1456.29 - Last mean reward per episode: -1452.42 - Last mean moves per episode: 59.49\n",
      "Saving new best model\n",
      "1549983 timesteps\n",
      "Best mean reward: -1452.42 - Last mean reward per episode: -1452.96 - Last mean moves per episode: 59.49\n",
      "1559934 timesteps\n",
      "Best mean reward: -1452.42 - Last mean reward per episode: -1448.80 - Last mean moves per episode: 59.37\n",
      "Saving new best model\n",
      "1569862 timesteps\n",
      "Best mean reward: -1448.80 - Last mean reward per episode: -1447.12 - Last mean moves per episode: 59.32\n",
      "Saving new best model\n",
      "1579980 timesteps\n",
      "Best mean reward: -1447.12 - Last mean reward per episode: -1450.31 - Last mean moves per episode: 59.40\n",
      "1589954 timesteps\n",
      "Best mean reward: -1447.12 - Last mean reward per episode: -1445.28 - Last mean moves per episode: 59.25\n",
      "Saving new best model\n",
      "1599980 timesteps\n",
      "Best mean reward: -1445.28 - Last mean reward per episode: -1444.12 - Last mean moves per episode: 59.21\n",
      "Saving new best model\n",
      "1609985 timesteps\n",
      "Best mean reward: -1444.12 - Last mean reward per episode: -1444.37 - Last mean moves per episode: 59.20\n",
      "1619978 timesteps\n",
      "Best mean reward: -1444.12 - Last mean reward per episode: -1440.41 - Last mean moves per episode: 59.09\n",
      "Saving new best model\n",
      "1629974 timesteps\n",
      "Best mean reward: -1440.41 - Last mean reward per episode: -1438.69 - Last mean moves per episode: 59.04\n",
      "Saving new best model\n",
      "1639998 timesteps\n",
      "Best mean reward: -1438.69 - Last mean reward per episode: -1435.11 - Last mean moves per episode: 58.93\n",
      "Saving new best model\n",
      "1649994 timesteps\n",
      "Best mean reward: -1435.11 - Last mean reward per episode: -1432.14 - Last mean moves per episode: 58.86\n",
      "Saving new best model\n",
      "1659952 timesteps\n",
      "Best mean reward: -1432.14 - Last mean reward per episode: -1430.22 - Last mean moves per episode: 58.82\n",
      "Saving new best model\n",
      "1669959 timesteps\n",
      "Best mean reward: -1430.22 - Last mean reward per episode: -1430.64 - Last mean moves per episode: 58.82\n",
      "1679981 timesteps\n",
      "Best mean reward: -1430.22 - Last mean reward per episode: -1430.65 - Last mean moves per episode: 58.81\n",
      "1689973 timesteps\n",
      "Best mean reward: -1430.22 - Last mean reward per episode: -1427.72 - Last mean moves per episode: 58.74\n",
      "Saving new best model\n",
      "1699936 timesteps\n",
      "Best mean reward: -1427.72 - Last mean reward per episode: -1427.89 - Last mean moves per episode: 58.74\n",
      "1709984 timesteps\n",
      "Best mean reward: -1427.72 - Last mean reward per episode: -1421.54 - Last mean moves per episode: 58.58\n",
      "Saving new best model\n",
      "1719971 timesteps\n",
      "Best mean reward: -1421.54 - Last mean reward per episode: -1421.01 - Last mean moves per episode: 58.56\n",
      "Saving new best model\n",
      "1729955 timesteps\n",
      "Best mean reward: -1421.01 - Last mean reward per episode: -1417.28 - Last mean moves per episode: 58.46\n",
      "Saving new best model\n",
      "1739998 timesteps\n",
      "Best mean reward: -1417.28 - Last mean reward per episode: -1414.65 - Last mean moves per episode: 58.39\n",
      "Saving new best model\n",
      "1749967 timesteps\n",
      "Best mean reward: -1414.65 - Last mean reward per episode: -1411.54 - Last mean moves per episode: 58.31\n",
      "Saving new best model\n",
      "1759992 timesteps\n",
      "Best mean reward: -1411.54 - Last mean reward per episode: -1410.68 - Last mean moves per episode: 58.28\n",
      "Saving new best model\n",
      "1769964 timesteps\n",
      "Best mean reward: -1410.68 - Last mean reward per episode: -1411.01 - Last mean moves per episode: 58.30\n",
      "1779948 timesteps\n",
      "Best mean reward: -1410.68 - Last mean reward per episode: -1413.74 - Last mean moves per episode: 58.36\n",
      "1789996 timesteps\n",
      "Best mean reward: -1410.68 - Last mean reward per episode: -1411.14 - Last mean moves per episode: 58.29\n",
      "1799986 timesteps\n",
      "Best mean reward: -1410.68 - Last mean reward per episode: -1407.06 - Last mean moves per episode: 58.17\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1809895 timesteps\n",
      "Best mean reward: -1407.06 - Last mean reward per episode: -1402.62 - Last mean moves per episode: 58.06\n",
      "Saving new best model\n",
      "1819937 timesteps\n",
      "Best mean reward: -1402.62 - Last mean reward per episode: -1402.35 - Last mean moves per episode: 58.03\n",
      "Saving new best model\n",
      "1829986 timesteps\n",
      "Best mean reward: -1402.35 - Last mean reward per episode: -1398.75 - Last mean moves per episode: 57.94\n",
      "Saving new best model\n",
      "1839991 timesteps\n",
      "Best mean reward: -1398.75 - Last mean reward per episode: -1397.27 - Last mean moves per episode: 57.91\n",
      "Saving new best model\n",
      "1849984 timesteps\n",
      "Best mean reward: -1397.27 - Last mean reward per episode: -1396.43 - Last mean moves per episode: 57.90\n",
      "Saving new best model\n",
      "1859958 timesteps\n",
      "Best mean reward: -1396.43 - Last mean reward per episode: -1393.03 - Last mean moves per episode: 57.81\n",
      "Saving new best model\n",
      "1869921 timesteps\n",
      "Best mean reward: -1393.03 - Last mean reward per episode: -1394.55 - Last mean moves per episode: 57.84\n",
      "1879987 timesteps\n",
      "Best mean reward: -1393.03 - Last mean reward per episode: -1393.29 - Last mean moves per episode: 57.81\n",
      "1889930 timesteps\n",
      "Best mean reward: -1393.03 - Last mean reward per episode: -1395.27 - Last mean moves per episode: 57.86\n",
      "1899963 timesteps\n",
      "Best mean reward: -1393.03 - Last mean reward per episode: -1396.70 - Last mean moves per episode: 57.89\n",
      "1909942 timesteps\n",
      "Best mean reward: -1393.03 - Last mean reward per episode: -1391.81 - Last mean moves per episode: 57.77\n",
      "Saving new best model\n",
      "1919984 timesteps\n",
      "Best mean reward: -1391.81 - Last mean reward per episode: -1391.84 - Last mean moves per episode: 57.77\n",
      "1929978 timesteps\n",
      "Best mean reward: -1391.81 - Last mean reward per episode: -1392.27 - Last mean moves per episode: 57.78\n",
      "1939996 timesteps\n",
      "Best mean reward: -1391.81 - Last mean reward per episode: -1391.10 - Last mean moves per episode: 57.75\n",
      "Saving new best model\n",
      "1949947 timesteps\n",
      "Best mean reward: -1391.10 - Last mean reward per episode: -1391.42 - Last mean moves per episode: 57.74\n",
      "1959942 timesteps\n",
      "Best mean reward: -1391.10 - Last mean reward per episode: -1386.89 - Last mean moves per episode: 57.63\n",
      "Saving new best model\n",
      "1969977 timesteps\n",
      "Best mean reward: -1386.89 - Last mean reward per episode: -1383.34 - Last mean moves per episode: 57.54\n",
      "Saving new best model\n",
      "1979974 timesteps\n",
      "Best mean reward: -1383.34 - Last mean reward per episode: -1383.65 - Last mean moves per episode: 57.55\n",
      "1989971 timesteps\n",
      "Best mean reward: -1383.34 - Last mean reward per episode: -1380.87 - Last mean moves per episode: 57.48\n",
      "Saving new best model\n",
      "1999977 timesteps\n",
      "Best mean reward: -1380.87 - Last mean reward per episode: -1382.17 - Last mean moves per episode: 57.51\n",
      "2009988 timesteps\n",
      "Best mean reward: -1380.87 - Last mean reward per episode: -1382.34 - Last mean moves per episode: 57.50\n",
      "2019948 timesteps\n",
      "Best mean reward: -1380.87 - Last mean reward per episode: -1379.25 - Last mean moves per episode: 57.42\n",
      "Saving new best model\n",
      "2029953 timesteps\n",
      "Best mean reward: -1379.25 - Last mean reward per episode: -1377.54 - Last mean moves per episode: 57.38\n",
      "Saving new best model\n",
      "2039967 timesteps\n",
      "Best mean reward: -1377.54 - Last mean reward per episode: -1377.05 - Last mean moves per episode: 57.34\n",
      "Saving new best model\n",
      "2049992 timesteps\n",
      "Best mean reward: -1377.05 - Last mean reward per episode: -1378.12 - Last mean moves per episode: 57.37\n",
      "2059943 timesteps\n",
      "Best mean reward: -1377.05 - Last mean reward per episode: -1377.43 - Last mean moves per episode: 57.35\n",
      "2069988 timesteps\n",
      "Best mean reward: -1377.05 - Last mean reward per episode: -1377.92 - Last mean moves per episode: 57.36\n",
      "2079995 timesteps\n",
      "Best mean reward: -1377.05 - Last mean reward per episode: -1376.76 - Last mean moves per episode: 57.32\n",
      "Saving new best model\n",
      "2089977 timesteps\n",
      "Best mean reward: -1376.76 - Last mean reward per episode: -1374.39 - Last mean moves per episode: 57.24\n",
      "Saving new best model\n",
      "2099953 timesteps\n",
      "Best mean reward: -1374.39 - Last mean reward per episode: -1372.78 - Last mean moves per episode: 57.20\n",
      "Saving new best model\n",
      "2110000 timesteps\n",
      "Best mean reward: -1372.78 - Last mean reward per episode: -1373.12 - Last mean moves per episode: 57.21\n",
      "2119962 timesteps\n",
      "Best mean reward: -1372.78 - Last mean reward per episode: -1371.55 - Last mean moves per episode: 57.17\n",
      "Saving new best model\n",
      "2129988 timesteps\n",
      "Best mean reward: -1371.55 - Last mean reward per episode: -1371.89 - Last mean moves per episode: 57.18\n",
      "2139974 timesteps\n",
      "Best mean reward: -1371.55 - Last mean reward per episode: -1369.90 - Last mean moves per episode: 57.14\n",
      "Saving new best model\n",
      "2149959 timesteps\n",
      "Best mean reward: -1369.90 - Last mean reward per episode: -1364.98 - Last mean moves per episode: 57.00\n",
      "Saving new best model\n",
      "2159978 timesteps\n",
      "Best mean reward: -1364.98 - Last mean reward per episode: -1365.04 - Last mean moves per episode: 57.00\n",
      "2169962 timesteps\n",
      "Best mean reward: -1364.98 - Last mean reward per episode: -1365.83 - Last mean moves per episode: 57.01\n",
      "2179976 timesteps\n",
      "Best mean reward: -1364.98 - Last mean reward per episode: -1363.90 - Last mean moves per episode: 56.97\n",
      "Saving new best model\n",
      "2189984 timesteps\n",
      "Best mean reward: -1363.90 - Last mean reward per episode: -1363.91 - Last mean moves per episode: 56.96\n",
      "2199969 timesteps\n",
      "Best mean reward: -1363.90 - Last mean reward per episode: -1363.23 - Last mean moves per episode: 56.95\n",
      "Saving new best model\n",
      "2209986 timesteps\n",
      "Best mean reward: -1363.23 - Last mean reward per episode: -1364.91 - Last mean moves per episode: 56.99\n",
      "2219930 timesteps\n",
      "Best mean reward: -1363.23 - Last mean reward per episode: -1365.68 - Last mean moves per episode: 57.00\n",
      "2229994 timesteps\n",
      "Best mean reward: -1363.23 - Last mean reward per episode: -1361.78 - Last mean moves per episode: 56.87\n",
      "Saving new best model\n",
      "2239952 timesteps\n",
      "Best mean reward: -1361.78 - Last mean reward per episode: -1358.30 - Last mean moves per episode: 56.77\n",
      "Saving new best model\n",
      "2249977 timesteps\n",
      "Best mean reward: -1358.30 - Last mean reward per episode: -1359.20 - Last mean moves per episode: 56.79\n",
      "2259977 timesteps\n",
      "Best mean reward: -1358.30 - Last mean reward per episode: -1358.55 - Last mean moves per episode: 56.77\n",
      "2269986 timesteps\n",
      "Best mean reward: -1358.30 - Last mean reward per episode: -1355.12 - Last mean moves per episode: 56.68\n",
      "Saving new best model\n",
      "2279930 timesteps\n",
      "Best mean reward: -1355.12 - Last mean reward per episode: -1355.65 - Last mean moves per episode: 56.68\n",
      "2289968 timesteps\n",
      "Best mean reward: -1355.12 - Last mean reward per episode: -1355.43 - Last mean moves per episode: 56.67\n",
      "2299935 timesteps\n",
      "Best mean reward: -1355.12 - Last mean reward per episode: -1355.28 - Last mean moves per episode: 56.67\n",
      "2309946 timesteps\n",
      "Best mean reward: -1355.12 - Last mean reward per episode: -1354.94 - Last mean moves per episode: 56.67\n",
      "Saving new best model\n",
      "2319978 timesteps\n",
      "Best mean reward: -1354.94 - Last mean reward per episode: -1356.23 - Last mean moves per episode: 56.69\n",
      "2329929 timesteps\n",
      "Best mean reward: -1354.94 - Last mean reward per episode: -1351.72 - Last mean moves per episode: 56.57\n",
      "Saving new best model\n",
      "2339987 timesteps\n",
      "Best mean reward: -1351.72 - Last mean reward per episode: -1351.70 - Last mean moves per episode: 56.56\n",
      "Saving new best model\n",
      "2349943 timesteps\n",
      "Best mean reward: -1351.70 - Last mean reward per episode: -1352.22 - Last mean moves per episode: 56.58\n",
      "2359988 timesteps\n",
      "Best mean reward: -1351.70 - Last mean reward per episode: -1352.70 - Last mean moves per episode: 56.59\n",
      "2369963 timesteps\n",
      "Best mean reward: -1351.70 - Last mean reward per episode: -1352.46 - Last mean moves per episode: 56.58\n",
      "2379971 timesteps\n",
      "Best mean reward: -1351.70 - Last mean reward per episode: -1352.63 - Last mean moves per episode: 56.59\n",
      "2389958 timesteps\n",
      "Best mean reward: -1351.70 - Last mean reward per episode: -1351.42 - Last mean moves per episode: 56.57\n",
      "Saving new best model\n",
      "2399945 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1352.94 - Last mean moves per episode: 56.59\n",
      "2409982 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1352.48 - Last mean moves per episode: 56.58\n",
      "2419943 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1353.54 - Last mean moves per episode: 56.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2429987 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1356.24 - Last mean moves per episode: 56.66\n",
      "2439979 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1354.27 - Last mean moves per episode: 56.61\n",
      "2449962 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1353.95 - Last mean moves per episode: 56.60\n",
      "2459975 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1351.68 - Last mean moves per episode: 56.55\n",
      "2469983 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1351.63 - Last mean moves per episode: 56.53\n",
      "2479963 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1353.26 - Last mean moves per episode: 56.55\n",
      "2489999 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1355.18 - Last mean moves per episode: 56.61\n",
      "2499973 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1352.27 - Last mean moves per episode: 56.53\n",
      "2509930 timesteps\n",
      "Best mean reward: -1351.42 - Last mean reward per episode: -1348.35 - Last mean moves per episode: 56.44\n",
      "Saving new best model\n",
      "2519950 timesteps\n",
      "Best mean reward: -1348.35 - Last mean reward per episode: -1350.61 - Last mean moves per episode: 56.49\n",
      "2529987 timesteps\n",
      "Best mean reward: -1348.35 - Last mean reward per episode: -1353.31 - Last mean moves per episode: 56.56\n",
      "2539959 timesteps\n",
      "Best mean reward: -1348.35 - Last mean reward per episode: -1349.29 - Last mean moves per episode: 56.46\n",
      "2549981 timesteps\n",
      "Best mean reward: -1348.35 - Last mean reward per episode: -1347.52 - Last mean moves per episode: 56.43\n",
      "Saving new best model\n",
      "2559978 timesteps\n",
      "Best mean reward: -1347.52 - Last mean reward per episode: -1350.35 - Last mean moves per episode: 56.51\n",
      "2569985 timesteps\n",
      "Best mean reward: -1347.52 - Last mean reward per episode: -1347.94 - Last mean moves per episode: 56.44\n",
      "2579970 timesteps\n",
      "Best mean reward: -1347.52 - Last mean reward per episode: -1348.75 - Last mean moves per episode: 56.47\n",
      "2589998 timesteps\n",
      "Best mean reward: -1347.52 - Last mean reward per episode: -1348.13 - Last mean moves per episode: 56.44\n",
      "2599979 timesteps\n",
      "Best mean reward: -1347.52 - Last mean reward per episode: -1347.46 - Last mean moves per episode: 56.42\n",
      "Saving new best model\n",
      "2609956 timesteps\n",
      "Best mean reward: -1347.46 - Last mean reward per episode: -1344.73 - Last mean moves per episode: 56.35\n",
      "Saving new best model\n",
      "2619961 timesteps\n",
      "Best mean reward: -1344.73 - Last mean reward per episode: -1343.88 - Last mean moves per episode: 56.33\n",
      "Saving new best model\n",
      "2629987 timesteps\n",
      "Best mean reward: -1343.88 - Last mean reward per episode: -1341.11 - Last mean moves per episode: 56.25\n",
      "Saving new best model\n",
      "2639979 timesteps\n",
      "Best mean reward: -1341.11 - Last mean reward per episode: -1339.74 - Last mean moves per episode: 56.22\n",
      "Saving new best model\n",
      "2649999 timesteps\n",
      "Best mean reward: -1339.74 - Last mean reward per episode: -1338.52 - Last mean moves per episode: 56.19\n",
      "Saving new best model\n",
      "2659988 timesteps\n",
      "Best mean reward: -1338.52 - Last mean reward per episode: -1340.04 - Last mean moves per episode: 56.23\n",
      "2669961 timesteps\n",
      "Best mean reward: -1338.52 - Last mean reward per episode: -1338.88 - Last mean moves per episode: 56.20\n",
      "2679961 timesteps\n",
      "Best mean reward: -1338.52 - Last mean reward per episode: -1338.08 - Last mean moves per episode: 56.18\n",
      "Saving new best model\n",
      "2689987 timesteps\n",
      "Best mean reward: -1338.08 - Last mean reward per episode: -1338.19 - Last mean moves per episode: 56.18\n",
      "2699989 timesteps\n",
      "Best mean reward: -1338.08 - Last mean reward per episode: -1337.75 - Last mean moves per episode: 56.16\n",
      "Saving new best model\n",
      "2709974 timesteps\n",
      "Best mean reward: -1337.75 - Last mean reward per episode: -1336.32 - Last mean moves per episode: 56.13\n",
      "Saving new best model\n",
      "2719935 timesteps\n",
      "Best mean reward: -1336.32 - Last mean reward per episode: -1335.87 - Last mean moves per episode: 56.11\n",
      "Saving new best model\n",
      "2729928 timesteps\n",
      "Best mean reward: -1335.87 - Last mean reward per episode: -1332.52 - Last mean moves per episode: 56.02\n",
      "Saving new best model\n",
      "2739979 timesteps\n",
      "Best mean reward: -1332.52 - Last mean reward per episode: -1332.89 - Last mean moves per episode: 56.02\n",
      "2749989 timesteps\n",
      "Best mean reward: -1332.52 - Last mean reward per episode: -1330.13 - Last mean moves per episode: 55.96\n",
      "Saving new best model\n",
      "2759972 timesteps\n",
      "Best mean reward: -1330.13 - Last mean reward per episode: -1327.95 - Last mean moves per episode: 55.90\n",
      "Saving new best model\n",
      "2769985 timesteps\n",
      "Best mean reward: -1327.95 - Last mean reward per episode: -1323.54 - Last mean moves per episode: 55.78\n",
      "Saving new best model\n",
      "2779941 timesteps\n",
      "Best mean reward: -1323.54 - Last mean reward per episode: -1322.58 - Last mean moves per episode: 55.77\n",
      "Saving new best model\n",
      "2789998 timesteps\n",
      "Best mean reward: -1322.58 - Last mean reward per episode: -1322.10 - Last mean moves per episode: 55.77\n",
      "Saving new best model\n",
      "2799913 timesteps\n",
      "Best mean reward: -1322.10 - Last mean reward per episode: -1320.79 - Last mean moves per episode: 55.72\n",
      "Saving new best model\n",
      "2809948 timesteps\n",
      "Best mean reward: -1320.79 - Last mean reward per episode: -1317.00 - Last mean moves per episode: 55.63\n",
      "Saving new best model\n",
      "2819952 timesteps\n",
      "Best mean reward: -1317.00 - Last mean reward per episode: -1317.60 - Last mean moves per episode: 55.63\n",
      "2829953 timesteps\n",
      "Best mean reward: -1317.00 - Last mean reward per episode: -1319.28 - Last mean moves per episode: 55.67\n",
      "2839997 timesteps\n",
      "Best mean reward: -1317.00 - Last mean reward per episode: -1319.61 - Last mean moves per episode: 55.69\n",
      "2849937 timesteps\n",
      "Best mean reward: -1317.00 - Last mean reward per episode: -1319.30 - Last mean moves per episode: 55.69\n",
      "2859984 timesteps\n",
      "Best mean reward: -1317.00 - Last mean reward per episode: -1318.26 - Last mean moves per episode: 55.66\n",
      "2869941 timesteps\n",
      "Best mean reward: -1317.00 - Last mean reward per episode: -1312.60 - Last mean moves per episode: 55.51\n",
      "Saving new best model\n",
      "2879919 timesteps\n",
      "Best mean reward: -1312.60 - Last mean reward per episode: -1311.67 - Last mean moves per episode: 55.49\n",
      "Saving new best model\n",
      "2889962 timesteps\n",
      "Best mean reward: -1311.67 - Last mean reward per episode: -1307.97 - Last mean moves per episode: 55.39\n",
      "Saving new best model\n",
      "2899980 timesteps\n",
      "Best mean reward: -1307.97 - Last mean reward per episode: -1306.16 - Last mean moves per episode: 55.33\n",
      "Saving new best model\n",
      "2909963 timesteps\n",
      "Best mean reward: -1306.16 - Last mean reward per episode: -1303.45 - Last mean moves per episode: 55.26\n",
      "Saving new best model\n",
      "2919967 timesteps\n",
      "Best mean reward: -1303.45 - Last mean reward per episode: -1302.33 - Last mean moves per episode: 55.23\n",
      "Saving new best model\n",
      "2929980 timesteps\n",
      "Best mean reward: -1302.33 - Last mean reward per episode: -1300.74 - Last mean moves per episode: 55.18\n",
      "Saving new best model\n",
      "2939929 timesteps\n",
      "Best mean reward: -1300.74 - Last mean reward per episode: -1298.56 - Last mean moves per episode: 55.13\n",
      "Saving new best model\n",
      "2949952 timesteps\n",
      "Best mean reward: -1298.56 - Last mean reward per episode: -1296.23 - Last mean moves per episode: 55.07\n",
      "Saving new best model\n",
      "2959949 timesteps\n",
      "Best mean reward: -1296.23 - Last mean reward per episode: -1295.76 - Last mean moves per episode: 55.06\n",
      "Saving new best model\n",
      "2969978 timesteps\n",
      "Best mean reward: -1295.76 - Last mean reward per episode: -1296.09 - Last mean moves per episode: 55.08\n",
      "2979876 timesteps\n",
      "Best mean reward: -1295.76 - Last mean reward per episode: -1290.38 - Last mean moves per episode: 54.92\n",
      "Saving new best model\n",
      "2989937 timesteps\n",
      "Best mean reward: -1290.38 - Last mean reward per episode: -1290.16 - Last mean moves per episode: 54.91\n",
      "Saving new best model\n",
      "2999956 timesteps\n",
      "Best mean reward: -1290.16 - Last mean reward per episode: -1287.02 - Last mean moves per episode: 54.83\n",
      "Saving new best model\n",
      "3009975 timesteps\n",
      "Best mean reward: -1287.02 - Last mean reward per episode: -1288.27 - Last mean moves per episode: 54.87\n",
      "3019951 timesteps\n",
      "Best mean reward: -1287.02 - Last mean reward per episode: -1283.90 - Last mean moves per episode: 54.75\n",
      "Saving new best model\n",
      "3029977 timesteps\n",
      "Best mean reward: -1283.90 - Last mean reward per episode: -1279.25 - Last mean moves per episode: 54.63\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3039996 timesteps\n",
      "Best mean reward: -1279.25 - Last mean reward per episode: -1276.84 - Last mean moves per episode: 54.55\n",
      "Saving new best model\n",
      "3049953 timesteps\n",
      "Best mean reward: -1276.84 - Last mean reward per episode: -1277.87 - Last mean moves per episode: 54.58\n",
      "3059996 timesteps\n",
      "Best mean reward: -1276.84 - Last mean reward per episode: -1276.77 - Last mean moves per episode: 54.54\n",
      "Saving new best model\n",
      "3069976 timesteps\n",
      "Best mean reward: -1276.77 - Last mean reward per episode: -1268.79 - Last mean moves per episode: 54.33\n",
      "Saving new best model\n",
      "3079956 timesteps\n",
      "Best mean reward: -1268.79 - Last mean reward per episode: -1267.14 - Last mean moves per episode: 54.29\n",
      "Saving new best model\n",
      "3089974 timesteps\n",
      "Best mean reward: -1267.14 - Last mean reward per episode: -1266.16 - Last mean moves per episode: 54.26\n",
      "Saving new best model\n",
      "3099940 timesteps\n",
      "Best mean reward: -1266.16 - Last mean reward per episode: -1261.66 - Last mean moves per episode: 54.13\n",
      "Saving new best model\n",
      "3109985 timesteps\n",
      "Best mean reward: -1261.66 - Last mean reward per episode: -1260.85 - Last mean moves per episode: 54.11\n",
      "Saving new best model\n",
      "3119955 timesteps\n",
      "Best mean reward: -1260.85 - Last mean reward per episode: -1256.62 - Last mean moves per episode: 54.00\n",
      "Saving new best model\n",
      "3129979 timesteps\n",
      "Best mean reward: -1256.62 - Last mean reward per episode: -1256.41 - Last mean moves per episode: 54.02\n",
      "Saving new best model\n",
      "3139958 timesteps\n",
      "Best mean reward: -1256.41 - Last mean reward per episode: -1251.84 - Last mean moves per episode: 53.91\n",
      "Saving new best model\n",
      "3149934 timesteps\n",
      "Best mean reward: -1251.84 - Last mean reward per episode: -1249.61 - Last mean moves per episode: 53.84\n",
      "Saving new best model\n",
      "3159992 timesteps\n",
      "Best mean reward: -1249.61 - Last mean reward per episode: -1247.71 - Last mean moves per episode: 53.79\n",
      "Saving new best model\n",
      "3169931 timesteps\n",
      "Best mean reward: -1247.71 - Last mean reward per episode: -1247.36 - Last mean moves per episode: 53.79\n",
      "Saving new best model\n",
      "3179957 timesteps\n",
      "Best mean reward: -1247.36 - Last mean reward per episode: -1244.91 - Last mean moves per episode: 53.73\n",
      "Saving new best model\n",
      "3189971 timesteps\n",
      "Best mean reward: -1244.91 - Last mean reward per episode: -1244.67 - Last mean moves per episode: 53.72\n",
      "Saving new best model\n",
      "3199954 timesteps\n",
      "Best mean reward: -1244.67 - Last mean reward per episode: -1241.27 - Last mean moves per episode: 53.63\n",
      "Saving new best model\n",
      "3209992 timesteps\n",
      "Best mean reward: -1241.27 - Last mean reward per episode: -1240.02 - Last mean moves per episode: 53.61\n",
      "Saving new best model\n",
      "3219988 timesteps\n",
      "Best mean reward: -1240.02 - Last mean reward per episode: -1240.37 - Last mean moves per episode: 53.61\n",
      "3229976 timesteps\n",
      "Best mean reward: -1240.02 - Last mean reward per episode: -1236.24 - Last mean moves per episode: 53.50\n",
      "Saving new best model\n",
      "3239986 timesteps\n",
      "Best mean reward: -1236.24 - Last mean reward per episode: -1235.75 - Last mean moves per episode: 53.48\n",
      "Saving new best model\n",
      "3249989 timesteps\n",
      "Best mean reward: -1235.75 - Last mean reward per episode: -1232.98 - Last mean moves per episode: 53.41\n",
      "Saving new best model\n",
      "3259886 timesteps\n",
      "Best mean reward: -1232.98 - Last mean reward per episode: -1235.66 - Last mean moves per episode: 53.48\n",
      "3269969 timesteps\n",
      "Best mean reward: -1232.98 - Last mean reward per episode: -1235.56 - Last mean moves per episode: 53.48\n",
      "3279987 timesteps\n",
      "Best mean reward: -1232.98 - Last mean reward per episode: -1235.17 - Last mean moves per episode: 53.48\n",
      "3289971 timesteps\n",
      "Best mean reward: -1232.98 - Last mean reward per episode: -1238.33 - Last mean moves per episode: 53.56\n",
      "3299862 timesteps\n",
      "Best mean reward: -1232.98 - Last mean reward per episode: -1238.37 - Last mean moves per episode: 53.56\n",
      "3309933 timesteps\n",
      "Best mean reward: -1232.98 - Last mean reward per episode: -1237.35 - Last mean moves per episode: 53.54\n",
      "3319985 timesteps\n",
      "Best mean reward: -1232.98 - Last mean reward per episode: -1235.52 - Last mean moves per episode: 53.48\n",
      "3329948 timesteps\n",
      "Best mean reward: -1232.98 - Last mean reward per episode: -1234.35 - Last mean moves per episode: 53.46\n",
      "3339931 timesteps\n",
      "Best mean reward: -1232.98 - Last mean reward per episode: -1233.22 - Last mean moves per episode: 53.42\n",
      "3349950 timesteps\n",
      "Best mean reward: -1232.98 - Last mean reward per episode: -1231.33 - Last mean moves per episode: 53.36\n",
      "Saving new best model\n",
      "3359931 timesteps\n",
      "Best mean reward: -1231.33 - Last mean reward per episode: -1226.25 - Last mean moves per episode: 53.23\n",
      "Saving new best model\n",
      "3369962 timesteps\n",
      "Best mean reward: -1226.25 - Last mean reward per episode: -1224.86 - Last mean moves per episode: 53.19\n",
      "Saving new best model\n",
      "3379957 timesteps\n",
      "Best mean reward: -1224.86 - Last mean reward per episode: -1224.44 - Last mean moves per episode: 53.18\n",
      "Saving new best model\n",
      "3389993 timesteps\n",
      "Best mean reward: -1224.44 - Last mean reward per episode: -1222.30 - Last mean moves per episode: 53.12\n",
      "Saving new best model\n",
      "3399993 timesteps\n",
      "Best mean reward: -1222.30 - Last mean reward per episode: -1223.97 - Last mean moves per episode: 53.17\n",
      "3409968 timesteps\n",
      "Best mean reward: -1222.30 - Last mean reward per episode: -1222.59 - Last mean moves per episode: 53.11\n",
      "3419977 timesteps\n",
      "Best mean reward: -1222.30 - Last mean reward per episode: -1224.24 - Last mean moves per episode: 53.17\n",
      "3429991 timesteps\n",
      "Best mean reward: -1222.30 - Last mean reward per episode: -1223.36 - Last mean moves per episode: 53.16\n",
      "3439873 timesteps\n",
      "Best mean reward: -1222.30 - Last mean reward per episode: -1222.53 - Last mean moves per episode: 53.15\n",
      "3449982 timesteps\n",
      "Best mean reward: -1222.30 - Last mean reward per episode: -1221.81 - Last mean moves per episode: 53.15\n",
      "Saving new best model\n",
      "3459966 timesteps\n",
      "Best mean reward: -1221.81 - Last mean reward per episode: -1219.56 - Last mean moves per episode: 53.09\n",
      "Saving new best model\n",
      "3469993 timesteps\n",
      "Best mean reward: -1219.56 - Last mean reward per episode: -1218.52 - Last mean moves per episode: 53.06\n",
      "Saving new best model\n",
      "3479957 timesteps\n",
      "Best mean reward: -1218.52 - Last mean reward per episode: -1215.69 - Last mean moves per episode: 52.98\n",
      "Saving new best model\n",
      "3489976 timesteps\n",
      "Best mean reward: -1215.69 - Last mean reward per episode: -1212.56 - Last mean moves per episode: 52.91\n",
      "Saving new best model\n",
      "3499996 timesteps\n",
      "Best mean reward: -1212.56 - Last mean reward per episode: -1208.08 - Last mean moves per episode: 52.79\n",
      "Saving new best model\n",
      "3509982 timesteps\n",
      "Best mean reward: -1208.08 - Last mean reward per episode: -1208.03 - Last mean moves per episode: 52.79\n",
      "Saving new best model\n",
      "3519949 timesteps\n",
      "Best mean reward: -1208.03 - Last mean reward per episode: -1203.96 - Last mean moves per episode: 52.69\n",
      "Saving new best model\n",
      "3529993 timesteps\n",
      "Best mean reward: -1203.96 - Last mean reward per episode: -1202.16 - Last mean moves per episode: 52.63\n",
      "Saving new best model\n",
      "3539962 timesteps\n",
      "Best mean reward: -1202.16 - Last mean reward per episode: -1199.06 - Last mean moves per episode: 52.56\n",
      "Saving new best model\n",
      "3549976 timesteps\n",
      "Best mean reward: -1199.06 - Last mean reward per episode: -1199.68 - Last mean moves per episode: 52.57\n",
      "3559996 timesteps\n",
      "Best mean reward: -1199.06 - Last mean reward per episode: -1199.89 - Last mean moves per episode: 52.59\n",
      "3569967 timesteps\n",
      "Best mean reward: -1199.06 - Last mean reward per episode: -1199.95 - Last mean moves per episode: 52.61\n",
      "3579947 timesteps\n",
      "Best mean reward: -1199.06 - Last mean reward per episode: -1198.48 - Last mean moves per episode: 52.58\n",
      "Saving new best model\n",
      "3589996 timesteps\n",
      "Best mean reward: -1198.48 - Last mean reward per episode: -1199.02 - Last mean moves per episode: 52.59\n",
      "3599949 timesteps\n",
      "Best mean reward: -1198.48 - Last mean reward per episode: -1198.18 - Last mean moves per episode: 52.57\n",
      "Saving new best model\n",
      "3609989 timesteps\n",
      "Best mean reward: -1198.18 - Last mean reward per episode: -1196.57 - Last mean moves per episode: 52.55\n",
      "Saving new best model\n",
      "3619995 timesteps\n",
      "Best mean reward: -1196.57 - Last mean reward per episode: -1193.57 - Last mean moves per episode: 52.47\n",
      "Saving new best model\n",
      "3629995 timesteps\n",
      "Best mean reward: -1193.57 - Last mean reward per episode: -1188.96 - Last mean moves per episode: 52.34\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3639964 timesteps\n",
      "Best mean reward: -1188.96 - Last mean reward per episode: -1186.86 - Last mean moves per episode: 52.30\n",
      "Saving new best model\n",
      "3649976 timesteps\n",
      "Best mean reward: -1186.86 - Last mean reward per episode: -1187.37 - Last mean moves per episode: 52.30\n",
      "3659975 timesteps\n",
      "Best mean reward: -1186.86 - Last mean reward per episode: -1187.56 - Last mean moves per episode: 52.30\n",
      "3669987 timesteps\n",
      "Best mean reward: -1186.86 - Last mean reward per episode: -1186.14 - Last mean moves per episode: 52.26\n",
      "Saving new best model\n",
      "3679968 timesteps\n",
      "Best mean reward: -1186.14 - Last mean reward per episode: -1186.64 - Last mean moves per episode: 52.28\n",
      "3689950 timesteps\n",
      "Best mean reward: -1186.14 - Last mean reward per episode: -1187.38 - Last mean moves per episode: 52.29\n",
      "3699915 timesteps\n",
      "Best mean reward: -1186.14 - Last mean reward per episode: -1185.59 - Last mean moves per episode: 52.24\n",
      "Saving new best model\n",
      "3709972 timesteps\n",
      "Best mean reward: -1185.59 - Last mean reward per episode: -1184.55 - Last mean moves per episode: 52.21\n",
      "Saving new best model\n",
      "3719955 timesteps\n",
      "Best mean reward: -1184.55 - Last mean reward per episode: -1184.47 - Last mean moves per episode: 52.21\n",
      "Saving new best model\n",
      "3729952 timesteps\n",
      "Best mean reward: -1184.47 - Last mean reward per episode: -1181.24 - Last mean moves per episode: 52.11\n",
      "Saving new best model\n",
      "3739985 timesteps\n",
      "Best mean reward: -1181.24 - Last mean reward per episode: -1184.62 - Last mean moves per episode: 52.20\n",
      "3749989 timesteps\n",
      "Best mean reward: -1181.24 - Last mean reward per episode: -1183.29 - Last mean moves per episode: 52.17\n",
      "3759975 timesteps\n",
      "Best mean reward: -1181.24 - Last mean reward per episode: -1179.26 - Last mean moves per episode: 52.07\n",
      "Saving new best model\n",
      "3769989 timesteps\n",
      "Best mean reward: -1179.26 - Last mean reward per episode: -1177.80 - Last mean moves per episode: 52.03\n",
      "Saving new best model\n",
      "3779955 timesteps\n",
      "Best mean reward: -1177.80 - Last mean reward per episode: -1177.34 - Last mean moves per episode: 52.03\n",
      "Saving new best model\n",
      "3789952 timesteps\n",
      "Best mean reward: -1177.34 - Last mean reward per episode: -1175.53 - Last mean moves per episode: 51.98\n",
      "Saving new best model\n",
      "3799991 timesteps\n",
      "Best mean reward: -1175.53 - Last mean reward per episode: -1174.24 - Last mean moves per episode: 51.94\n",
      "Saving new best model\n",
      "3809947 timesteps\n",
      "Best mean reward: -1174.24 - Last mean reward per episode: -1170.72 - Last mean moves per episode: 51.86\n",
      "Saving new best model\n",
      "3819952 timesteps\n",
      "Best mean reward: -1170.72 - Last mean reward per episode: -1168.91 - Last mean moves per episode: 51.81\n",
      "Saving new best model\n",
      "3829961 timesteps\n",
      "Best mean reward: -1168.91 - Last mean reward per episode: -1168.25 - Last mean moves per episode: 51.79\n",
      "Saving new best model\n",
      "3839976 timesteps\n",
      "Best mean reward: -1168.25 - Last mean reward per episode: -1166.59 - Last mean moves per episode: 51.75\n",
      "Saving new best model\n",
      "3849980 timesteps\n",
      "Best mean reward: -1166.59 - Last mean reward per episode: -1165.67 - Last mean moves per episode: 51.73\n",
      "Saving new best model\n",
      "3859975 timesteps\n",
      "Best mean reward: -1165.67 - Last mean reward per episode: -1166.34 - Last mean moves per episode: 51.76\n",
      "3869990 timesteps\n",
      "Best mean reward: -1165.67 - Last mean reward per episode: -1168.41 - Last mean moves per episode: 51.82\n",
      "3879979 timesteps\n",
      "Best mean reward: -1165.67 - Last mean reward per episode: -1169.70 - Last mean moves per episode: 51.85\n",
      "3889946 timesteps\n",
      "Best mean reward: -1165.67 - Last mean reward per episode: -1169.72 - Last mean moves per episode: 51.85\n",
      "3899983 timesteps\n",
      "Best mean reward: -1165.67 - Last mean reward per episode: -1169.06 - Last mean moves per episode: 51.82\n",
      "3909989 timesteps\n",
      "Best mean reward: -1165.67 - Last mean reward per episode: -1171.07 - Last mean moves per episode: 51.88\n",
      "3919947 timesteps\n",
      "Best mean reward: -1165.67 - Last mean reward per episode: -1166.41 - Last mean moves per episode: 51.76\n",
      "3929954 timesteps\n",
      "Best mean reward: -1165.67 - Last mean reward per episode: -1165.58 - Last mean moves per episode: 51.74\n",
      "Saving new best model\n",
      "3939975 timesteps\n",
      "Best mean reward: -1165.58 - Last mean reward per episode: -1165.01 - Last mean moves per episode: 51.72\n",
      "Saving new best model\n",
      "3949944 timesteps\n",
      "Best mean reward: -1165.01 - Last mean reward per episode: -1162.52 - Last mean moves per episode: 51.66\n",
      "Saving new best model\n",
      "3959950 timesteps\n",
      "Best mean reward: -1162.52 - Last mean reward per episode: -1160.92 - Last mean moves per episode: 51.61\n",
      "Saving new best model\n",
      "3969999 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1162.75 - Last mean moves per episode: 51.65\n",
      "3979943 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1162.57 - Last mean moves per episode: 51.65\n",
      "3989980 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1163.17 - Last mean moves per episode: 51.67\n",
      "3999992 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1161.57 - Last mean moves per episode: 51.62\n",
      "4009985 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1163.78 - Last mean moves per episode: 51.68\n",
      "4019974 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1164.70 - Last mean moves per episode: 51.71\n",
      "4029890 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1163.58 - Last mean moves per episode: 51.68\n",
      "4039965 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1165.05 - Last mean moves per episode: 51.71\n",
      "4049989 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1164.82 - Last mean moves per episode: 51.71\n",
      "4059981 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1163.01 - Last mean moves per episode: 51.68\n",
      "4069994 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1162.62 - Last mean moves per episode: 51.67\n",
      "4079952 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1161.21 - Last mean moves per episode: 51.63\n",
      "4089941 timesteps\n",
      "Best mean reward: -1160.92 - Last mean reward per episode: -1159.77 - Last mean moves per episode: 51.58\n",
      "Saving new best model\n",
      "4099959 timesteps\n",
      "Best mean reward: -1159.77 - Last mean reward per episode: -1158.82 - Last mean moves per episode: 51.56\n",
      "Saving new best model\n",
      "4109998 timesteps\n",
      "Best mean reward: -1158.82 - Last mean reward per episode: -1160.02 - Last mean moves per episode: 51.59\n",
      "4119987 timesteps\n",
      "Best mean reward: -1158.82 - Last mean reward per episode: -1155.58 - Last mean moves per episode: 51.47\n",
      "Saving new best model\n",
      "4129945 timesteps\n",
      "Best mean reward: -1155.58 - Last mean reward per episode: -1153.64 - Last mean moves per episode: 51.42\n",
      "Saving new best model\n",
      "4139979 timesteps\n",
      "Best mean reward: -1153.64 - Last mean reward per episode: -1153.67 - Last mean moves per episode: 51.43\n",
      "4149983 timesteps\n",
      "Best mean reward: -1153.64 - Last mean reward per episode: -1154.61 - Last mean moves per episode: 51.46\n",
      "4159980 timesteps\n",
      "Best mean reward: -1153.64 - Last mean reward per episode: -1156.05 - Last mean moves per episode: 51.50\n",
      "4169944 timesteps\n",
      "Best mean reward: -1153.64 - Last mean reward per episode: -1156.46 - Last mean moves per episode: 51.51\n",
      "4179952 timesteps\n",
      "Best mean reward: -1153.64 - Last mean reward per episode: -1153.99 - Last mean moves per episode: 51.45\n",
      "4189963 timesteps\n",
      "Best mean reward: -1153.64 - Last mean reward per episode: -1150.06 - Last mean moves per episode: 51.35\n",
      "Saving new best model\n",
      "4199944 timesteps\n",
      "Best mean reward: -1150.06 - Last mean reward per episode: -1146.87 - Last mean moves per episode: 51.26\n",
      "Saving new best model\n",
      "4209987 timesteps\n",
      "Best mean reward: -1146.87 - Last mean reward per episode: -1146.11 - Last mean moves per episode: 51.24\n",
      "Saving new best model\n",
      "4219948 timesteps\n",
      "Best mean reward: -1146.11 - Last mean reward per episode: -1143.79 - Last mean moves per episode: 51.17\n",
      "Saving new best model\n",
      "4229998 timesteps\n",
      "Best mean reward: -1143.79 - Last mean reward per episode: -1144.46 - Last mean moves per episode: 51.19\n",
      "4239984 timesteps\n",
      "Best mean reward: -1143.79 - Last mean reward per episode: -1144.34 - Last mean moves per episode: 51.19\n",
      "4249993 timesteps\n",
      "Best mean reward: -1143.79 - Last mean reward per episode: -1142.08 - Last mean moves per episode: 51.14\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4259956 timesteps\n",
      "Best mean reward: -1142.08 - Last mean reward per episode: -1139.46 - Last mean moves per episode: 51.07\n",
      "Saving new best model\n",
      "4269976 timesteps\n",
      "Best mean reward: -1139.46 - Last mean reward per episode: -1141.94 - Last mean moves per episode: 51.14\n",
      "4279996 timesteps\n",
      "Best mean reward: -1139.46 - Last mean reward per episode: -1142.87 - Last mean moves per episode: 51.14\n",
      "4289971 timesteps\n",
      "Best mean reward: -1139.46 - Last mean reward per episode: -1140.62 - Last mean moves per episode: 51.07\n",
      "4299974 timesteps\n",
      "Best mean reward: -1139.46 - Last mean reward per episode: -1138.58 - Last mean moves per episode: 51.01\n",
      "Saving new best model\n",
      "4309985 timesteps\n",
      "Best mean reward: -1138.58 - Last mean reward per episode: -1137.23 - Last mean moves per episode: 50.98\n",
      "Saving new best model\n",
      "4319950 timesteps\n",
      "Best mean reward: -1137.23 - Last mean reward per episode: -1135.12 - Last mean moves per episode: 50.92\n",
      "Saving new best model\n",
      "4329969 timesteps\n",
      "Best mean reward: -1135.12 - Last mean reward per episode: -1134.80 - Last mean moves per episode: 50.92\n",
      "Saving new best model\n",
      "4339950 timesteps\n",
      "Best mean reward: -1134.80 - Last mean reward per episode: -1136.24 - Last mean moves per episode: 50.97\n",
      "4349945 timesteps\n",
      "Best mean reward: -1134.80 - Last mean reward per episode: -1136.75 - Last mean moves per episode: 50.99\n",
      "4359960 timesteps\n",
      "Best mean reward: -1134.80 - Last mean reward per episode: -1136.60 - Last mean moves per episode: 50.98\n",
      "4369999 timesteps\n",
      "Best mean reward: -1134.80 - Last mean reward per episode: -1134.34 - Last mean moves per episode: 50.91\n",
      "Saving new best model\n",
      "4379939 timesteps\n",
      "Best mean reward: -1134.34 - Last mean reward per episode: -1129.67 - Last mean moves per episode: 50.78\n",
      "Saving new best model\n",
      "4389984 timesteps\n",
      "Best mean reward: -1129.67 - Last mean reward per episode: -1129.59 - Last mean moves per episode: 50.78\n",
      "Saving new best model\n",
      "4399935 timesteps\n",
      "Best mean reward: -1129.59 - Last mean reward per episode: -1125.76 - Last mean moves per episode: 50.68\n",
      "Saving new best model\n",
      "4409944 timesteps\n",
      "Best mean reward: -1125.76 - Last mean reward per episode: -1122.17 - Last mean moves per episode: 50.59\n",
      "Saving new best model\n",
      "4419951 timesteps\n",
      "Best mean reward: -1122.17 - Last mean reward per episode: -1120.52 - Last mean moves per episode: 50.56\n",
      "Saving new best model\n",
      "4429972 timesteps\n",
      "Best mean reward: -1120.52 - Last mean reward per episode: -1120.16 - Last mean moves per episode: 50.54\n",
      "Saving new best model\n",
      "4439998 timesteps\n",
      "Best mean reward: -1120.16 - Last mean reward per episode: -1120.56 - Last mean moves per episode: 50.56\n",
      "4449922 timesteps\n",
      "Best mean reward: -1120.16 - Last mean reward per episode: -1116.07 - Last mean moves per episode: 50.44\n",
      "Saving new best model\n",
      "4459933 timesteps\n",
      "Best mean reward: -1116.07 - Last mean reward per episode: -1113.42 - Last mean moves per episode: 50.38\n",
      "Saving new best model\n",
      "4469972 timesteps\n",
      "Best mean reward: -1113.42 - Last mean reward per episode: -1112.98 - Last mean moves per episode: 50.35\n",
      "Saving new best model\n",
      "4479990 timesteps\n",
      "Best mean reward: -1112.98 - Last mean reward per episode: -1111.16 - Last mean moves per episode: 50.31\n",
      "Saving new best model\n",
      "4490000 timesteps\n",
      "Best mean reward: -1111.16 - Last mean reward per episode: -1107.81 - Last mean moves per episode: 50.22\n",
      "Saving new best model\n",
      "4499960 timesteps\n",
      "Best mean reward: -1107.81 - Last mean reward per episode: -1109.83 - Last mean moves per episode: 50.27\n",
      "4509969 timesteps\n",
      "Best mean reward: -1107.81 - Last mean reward per episode: -1109.36 - Last mean moves per episode: 50.26\n",
      "4519997 timesteps\n",
      "Best mean reward: -1107.81 - Last mean reward per episode: -1108.38 - Last mean moves per episode: 50.23\n",
      "4529994 timesteps\n",
      "Best mean reward: -1107.81 - Last mean reward per episode: -1103.99 - Last mean moves per episode: 50.11\n",
      "Saving new best model\n",
      "4539994 timesteps\n",
      "Best mean reward: -1103.99 - Last mean reward per episode: -1103.01 - Last mean moves per episode: 50.09\n",
      "Saving new best model\n",
      "4549991 timesteps\n",
      "Best mean reward: -1103.01 - Last mean reward per episode: -1102.53 - Last mean moves per episode: 50.08\n",
      "Saving new best model\n",
      "4559984 timesteps\n",
      "Best mean reward: -1102.53 - Last mean reward per episode: -1102.63 - Last mean moves per episode: 50.08\n",
      "4569978 timesteps\n",
      "Best mean reward: -1102.53 - Last mean reward per episode: -1098.59 - Last mean moves per episode: 49.97\n",
      "Saving new best model\n",
      "4579944 timesteps\n",
      "Best mean reward: -1098.59 - Last mean reward per episode: -1096.24 - Last mean moves per episode: 49.91\n",
      "Saving new best model\n",
      "4589969 timesteps\n",
      "Best mean reward: -1096.24 - Last mean reward per episode: -1091.14 - Last mean moves per episode: 49.77\n",
      "Saving new best model\n",
      "4599965 timesteps\n",
      "Best mean reward: -1091.14 - Last mean reward per episode: -1088.95 - Last mean moves per episode: 49.71\n",
      "Saving new best model\n",
      "4609973 timesteps\n",
      "Best mean reward: -1088.95 - Last mean reward per episode: -1087.20 - Last mean moves per episode: 49.66\n",
      "Saving new best model\n",
      "4619985 timesteps\n",
      "Best mean reward: -1087.20 - Last mean reward per episode: -1089.29 - Last mean moves per episode: 49.72\n",
      "4629864 timesteps\n",
      "Best mean reward: -1087.20 - Last mean reward per episode: -1087.31 - Last mean moves per episode: 49.66\n",
      "4639952 timesteps\n",
      "Best mean reward: -1087.20 - Last mean reward per episode: -1085.05 - Last mean moves per episode: 49.60\n",
      "Saving new best model\n",
      "4649964 timesteps\n",
      "Best mean reward: -1085.05 - Last mean reward per episode: -1080.41 - Last mean moves per episode: 49.46\n",
      "Saving new best model\n",
      "4659915 timesteps\n",
      "Best mean reward: -1080.41 - Last mean reward per episode: -1077.28 - Last mean moves per episode: 49.39\n",
      "Saving new best model\n",
      "4669948 timesteps\n",
      "Best mean reward: -1077.28 - Last mean reward per episode: -1075.13 - Last mean moves per episode: 49.32\n",
      "Saving new best model\n",
      "4679947 timesteps\n",
      "Best mean reward: -1075.13 - Last mean reward per episode: -1077.90 - Last mean moves per episode: 49.41\n",
      "4689966 timesteps\n",
      "Best mean reward: -1075.13 - Last mean reward per episode: -1077.52 - Last mean moves per episode: 49.40\n",
      "4699965 timesteps\n",
      "Best mean reward: -1075.13 - Last mean reward per episode: -1077.59 - Last mean moves per episode: 49.40\n",
      "4709960 timesteps\n",
      "Best mean reward: -1075.13 - Last mean reward per episode: -1074.04 - Last mean moves per episode: 49.31\n",
      "Saving new best model\n",
      "4719994 timesteps\n",
      "Best mean reward: -1074.04 - Last mean reward per episode: -1074.16 - Last mean moves per episode: 49.31\n",
      "4729967 timesteps\n",
      "Best mean reward: -1074.04 - Last mean reward per episode: -1070.68 - Last mean moves per episode: 49.23\n",
      "Saving new best model\n",
      "4739974 timesteps\n",
      "Best mean reward: -1070.68 - Last mean reward per episode: -1067.57 - Last mean moves per episode: 49.16\n",
      "Saving new best model\n",
      "4749982 timesteps\n",
      "Best mean reward: -1067.57 - Last mean reward per episode: -1065.94 - Last mean moves per episode: 49.09\n",
      "Saving new best model\n",
      "4759980 timesteps\n",
      "Best mean reward: -1065.94 - Last mean reward per episode: -1062.89 - Last mean moves per episode: 49.01\n",
      "Saving new best model\n",
      "4769940 timesteps\n",
      "Best mean reward: -1062.89 - Last mean reward per episode: -1062.65 - Last mean moves per episode: 49.02\n",
      "Saving new best model\n",
      "4779984 timesteps\n",
      "Best mean reward: -1062.65 - Last mean reward per episode: -1059.75 - Last mean moves per episode: 48.95\n",
      "Saving new best model\n",
      "4790000 timesteps\n",
      "Best mean reward: -1059.75 - Last mean reward per episode: -1060.15 - Last mean moves per episode: 48.99\n",
      "4799971 timesteps\n",
      "Best mean reward: -1059.75 - Last mean reward per episode: -1059.27 - Last mean moves per episode: 48.97\n",
      "Saving new best model\n",
      "4809973 timesteps\n",
      "Best mean reward: -1059.27 - Last mean reward per episode: -1057.82 - Last mean moves per episode: 48.93\n",
      "Saving new best model\n",
      "4819980 timesteps\n",
      "Best mean reward: -1057.82 - Last mean reward per episode: -1056.85 - Last mean moves per episode: 48.90\n",
      "Saving new best model\n",
      "4829991 timesteps\n",
      "Best mean reward: -1056.85 - Last mean reward per episode: -1055.21 - Last mean moves per episode: 48.85\n",
      "Saving new best model\n",
      "4839990 timesteps\n",
      "Best mean reward: -1055.21 - Last mean reward per episode: -1051.90 - Last mean moves per episode: 48.76\n",
      "Saving new best model\n",
      "4849972 timesteps\n",
      "Best mean reward: -1051.90 - Last mean reward per episode: -1047.98 - Last mean moves per episode: 48.66\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4859981 timesteps\n",
      "Best mean reward: -1047.98 - Last mean reward per episode: -1048.00 - Last mean moves per episode: 48.67\n",
      "4869969 timesteps\n",
      "Best mean reward: -1047.98 - Last mean reward per episode: -1049.87 - Last mean moves per episode: 48.73\n",
      "4879959 timesteps\n",
      "Best mean reward: -1047.98 - Last mean reward per episode: -1047.73 - Last mean moves per episode: 48.68\n",
      "Saving new best model\n",
      "4889970 timesteps\n",
      "Best mean reward: -1047.73 - Last mean reward per episode: -1047.21 - Last mean moves per episode: 48.66\n",
      "Saving new best model\n",
      "4899974 timesteps\n",
      "Best mean reward: -1047.21 - Last mean reward per episode: -1049.87 - Last mean moves per episode: 48.75\n",
      "4909988 timesteps\n",
      "Best mean reward: -1047.21 - Last mean reward per episode: -1048.19 - Last mean moves per episode: 48.70\n",
      "4919991 timesteps\n",
      "Best mean reward: -1047.21 - Last mean reward per episode: -1045.32 - Last mean moves per episode: 48.63\n",
      "Saving new best model\n",
      "4929972 timesteps\n",
      "Best mean reward: -1045.32 - Last mean reward per episode: -1045.42 - Last mean moves per episode: 48.63\n",
      "4939987 timesteps\n",
      "Best mean reward: -1045.32 - Last mean reward per episode: -1047.33 - Last mean moves per episode: 48.67\n",
      "4949957 timesteps\n",
      "Best mean reward: -1045.32 - Last mean reward per episode: -1045.69 - Last mean moves per episode: 48.63\n",
      "4959945 timesteps\n",
      "Best mean reward: -1045.32 - Last mean reward per episode: -1042.88 - Last mean moves per episode: 48.56\n",
      "Saving new best model\n",
      "4969953 timesteps\n",
      "Best mean reward: -1042.88 - Last mean reward per episode: -1043.55 - Last mean moves per episode: 48.57\n",
      "4979967 timesteps\n",
      "Best mean reward: -1042.88 - Last mean reward per episode: -1039.81 - Last mean moves per episode: 48.46\n",
      "Saving new best model\n",
      "4989972 timesteps\n",
      "Best mean reward: -1039.81 - Last mean reward per episode: -1037.58 - Last mean moves per episode: 48.40\n",
      "Saving new best model\n",
      "4999987 timesteps\n",
      "Best mean reward: -1037.58 - Last mean reward per episode: -1034.07 - Last mean moves per episode: 48.31\n",
      "Saving new best model\n",
      "5009963 timesteps\n",
      "Best mean reward: -1034.07 - Last mean reward per episode: -1032.97 - Last mean moves per episode: 48.29\n",
      "Saving new best model\n",
      "5020000 timesteps\n",
      "Best mean reward: -1032.97 - Last mean reward per episode: -1030.87 - Last mean moves per episode: 48.24\n",
      "Saving new best model\n",
      "5029980 timesteps\n",
      "Best mean reward: -1030.87 - Last mean reward per episode: -1030.88 - Last mean moves per episode: 48.23\n",
      "5039967 timesteps\n",
      "Best mean reward: -1030.87 - Last mean reward per episode: -1030.93 - Last mean moves per episode: 48.22\n",
      "5049984 timesteps\n",
      "Best mean reward: -1030.87 - Last mean reward per episode: -1030.10 - Last mean moves per episode: 48.19\n",
      "Saving new best model\n",
      "5059973 timesteps\n",
      "Best mean reward: -1030.10 - Last mean reward per episode: -1028.77 - Last mean moves per episode: 48.16\n",
      "Saving new best model\n",
      "5069977 timesteps\n",
      "Best mean reward: -1028.77 - Last mean reward per episode: -1028.79 - Last mean moves per episode: 48.16\n",
      "5079973 timesteps\n",
      "Best mean reward: -1028.77 - Last mean reward per episode: -1024.77 - Last mean moves per episode: 48.05\n",
      "Saving new best model\n",
      "5089944 timesteps\n",
      "Best mean reward: -1024.77 - Last mean reward per episode: -1022.58 - Last mean moves per episode: 47.99\n",
      "Saving new best model\n",
      "5099976 timesteps\n",
      "Best mean reward: -1022.58 - Last mean reward per episode: -1020.59 - Last mean moves per episode: 47.93\n",
      "Saving new best model\n",
      "5109978 timesteps\n",
      "Best mean reward: -1020.59 - Last mean reward per episode: -1019.58 - Last mean moves per episode: 47.91\n",
      "Saving new best model\n",
      "5119976 timesteps\n",
      "Best mean reward: -1019.58 - Last mean reward per episode: -1016.83 - Last mean moves per episode: 47.83\n",
      "Saving new best model\n",
      "5129979 timesteps\n",
      "Best mean reward: -1016.83 - Last mean reward per episode: -1018.67 - Last mean moves per episode: 47.88\n",
      "5139945 timesteps\n",
      "Best mean reward: -1016.83 - Last mean reward per episode: -1016.36 - Last mean moves per episode: 47.82\n",
      "Saving new best model\n",
      "5149988 timesteps\n",
      "Best mean reward: -1016.36 - Last mean reward per episode: -1013.54 - Last mean moves per episode: 47.75\n",
      "Saving new best model\n",
      "5159976 timesteps\n",
      "Best mean reward: -1013.54 - Last mean reward per episode: -1010.74 - Last mean moves per episode: 47.67\n",
      "Saving new best model\n",
      "5169924 timesteps\n",
      "Best mean reward: -1010.74 - Last mean reward per episode: -1009.64 - Last mean moves per episode: 47.64\n",
      "Saving new best model\n",
      "5179953 timesteps\n",
      "Best mean reward: -1009.64 - Last mean reward per episode: -1008.53 - Last mean moves per episode: 47.61\n",
      "Saving new best model\n",
      "5189927 timesteps\n",
      "Best mean reward: -1008.53 - Last mean reward per episode: -1004.24 - Last mean moves per episode: 47.50\n",
      "Saving new best model\n",
      "5199965 timesteps\n",
      "Best mean reward: -1004.24 - Last mean reward per episode: -1003.07 - Last mean moves per episode: 47.46\n",
      "Saving new best model\n",
      "5209937 timesteps\n",
      "Best mean reward: -1003.07 - Last mean reward per episode: -1004.70 - Last mean moves per episode: 47.49\n",
      "5219973 timesteps\n",
      "Best mean reward: -1003.07 - Last mean reward per episode: -1001.65 - Last mean moves per episode: 47.41\n",
      "Saving new best model\n",
      "5229966 timesteps\n",
      "Best mean reward: -1001.65 - Last mean reward per episode: -1001.92 - Last mean moves per episode: 47.43\n",
      "5239999 timesteps\n",
      "Best mean reward: -1001.65 - Last mean reward per episode: -1000.98 - Last mean moves per episode: 47.40\n",
      "Saving new best model\n",
      "5249996 timesteps\n",
      "Best mean reward: -1000.98 - Last mean reward per episode: -998.73 - Last mean moves per episode: 47.34\n",
      "Saving new best model\n",
      "5259995 timesteps\n",
      "Best mean reward: -998.73 - Last mean reward per episode: -997.83 - Last mean moves per episode: 47.31\n",
      "Saving new best model\n",
      "5269935 timesteps\n",
      "Best mean reward: -997.83 - Last mean reward per episode: -993.88 - Last mean moves per episode: 47.19\n",
      "Saving new best model\n",
      "5279999 timesteps\n",
      "Best mean reward: -993.88 - Last mean reward per episode: -994.24 - Last mean moves per episode: 47.20\n",
      "5289979 timesteps\n",
      "Best mean reward: -993.88 - Last mean reward per episode: -991.30 - Last mean moves per episode: 47.10\n",
      "Saving new best model\n",
      "5299992 timesteps\n",
      "Best mean reward: -991.30 - Last mean reward per episode: -988.77 - Last mean moves per episode: 47.04\n",
      "Saving new best model\n",
      "5309974 timesteps\n",
      "Best mean reward: -988.77 - Last mean reward per episode: -987.17 - Last mean moves per episode: 46.99\n",
      "Saving new best model\n",
      "5319996 timesteps\n",
      "Best mean reward: -987.17 - Last mean reward per episode: -985.34 - Last mean moves per episode: 46.92\n",
      "Saving new best model\n",
      "5329947 timesteps\n",
      "Best mean reward: -985.34 - Last mean reward per episode: -980.94 - Last mean moves per episode: 46.80\n",
      "Saving new best model\n",
      "5339992 timesteps\n",
      "Best mean reward: -980.94 - Last mean reward per episode: -978.32 - Last mean moves per episode: 46.72\n",
      "Saving new best model\n",
      "5349948 timesteps\n",
      "Best mean reward: -978.32 - Last mean reward per episode: -974.63 - Last mean moves per episode: 46.62\n",
      "Saving new best model\n",
      "5359974 timesteps\n",
      "Best mean reward: -974.63 - Last mean reward per episode: -973.89 - Last mean moves per episode: 46.58\n",
      "Saving new best model\n",
      "5369914 timesteps\n",
      "Best mean reward: -973.89 - Last mean reward per episode: -970.43 - Last mean moves per episode: 46.49\n",
      "Saving new best model\n",
      "5379970 timesteps\n",
      "Best mean reward: -970.43 - Last mean reward per episode: -970.72 - Last mean moves per episode: 46.49\n",
      "5389938 timesteps\n",
      "Best mean reward: -970.43 - Last mean reward per episode: -968.32 - Last mean moves per episode: 46.43\n",
      "Saving new best model\n",
      "5399966 timesteps\n",
      "Best mean reward: -968.32 - Last mean reward per episode: -966.27 - Last mean moves per episode: 46.37\n",
      "Saving new best model\n",
      "5409996 timesteps\n",
      "Best mean reward: -966.27 - Last mean reward per episode: -962.05 - Last mean moves per episode: 46.26\n",
      "Saving new best model\n",
      "5419988 timesteps\n",
      "Best mean reward: -962.05 - Last mean reward per episode: -960.89 - Last mean moves per episode: 46.23\n",
      "Saving new best model\n",
      "5429927 timesteps\n",
      "Best mean reward: -960.89 - Last mean reward per episode: -958.24 - Last mean moves per episode: 46.15\n",
      "Saving new best model\n",
      "5439969 timesteps\n",
      "Best mean reward: -958.24 - Last mean reward per episode: -958.05 - Last mean moves per episode: 46.15\n",
      "Saving new best model\n",
      "5449993 timesteps\n",
      "Best mean reward: -958.05 - Last mean reward per episode: -956.36 - Last mean moves per episode: 46.10\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5459975 timesteps\n",
      "Best mean reward: -956.36 - Last mean reward per episode: -955.74 - Last mean moves per episode: 46.09\n",
      "Saving new best model\n",
      "5469991 timesteps\n",
      "Best mean reward: -955.74 - Last mean reward per episode: -954.94 - Last mean moves per episode: 46.08\n",
      "Saving new best model\n",
      "5479964 timesteps\n",
      "Best mean reward: -954.94 - Last mean reward per episode: -954.29 - Last mean moves per episode: 46.06\n",
      "Saving new best model\n",
      "5489994 timesteps\n",
      "Best mean reward: -954.29 - Last mean reward per episode: -950.21 - Last mean moves per episode: 45.95\n",
      "Saving new best model\n",
      "5499970 timesteps\n",
      "Best mean reward: -950.21 - Last mean reward per episode: -948.99 - Last mean moves per episode: 45.92\n",
      "Saving new best model\n",
      "5509995 timesteps\n",
      "Best mean reward: -948.99 - Last mean reward per episode: -947.85 - Last mean moves per episode: 45.90\n",
      "Saving new best model\n",
      "5519975 timesteps\n",
      "Best mean reward: -947.85 - Last mean reward per episode: -945.14 - Last mean moves per episode: 45.83\n",
      "Saving new best model\n",
      "5529969 timesteps\n",
      "Best mean reward: -945.14 - Last mean reward per episode: -944.56 - Last mean moves per episode: 45.81\n",
      "Saving new best model\n",
      "5539995 timesteps\n",
      "Best mean reward: -944.56 - Last mean reward per episode: -945.92 - Last mean moves per episode: 45.86\n",
      "5549984 timesteps\n",
      "Best mean reward: -944.56 - Last mean reward per episode: -945.18 - Last mean moves per episode: 45.84\n",
      "5559966 timesteps\n",
      "Best mean reward: -944.56 - Last mean reward per episode: -942.77 - Last mean moves per episode: 45.78\n",
      "Saving new best model\n",
      "5569996 timesteps\n",
      "Best mean reward: -942.77 - Last mean reward per episode: -943.91 - Last mean moves per episode: 45.80\n",
      "5579985 timesteps\n",
      "Best mean reward: -942.77 - Last mean reward per episode: -944.23 - Last mean moves per episode: 45.80\n",
      "5589981 timesteps\n",
      "Best mean reward: -942.77 - Last mean reward per episode: -940.32 - Last mean moves per episode: 45.68\n",
      "Saving new best model\n",
      "5599965 timesteps\n",
      "Best mean reward: -940.32 - Last mean reward per episode: -939.63 - Last mean moves per episode: 45.66\n",
      "Saving new best model\n",
      "5609986 timesteps\n",
      "Best mean reward: -939.63 - Last mean reward per episode: -940.11 - Last mean moves per episode: 45.68\n",
      "5619967 timesteps\n",
      "Best mean reward: -939.63 - Last mean reward per episode: -938.28 - Last mean moves per episode: 45.63\n",
      "Saving new best model\n",
      "5629951 timesteps\n",
      "Best mean reward: -938.28 - Last mean reward per episode: -935.40 - Last mean moves per episode: 45.54\n",
      "Saving new best model\n",
      "5639984 timesteps\n",
      "Best mean reward: -935.40 - Last mean reward per episode: -937.46 - Last mean moves per episode: 45.61\n",
      "5649971 timesteps\n",
      "Best mean reward: -935.40 - Last mean reward per episode: -938.10 - Last mean moves per episode: 45.62\n",
      "5659977 timesteps\n",
      "Best mean reward: -935.40 - Last mean reward per episode: -938.09 - Last mean moves per episode: 45.63\n",
      "5669982 timesteps\n",
      "Best mean reward: -935.40 - Last mean reward per episode: -935.57 - Last mean moves per episode: 45.56\n",
      "5679948 timesteps\n",
      "Best mean reward: -935.40 - Last mean reward per episode: -935.62 - Last mean moves per episode: 45.56\n",
      "5689997 timesteps\n",
      "Best mean reward: -935.40 - Last mean reward per episode: -933.28 - Last mean moves per episode: 45.50\n",
      "Saving new best model\n",
      "5699970 timesteps\n",
      "Best mean reward: -933.28 - Last mean reward per episode: -931.40 - Last mean moves per episode: 45.44\n",
      "Saving new best model\n",
      "5709996 timesteps\n",
      "Best mean reward: -931.40 - Last mean reward per episode: -928.42 - Last mean moves per episode: 45.37\n",
      "Saving new best model\n",
      "5719987 timesteps\n",
      "Best mean reward: -928.42 - Last mean reward per episode: -929.73 - Last mean moves per episode: 45.41\n",
      "5729951 timesteps\n",
      "Best mean reward: -928.42 - Last mean reward per episode: -929.10 - Last mean moves per episode: 45.40\n",
      "5739990 timesteps\n",
      "Best mean reward: -928.42 - Last mean reward per episode: -930.03 - Last mean moves per episode: 45.43\n",
      "5749992 timesteps\n",
      "Best mean reward: -928.42 - Last mean reward per episode: -925.05 - Last mean moves per episode: 45.27\n",
      "Saving new best model\n",
      "5759946 timesteps\n",
      "Best mean reward: -925.05 - Last mean reward per episode: -925.21 - Last mean moves per episode: 45.27\n",
      "5769958 timesteps\n",
      "Best mean reward: -925.05 - Last mean reward per episode: -924.62 - Last mean moves per episode: 45.25\n",
      "Saving new best model\n",
      "5779989 timesteps\n",
      "Best mean reward: -924.62 - Last mean reward per episode: -923.83 - Last mean moves per episode: 45.25\n",
      "Saving new best model\n",
      "5789977 timesteps\n",
      "Best mean reward: -923.83 - Last mean reward per episode: -922.99 - Last mean moves per episode: 45.24\n",
      "Saving new best model\n",
      "5799996 timesteps\n",
      "Best mean reward: -922.99 - Last mean reward per episode: -922.39 - Last mean moves per episode: 45.23\n",
      "Saving new best model\n",
      "5809946 timesteps\n",
      "Best mean reward: -922.39 - Last mean reward per episode: -921.30 - Last mean moves per episode: 45.20\n",
      "Saving new best model\n",
      "5819985 timesteps\n",
      "Best mean reward: -921.30 - Last mean reward per episode: -917.87 - Last mean moves per episode: 45.11\n",
      "Saving new best model\n",
      "5829948 timesteps\n",
      "Best mean reward: -917.87 - Last mean reward per episode: -916.65 - Last mean moves per episode: 45.08\n",
      "Saving new best model\n",
      "5839971 timesteps\n",
      "Best mean reward: -916.65 - Last mean reward per episode: -916.69 - Last mean moves per episode: 45.08\n",
      "5849995 timesteps\n",
      "Best mean reward: -916.65 - Last mean reward per episode: -915.88 - Last mean moves per episode: 45.06\n",
      "Saving new best model\n",
      "5859968 timesteps\n",
      "Best mean reward: -915.88 - Last mean reward per episode: -912.10 - Last mean moves per episode: 44.95\n",
      "Saving new best model\n",
      "5869989 timesteps\n",
      "Best mean reward: -912.10 - Last mean reward per episode: -911.21 - Last mean moves per episode: 44.93\n",
      "Saving new best model\n",
      "5879981 timesteps\n",
      "Best mean reward: -911.21 - Last mean reward per episode: -909.50 - Last mean moves per episode: 44.91\n",
      "Saving new best model\n",
      "5889967 timesteps\n",
      "Best mean reward: -909.50 - Last mean reward per episode: -909.81 - Last mean moves per episode: 44.92\n",
      "5899959 timesteps\n",
      "Best mean reward: -909.50 - Last mean reward per episode: -908.70 - Last mean moves per episode: 44.90\n",
      "Saving new best model\n",
      "5909961 timesteps\n",
      "Best mean reward: -908.70 - Last mean reward per episode: -906.49 - Last mean moves per episode: 44.82\n",
      "Saving new best model\n",
      "5919978 timesteps\n",
      "Best mean reward: -906.49 - Last mean reward per episode: -904.56 - Last mean moves per episode: 44.77\n",
      "Saving new best model\n",
      "5929987 timesteps\n",
      "Best mean reward: -904.56 - Last mean reward per episode: -902.54 - Last mean moves per episode: 44.70\n",
      "Saving new best model\n",
      "5939995 timesteps\n",
      "Best mean reward: -902.54 - Last mean reward per episode: -904.03 - Last mean moves per episode: 44.76\n",
      "5949953 timesteps\n",
      "Best mean reward: -902.54 - Last mean reward per episode: -902.40 - Last mean moves per episode: 44.71\n",
      "Saving new best model\n",
      "5959960 timesteps\n",
      "Best mean reward: -902.40 - Last mean reward per episode: -899.48 - Last mean moves per episode: 44.63\n",
      "Saving new best model\n",
      "5969983 timesteps\n",
      "Best mean reward: -899.48 - Last mean reward per episode: -898.63 - Last mean moves per episode: 44.60\n",
      "Saving new best model\n",
      "5979960 timesteps\n",
      "Best mean reward: -898.63 - Last mean reward per episode: -896.92 - Last mean moves per episode: 44.56\n",
      "Saving new best model\n",
      "5989981 timesteps\n",
      "Best mean reward: -896.92 - Last mean reward per episode: -895.86 - Last mean moves per episode: 44.53\n",
      "Saving new best model\n",
      "5999939 timesteps\n",
      "Best mean reward: -895.86 - Last mean reward per episode: -893.44 - Last mean moves per episode: 44.47\n",
      "Saving new best model\n",
      "6009940 timesteps\n",
      "Best mean reward: -893.44 - Last mean reward per episode: -889.88 - Last mean moves per episode: 44.36\n",
      "Saving new best model\n",
      "6019988 timesteps\n",
      "Best mean reward: -889.88 - Last mean reward per episode: -888.29 - Last mean moves per episode: 44.31\n",
      "Saving new best model\n",
      "6029984 timesteps\n",
      "Best mean reward: -888.29 - Last mean reward per episode: -890.15 - Last mean moves per episode: 44.37\n",
      "6039972 timesteps\n",
      "Best mean reward: -888.29 - Last mean reward per episode: -891.13 - Last mean moves per episode: 44.40\n",
      "6049957 timesteps\n",
      "Best mean reward: -888.29 - Last mean reward per episode: -890.15 - Last mean moves per episode: 44.36\n",
      "6059956 timesteps\n",
      "Best mean reward: -888.29 - Last mean reward per episode: -887.49 - Last mean moves per episode: 44.30\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6069972 timesteps\n",
      "Best mean reward: -887.49 - Last mean reward per episode: -888.03 - Last mean moves per episode: 44.33\n",
      "6079982 timesteps\n",
      "Best mean reward: -887.49 - Last mean reward per episode: -885.03 - Last mean moves per episode: 44.25\n",
      "Saving new best model\n",
      "6089995 timesteps\n",
      "Best mean reward: -885.03 - Last mean reward per episode: -881.80 - Last mean moves per episode: 44.16\n",
      "Saving new best model\n",
      "6099981 timesteps\n",
      "Best mean reward: -881.80 - Last mean reward per episode: -879.26 - Last mean moves per episode: 44.10\n",
      "Saving new best model\n",
      "6109960 timesteps\n",
      "Best mean reward: -879.26 - Last mean reward per episode: -880.09 - Last mean moves per episode: 44.12\n",
      "6119989 timesteps\n",
      "Best mean reward: -879.26 - Last mean reward per episode: -878.57 - Last mean moves per episode: 44.09\n",
      "Saving new best model\n",
      "6129991 timesteps\n",
      "Best mean reward: -878.57 - Last mean reward per episode: -877.12 - Last mean moves per episode: 44.04\n",
      "Saving new best model\n",
      "6139962 timesteps\n",
      "Best mean reward: -877.12 - Last mean reward per episode: -875.62 - Last mean moves per episode: 43.99\n",
      "Saving new best model\n",
      "6149988 timesteps\n",
      "Best mean reward: -875.62 - Last mean reward per episode: -876.19 - Last mean moves per episode: 44.01\n",
      "6159951 timesteps\n",
      "Best mean reward: -875.62 - Last mean reward per episode: -872.22 - Last mean moves per episode: 43.90\n",
      "Saving new best model\n",
      "6169983 timesteps\n",
      "Best mean reward: -872.22 - Last mean reward per episode: -869.89 - Last mean moves per episode: 43.84\n",
      "Saving new best model\n",
      "6179979 timesteps\n",
      "Best mean reward: -869.89 - Last mean reward per episode: -868.69 - Last mean moves per episode: 43.83\n",
      "Saving new best model\n",
      "6189993 timesteps\n",
      "Best mean reward: -868.69 - Last mean reward per episode: -872.05 - Last mean moves per episode: 43.92\n",
      "6199977 timesteps\n",
      "Best mean reward: -868.69 - Last mean reward per episode: -870.02 - Last mean moves per episode: 43.88\n",
      "6209959 timesteps\n",
      "Best mean reward: -868.69 - Last mean reward per episode: -868.77 - Last mean moves per episode: 43.84\n",
      "6219999 timesteps\n",
      "Best mean reward: -868.69 - Last mean reward per episode: -867.14 - Last mean moves per episode: 43.79\n",
      "Saving new best model\n",
      "6229989 timesteps\n",
      "Best mean reward: -867.14 - Last mean reward per episode: -865.69 - Last mean moves per episode: 43.75\n",
      "Saving new best model\n",
      "6239990 timesteps\n",
      "Best mean reward: -865.69 - Last mean reward per episode: -862.89 - Last mean moves per episode: 43.68\n",
      "Saving new best model\n",
      "6249955 timesteps\n",
      "Best mean reward: -862.89 - Last mean reward per episode: -862.07 - Last mean moves per episode: 43.66\n",
      "Saving new best model\n",
      "6259969 timesteps\n",
      "Best mean reward: -862.07 - Last mean reward per episode: -861.34 - Last mean moves per episode: 43.64\n",
      "Saving new best model\n",
      "6269967 timesteps\n",
      "Best mean reward: -861.34 - Last mean reward per episode: -858.11 - Last mean moves per episode: 43.54\n",
      "Saving new best model\n",
      "6279982 timesteps\n",
      "Best mean reward: -858.11 - Last mean reward per episode: -855.26 - Last mean moves per episode: 43.47\n",
      "Saving new best model\n",
      "6289922 timesteps\n",
      "Best mean reward: -855.26 - Last mean reward per episode: -856.59 - Last mean moves per episode: 43.49\n",
      "6299978 timesteps\n",
      "Best mean reward: -855.26 - Last mean reward per episode: -858.02 - Last mean moves per episode: 43.54\n",
      "6309949 timesteps\n",
      "Best mean reward: -855.26 - Last mean reward per episode: -856.56 - Last mean moves per episode: 43.49\n",
      "6319967 timesteps\n",
      "Best mean reward: -855.26 - Last mean reward per episode: -856.99 - Last mean moves per episode: 43.50\n",
      "6329973 timesteps\n",
      "Best mean reward: -855.26 - Last mean reward per episode: -854.86 - Last mean moves per episode: 43.44\n",
      "Saving new best model\n",
      "6339980 timesteps\n",
      "Best mean reward: -854.86 - Last mean reward per episode: -854.07 - Last mean moves per episode: 43.42\n",
      "Saving new best model\n",
      "6349975 timesteps\n",
      "Best mean reward: -854.07 - Last mean reward per episode: -850.36 - Last mean moves per episode: 43.33\n",
      "Saving new best model\n",
      "6359985 timesteps\n",
      "Best mean reward: -850.36 - Last mean reward per episode: -851.29 - Last mean moves per episode: 43.36\n",
      "6369970 timesteps\n",
      "Best mean reward: -850.36 - Last mean reward per episode: -849.28 - Last mean moves per episode: 43.29\n",
      "Saving new best model\n",
      "6379983 timesteps\n",
      "Best mean reward: -849.28 - Last mean reward per episode: -848.69 - Last mean moves per episode: 43.28\n",
      "Saving new best model\n",
      "6389970 timesteps\n",
      "Best mean reward: -848.69 - Last mean reward per episode: -848.25 - Last mean moves per episode: 43.26\n",
      "Saving new best model\n",
      "6399959 timesteps\n",
      "Best mean reward: -848.25 - Last mean reward per episode: -847.37 - Last mean moves per episode: 43.24\n",
      "Saving new best model\n",
      "6409981 timesteps\n",
      "Best mean reward: -847.37 - Last mean reward per episode: -848.18 - Last mean moves per episode: 43.27\n",
      "6419983 timesteps\n",
      "Best mean reward: -847.37 - Last mean reward per episode: -845.81 - Last mean moves per episode: 43.21\n",
      "Saving new best model\n",
      "6429965 timesteps\n",
      "Best mean reward: -845.81 - Last mean reward per episode: -845.64 - Last mean moves per episode: 43.21\n",
      "Saving new best model\n",
      "6439995 timesteps\n",
      "Best mean reward: -845.64 - Last mean reward per episode: -842.13 - Last mean moves per episode: 43.12\n",
      "Saving new best model\n",
      "6449980 timesteps\n",
      "Best mean reward: -842.13 - Last mean reward per episode: -843.12 - Last mean moves per episode: 43.15\n",
      "6459997 timesteps\n",
      "Best mean reward: -842.13 - Last mean reward per episode: -840.73 - Last mean moves per episode: 43.08\n",
      "Saving new best model\n",
      "6469984 timesteps\n",
      "Best mean reward: -840.73 - Last mean reward per episode: -837.62 - Last mean moves per episode: 42.99\n",
      "Saving new best model\n",
      "6479986 timesteps\n",
      "Best mean reward: -837.62 - Last mean reward per episode: -838.14 - Last mean moves per episode: 43.00\n",
      "6489933 timesteps\n",
      "Best mean reward: -837.62 - Last mean reward per episode: -836.29 - Last mean moves per episode: 42.95\n",
      "Saving new best model\n",
      "6499996 timesteps\n",
      "Best mean reward: -836.29 - Last mean reward per episode: -834.71 - Last mean moves per episode: 42.91\n",
      "Saving new best model\n",
      "6509998 timesteps\n",
      "Best mean reward: -834.71 - Last mean reward per episode: -831.75 - Last mean moves per episode: 42.82\n",
      "Saving new best model\n",
      "6519988 timesteps\n",
      "Best mean reward: -831.75 - Last mean reward per episode: -830.69 - Last mean moves per episode: 42.80\n",
      "Saving new best model\n",
      "6529985 timesteps\n",
      "Best mean reward: -830.69 - Last mean reward per episode: -827.76 - Last mean moves per episode: 42.71\n",
      "Saving new best model\n",
      "6539981 timesteps\n",
      "Best mean reward: -827.76 - Last mean reward per episode: -825.14 - Last mean moves per episode: 42.63\n",
      "Saving new best model\n",
      "6549998 timesteps\n",
      "Best mean reward: -825.14 - Last mean reward per episode: -825.17 - Last mean moves per episode: 42.65\n",
      "6559984 timesteps\n",
      "Best mean reward: -825.14 - Last mean reward per episode: -823.77 - Last mean moves per episode: 42.60\n",
      "Saving new best model\n",
      "6569996 timesteps\n",
      "Best mean reward: -823.77 - Last mean reward per episode: -823.81 - Last mean moves per episode: 42.61\n",
      "6579950 timesteps\n",
      "Best mean reward: -823.77 - Last mean reward per episode: -822.09 - Last mean moves per episode: 42.56\n",
      "Saving new best model\n",
      "6589941 timesteps\n",
      "Best mean reward: -822.09 - Last mean reward per episode: -821.78 - Last mean moves per episode: 42.53\n",
      "Saving new best model\n",
      "6599968 timesteps\n",
      "Best mean reward: -821.78 - Last mean reward per episode: -820.86 - Last mean moves per episode: 42.50\n",
      "Saving new best model\n",
      "6609992 timesteps\n",
      "Best mean reward: -820.86 - Last mean reward per episode: -817.20 - Last mean moves per episode: 42.41\n",
      "Saving new best model\n",
      "6619964 timesteps\n",
      "Best mean reward: -817.20 - Last mean reward per episode: -814.28 - Last mean moves per episode: 42.32\n",
      "Saving new best model\n",
      "6629993 timesteps\n",
      "Best mean reward: -814.28 - Last mean reward per episode: -816.19 - Last mean moves per episode: 42.38\n",
      "6639966 timesteps\n",
      "Best mean reward: -814.28 - Last mean reward per episode: -817.11 - Last mean moves per episode: 42.40\n",
      "6649978 timesteps\n",
      "Best mean reward: -814.28 - Last mean reward per episode: -814.78 - Last mean moves per episode: 42.33\n",
      "6659950 timesteps\n",
      "Best mean reward: -814.28 - Last mean reward per episode: -815.14 - Last mean moves per episode: 42.33\n",
      "6669900 timesteps\n",
      "Best mean reward: -814.28 - Last mean reward per episode: -812.51 - Last mean moves per episode: 42.25\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6679962 timesteps\n",
      "Best mean reward: -812.51 - Last mean reward per episode: -813.43 - Last mean moves per episode: 42.28\n",
      "6689900 timesteps\n",
      "Best mean reward: -812.51 - Last mean reward per episode: -812.99 - Last mean moves per episode: 42.26\n",
      "6699957 timesteps\n",
      "Best mean reward: -812.51 - Last mean reward per episode: -812.37 - Last mean moves per episode: 42.26\n",
      "Saving new best model\n",
      "6709986 timesteps\n",
      "Best mean reward: -812.37 - Last mean reward per episode: -809.43 - Last mean moves per episode: 42.17\n",
      "Saving new best model\n",
      "6719984 timesteps\n",
      "Best mean reward: -809.43 - Last mean reward per episode: -807.87 - Last mean moves per episode: 42.12\n",
      "Saving new best model\n",
      "6729983 timesteps\n",
      "Best mean reward: -807.87 - Last mean reward per episode: -805.74 - Last mean moves per episode: 42.07\n",
      "Saving new best model\n",
      "6739979 timesteps\n",
      "Best mean reward: -805.74 - Last mean reward per episode: -803.86 - Last mean moves per episode: 42.01\n",
      "Saving new best model\n",
      "6749957 timesteps\n",
      "Best mean reward: -803.86 - Last mean reward per episode: -802.98 - Last mean moves per episode: 41.97\n",
      "Saving new best model\n",
      "6759955 timesteps\n",
      "Best mean reward: -802.98 - Last mean reward per episode: -803.47 - Last mean moves per episode: 41.98\n",
      "6769947 timesteps\n",
      "Best mean reward: -802.98 - Last mean reward per episode: -804.13 - Last mean moves per episode: 42.00\n",
      "6779946 timesteps\n",
      "Best mean reward: -802.98 - Last mean reward per episode: -802.03 - Last mean moves per episode: 41.94\n",
      "Saving new best model\n",
      "6789979 timesteps\n",
      "Best mean reward: -802.03 - Last mean reward per episode: -799.55 - Last mean moves per episode: 41.89\n",
      "Saving new best model\n",
      "6799928 timesteps\n",
      "Best mean reward: -799.55 - Last mean reward per episode: -798.43 - Last mean moves per episode: 41.85\n",
      "Saving new best model\n",
      "6809971 timesteps\n",
      "Best mean reward: -798.43 - Last mean reward per episode: -797.90 - Last mean moves per episode: 41.85\n",
      "Saving new best model\n",
      "6819979 timesteps\n",
      "Best mean reward: -797.90 - Last mean reward per episode: -794.67 - Last mean moves per episode: 41.74\n",
      "Saving new best model\n",
      "6829971 timesteps\n",
      "Best mean reward: -794.67 - Last mean reward per episode: -793.36 - Last mean moves per episode: 41.70\n",
      "Saving new best model\n",
      "6839983 timesteps\n",
      "Best mean reward: -793.36 - Last mean reward per episode: -793.41 - Last mean moves per episode: 41.70\n",
      "6849959 timesteps\n",
      "Best mean reward: -793.36 - Last mean reward per episode: -789.68 - Last mean moves per episode: 41.58\n",
      "Saving new best model\n",
      "6859954 timesteps\n",
      "Best mean reward: -789.68 - Last mean reward per episode: -789.63 - Last mean moves per episode: 41.58\n",
      "Saving new best model\n",
      "6869980 timesteps\n",
      "Best mean reward: -789.63 - Last mean reward per episode: -786.92 - Last mean moves per episode: 41.50\n",
      "Saving new best model\n",
      "6879974 timesteps\n",
      "Best mean reward: -786.92 - Last mean reward per episode: -785.19 - Last mean moves per episode: 41.45\n",
      "Saving new best model\n",
      "6889967 timesteps\n",
      "Best mean reward: -785.19 - Last mean reward per episode: -782.98 - Last mean moves per episode: 41.40\n",
      "Saving new best model\n",
      "6900000 timesteps\n",
      "Best mean reward: -782.98 - Last mean reward per episode: -780.39 - Last mean moves per episode: 41.31\n",
      "Saving new best model\n",
      "6909993 timesteps\n",
      "Best mean reward: -780.39 - Last mean reward per episode: -779.68 - Last mean moves per episode: 41.28\n",
      "Saving new best model\n",
      "6919997 timesteps\n",
      "Best mean reward: -779.68 - Last mean reward per episode: -780.27 - Last mean moves per episode: 41.30\n",
      "6929982 timesteps\n",
      "Best mean reward: -779.68 - Last mean reward per episode: -782.50 - Last mean moves per episode: 41.35\n",
      "6939983 timesteps\n",
      "Best mean reward: -779.68 - Last mean reward per episode: -780.30 - Last mean moves per episode: 41.29\n",
      "6949940 timesteps\n",
      "Best mean reward: -779.68 - Last mean reward per episode: -779.74 - Last mean moves per episode: 41.26\n",
      "6959961 timesteps\n",
      "Best mean reward: -779.68 - Last mean reward per episode: -776.54 - Last mean moves per episode: 41.15\n",
      "Saving new best model\n",
      "6969955 timesteps\n",
      "Best mean reward: -776.54 - Last mean reward per episode: -775.43 - Last mean moves per episode: 41.13\n",
      "Saving new best model\n",
      "6979987 timesteps\n",
      "Best mean reward: -775.43 - Last mean reward per episode: -774.34 - Last mean moves per episode: 41.10\n",
      "Saving new best model\n",
      "6989971 timesteps\n",
      "Best mean reward: -774.34 - Last mean reward per episode: -771.33 - Last mean moves per episode: 41.02\n",
      "Saving new best model\n",
      "6999976 timesteps\n",
      "Best mean reward: -771.33 - Last mean reward per episode: -771.10 - Last mean moves per episode: 41.02\n",
      "Saving new best model\n",
      "7009973 timesteps\n",
      "Best mean reward: -771.10 - Last mean reward per episode: -768.80 - Last mean moves per episode: 40.96\n",
      "Saving new best model\n",
      "7019948 timesteps\n",
      "Best mean reward: -768.80 - Last mean reward per episode: -769.19 - Last mean moves per episode: 40.96\n",
      "7029971 timesteps\n",
      "Best mean reward: -768.80 - Last mean reward per episode: -769.01 - Last mean moves per episode: 40.95\n",
      "7039998 timesteps\n",
      "Best mean reward: -768.80 - Last mean reward per episode: -765.53 - Last mean moves per episode: 40.84\n",
      "Saving new best model\n",
      "7049965 timesteps\n",
      "Best mean reward: -765.53 - Last mean reward per episode: -763.77 - Last mean moves per episode: 40.79\n",
      "Saving new best model\n",
      "7059975 timesteps\n",
      "Best mean reward: -763.77 - Last mean reward per episode: -760.80 - Last mean moves per episode: 40.70\n",
      "Saving new best model\n",
      "7069980 timesteps\n",
      "Best mean reward: -760.80 - Last mean reward per episode: -759.79 - Last mean moves per episode: 40.68\n",
      "Saving new best model\n",
      "7079986 timesteps\n",
      "Best mean reward: -759.79 - Last mean reward per episode: -759.11 - Last mean moves per episode: 40.66\n",
      "Saving new best model\n",
      "7089981 timesteps\n",
      "Best mean reward: -759.11 - Last mean reward per episode: -757.17 - Last mean moves per episode: 40.60\n",
      "Saving new best model\n",
      "7099974 timesteps\n",
      "Best mean reward: -757.17 - Last mean reward per episode: -756.51 - Last mean moves per episode: 40.58\n",
      "Saving new best model\n",
      "7109998 timesteps\n",
      "Best mean reward: -756.51 - Last mean reward per episode: -755.21 - Last mean moves per episode: 40.55\n",
      "Saving new best model\n",
      "7119973 timesteps\n",
      "Best mean reward: -755.21 - Last mean reward per episode: -754.17 - Last mean moves per episode: 40.53\n",
      "Saving new best model\n",
      "7129971 timesteps\n",
      "Best mean reward: -754.17 - Last mean reward per episode: -753.38 - Last mean moves per episode: 40.50\n",
      "Saving new best model\n",
      "7139957 timesteps\n",
      "Best mean reward: -753.38 - Last mean reward per episode: -752.23 - Last mean moves per episode: 40.47\n",
      "Saving new best model\n",
      "7149922 timesteps\n",
      "Best mean reward: -752.23 - Last mean reward per episode: -746.10 - Last mean moves per episode: 40.32\n",
      "Saving new best model\n",
      "7159999 timesteps\n",
      "Best mean reward: -746.10 - Last mean reward per episode: -742.72 - Last mean moves per episode: 40.21\n",
      "Saving new best model\n",
      "7169985 timesteps\n",
      "Best mean reward: -742.72 - Last mean reward per episode: -738.83 - Last mean moves per episode: 40.10\n",
      "Saving new best model\n",
      "7179973 timesteps\n",
      "Best mean reward: -738.83 - Last mean reward per episode: -738.19 - Last mean moves per episode: 40.08\n",
      "Saving new best model\n",
      "7189991 timesteps\n",
      "Best mean reward: -738.19 - Last mean reward per episode: -735.77 - Last mean moves per episode: 40.02\n",
      "Saving new best model\n",
      "7199965 timesteps\n",
      "Best mean reward: -735.77 - Last mean reward per episode: -733.66 - Last mean moves per episode: 39.94\n",
      "Saving new best model\n",
      "7209976 timesteps\n",
      "Best mean reward: -733.66 - Last mean reward per episode: -732.39 - Last mean moves per episode: 39.90\n",
      "Saving new best model\n",
      "7219972 timesteps\n",
      "Best mean reward: -732.39 - Last mean reward per episode: -730.71 - Last mean moves per episode: 39.86\n",
      "Saving new best model\n",
      "7229970 timesteps\n",
      "Best mean reward: -730.71 - Last mean reward per episode: -728.35 - Last mean moves per episode: 39.79\n",
      "Saving new best model\n",
      "7239989 timesteps\n",
      "Best mean reward: -728.35 - Last mean reward per episode: -727.18 - Last mean moves per episode: 39.76\n",
      "Saving new best model\n",
      "7249999 timesteps\n",
      "Best mean reward: -727.18 - Last mean reward per episode: -727.66 - Last mean moves per episode: 39.78\n",
      "7259997 timesteps\n",
      "Best mean reward: -727.18 - Last mean reward per episode: -727.77 - Last mean moves per episode: 39.78\n",
      "7269974 timesteps\n",
      "Best mean reward: -727.18 - Last mean reward per episode: -724.09 - Last mean moves per episode: 39.68\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7279998 timesteps\n",
      "Best mean reward: -724.09 - Last mean reward per episode: -722.46 - Last mean moves per episode: 39.63\n",
      "Saving new best model\n",
      "7289975 timesteps\n",
      "Best mean reward: -722.46 - Last mean reward per episode: -721.35 - Last mean moves per episode: 39.60\n",
      "Saving new best model\n",
      "7299993 timesteps\n",
      "Best mean reward: -721.35 - Last mean reward per episode: -718.04 - Last mean moves per episode: 39.53\n",
      "Saving new best model\n",
      "7309971 timesteps\n",
      "Best mean reward: -718.04 - Last mean reward per episode: -715.56 - Last mean moves per episode: 39.46\n",
      "Saving new best model\n",
      "7319973 timesteps\n",
      "Best mean reward: -715.56 - Last mean reward per episode: -711.83 - Last mean moves per episode: 39.36\n",
      "Saving new best model\n",
      "7329978 timesteps\n",
      "Best mean reward: -711.83 - Last mean reward per episode: -711.08 - Last mean moves per episode: 39.36\n",
      "Saving new best model\n",
      "7339975 timesteps\n",
      "Best mean reward: -711.08 - Last mean reward per episode: -711.80 - Last mean moves per episode: 39.37\n",
      "7350000 timesteps\n",
      "Best mean reward: -711.08 - Last mean reward per episode: -712.65 - Last mean moves per episode: 39.40\n",
      "7359969 timesteps\n",
      "Best mean reward: -711.08 - Last mean reward per episode: -710.82 - Last mean moves per episode: 39.35\n",
      "Saving new best model\n",
      "7369968 timesteps\n",
      "Best mean reward: -710.82 - Last mean reward per episode: -708.22 - Last mean moves per episode: 39.28\n",
      "Saving new best model\n",
      "7379996 timesteps\n",
      "Best mean reward: -708.22 - Last mean reward per episode: -706.96 - Last mean moves per episode: 39.24\n",
      "Saving new best model\n",
      "7389946 timesteps\n",
      "Best mean reward: -706.96 - Last mean reward per episode: -707.50 - Last mean moves per episode: 39.25\n",
      "7399999 timesteps\n",
      "Best mean reward: -706.96 - Last mean reward per episode: -705.73 - Last mean moves per episode: 39.20\n",
      "Saving new best model\n",
      "7409983 timesteps\n",
      "Best mean reward: -705.73 - Last mean reward per episode: -706.42 - Last mean moves per episode: 39.22\n",
      "7419981 timesteps\n",
      "Best mean reward: -705.73 - Last mean reward per episode: -705.92 - Last mean moves per episode: 39.20\n",
      "7429973 timesteps\n",
      "Best mean reward: -705.73 - Last mean reward per episode: -705.60 - Last mean moves per episode: 39.20\n",
      "Saving new best model\n",
      "7439940 timesteps\n",
      "Best mean reward: -705.60 - Last mean reward per episode: -703.18 - Last mean moves per episode: 39.14\n",
      "Saving new best model\n",
      "7449992 timesteps\n",
      "Best mean reward: -703.18 - Last mean reward per episode: -702.43 - Last mean moves per episode: 39.13\n",
      "Saving new best model\n",
      "7459996 timesteps\n",
      "Best mean reward: -702.43 - Last mean reward per episode: -699.18 - Last mean moves per episode: 39.03\n",
      "Saving new best model\n",
      "7469959 timesteps\n",
      "Best mean reward: -699.18 - Last mean reward per episode: -696.27 - Last mean moves per episode: 38.95\n",
      "Saving new best model\n",
      "7479987 timesteps\n",
      "Best mean reward: -696.27 - Last mean reward per episode: -694.78 - Last mean moves per episode: 38.90\n",
      "Saving new best model\n",
      "7489988 timesteps\n",
      "Best mean reward: -694.78 - Last mean reward per episode: -691.82 - Last mean moves per episode: 38.81\n",
      "Saving new best model\n",
      "7499995 timesteps\n",
      "Best mean reward: -691.82 - Last mean reward per episode: -690.59 - Last mean moves per episode: 38.76\n",
      "Saving new best model\n",
      "7509970 timesteps\n",
      "Best mean reward: -690.59 - Last mean reward per episode: -690.75 - Last mean moves per episode: 38.76\n",
      "7519976 timesteps\n",
      "Best mean reward: -690.59 - Last mean reward per episode: -690.40 - Last mean moves per episode: 38.74\n",
      "Saving new best model\n",
      "7529965 timesteps\n",
      "Best mean reward: -690.40 - Last mean reward per episode: -689.41 - Last mean moves per episode: 38.70\n",
      "Saving new best model\n",
      "7539978 timesteps\n",
      "Best mean reward: -689.41 - Last mean reward per episode: -688.19 - Last mean moves per episode: 38.66\n",
      "Saving new best model\n",
      "7549986 timesteps\n",
      "Best mean reward: -688.19 - Last mean reward per episode: -689.94 - Last mean moves per episode: 38.70\n",
      "7559974 timesteps\n",
      "Best mean reward: -688.19 - Last mean reward per episode: -690.73 - Last mean moves per episode: 38.70\n",
      "7569968 timesteps\n",
      "Best mean reward: -688.19 - Last mean reward per episode: -691.93 - Last mean moves per episode: 38.73\n",
      "7579968 timesteps\n",
      "Best mean reward: -688.19 - Last mean reward per episode: -688.61 - Last mean moves per episode: 38.64\n",
      "7589997 timesteps\n",
      "Best mean reward: -688.19 - Last mean reward per episode: -688.22 - Last mean moves per episode: 38.64\n",
      "7599981 timesteps\n",
      "Best mean reward: -688.19 - Last mean reward per episode: -687.71 - Last mean moves per episode: 38.62\n",
      "Saving new best model\n",
      "7609885 timesteps\n",
      "Best mean reward: -687.71 - Last mean reward per episode: -688.73 - Last mean moves per episode: 38.65\n",
      "7619991 timesteps\n",
      "Best mean reward: -687.71 - Last mean reward per episode: -687.92 - Last mean moves per episode: 38.63\n",
      "7629997 timesteps\n",
      "Best mean reward: -687.71 - Last mean reward per episode: -684.12 - Last mean moves per episode: 38.54\n",
      "Saving new best model\n",
      "7639987 timesteps\n",
      "Best mean reward: -684.12 - Last mean reward per episode: -681.96 - Last mean moves per episode: 38.48\n",
      "Saving new best model\n",
      "7649957 timesteps\n",
      "Best mean reward: -681.96 - Last mean reward per episode: -680.12 - Last mean moves per episode: 38.43\n",
      "Saving new best model\n",
      "7659974 timesteps\n",
      "Best mean reward: -680.12 - Last mean reward per episode: -678.36 - Last mean moves per episode: 38.38\n",
      "Saving new best model\n",
      "7669942 timesteps\n",
      "Best mean reward: -678.36 - Last mean reward per episode: -678.65 - Last mean moves per episode: 38.38\n",
      "7679969 timesteps\n",
      "Best mean reward: -678.36 - Last mean reward per episode: -677.32 - Last mean moves per episode: 38.34\n",
      "Saving new best model\n",
      "7689991 timesteps\n",
      "Best mean reward: -677.32 - Last mean reward per episode: -676.68 - Last mean moves per episode: 38.30\n",
      "Saving new best model\n",
      "7699978 timesteps\n",
      "Best mean reward: -676.68 - Last mean reward per episode: -674.82 - Last mean moves per episode: 38.25\n",
      "Saving new best model\n",
      "7709979 timesteps\n",
      "Best mean reward: -674.82 - Last mean reward per episode: -672.22 - Last mean moves per episode: 38.17\n",
      "Saving new best model\n",
      "7719986 timesteps\n",
      "Best mean reward: -672.22 - Last mean reward per episode: -669.40 - Last mean moves per episode: 38.08\n",
      "Saving new best model\n",
      "7729990 timesteps\n",
      "Best mean reward: -669.40 - Last mean reward per episode: -668.10 - Last mean moves per episode: 38.06\n",
      "Saving new best model\n",
      "7739985 timesteps\n",
      "Best mean reward: -668.10 - Last mean reward per episode: -665.91 - Last mean moves per episode: 37.99\n",
      "Saving new best model\n",
      "7749990 timesteps\n",
      "Best mean reward: -665.91 - Last mean reward per episode: -664.15 - Last mean moves per episode: 37.92\n",
      "Saving new best model\n",
      "7759982 timesteps\n",
      "Best mean reward: -664.15 - Last mean reward per episode: -664.20 - Last mean moves per episode: 37.91\n",
      "7769976 timesteps\n",
      "Best mean reward: -664.15 - Last mean reward per episode: -661.90 - Last mean moves per episode: 37.86\n",
      "Saving new best model\n",
      "7779996 timesteps\n",
      "Best mean reward: -661.90 - Last mean reward per episode: -660.37 - Last mean moves per episode: 37.79\n",
      "Saving new best model\n",
      "7789996 timesteps\n",
      "Best mean reward: -660.37 - Last mean reward per episode: -656.71 - Last mean moves per episode: 37.69\n",
      "Saving new best model\n",
      "7799987 timesteps\n",
      "Best mean reward: -656.71 - Last mean reward per episode: -655.35 - Last mean moves per episode: 37.66\n",
      "Saving new best model\n",
      "7809986 timesteps\n",
      "Best mean reward: -655.35 - Last mean reward per episode: -654.02 - Last mean moves per episode: 37.63\n",
      "Saving new best model\n",
      "7819974 timesteps\n",
      "Best mean reward: -654.02 - Last mean reward per episode: -652.97 - Last mean moves per episode: 37.59\n",
      "Saving new best model\n",
      "7829969 timesteps\n",
      "Best mean reward: -652.97 - Last mean reward per episode: -654.02 - Last mean moves per episode: 37.61\n",
      "7839996 timesteps\n",
      "Best mean reward: -652.97 - Last mean reward per episode: -655.10 - Last mean moves per episode: 37.65\n",
      "7849978 timesteps\n",
      "Best mean reward: -652.97 - Last mean reward per episode: -654.97 - Last mean moves per episode: 37.64\n",
      "7859993 timesteps\n",
      "Best mean reward: -652.97 - Last mean reward per episode: -655.45 - Last mean moves per episode: 37.66\n",
      "7869994 timesteps\n",
      "Best mean reward: -652.97 - Last mean reward per episode: -655.49 - Last mean moves per episode: 37.66\n",
      "7879962 timesteps\n",
      "Best mean reward: -652.97 - Last mean reward per episode: -655.85 - Last mean moves per episode: 37.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7889993 timesteps\n",
      "Best mean reward: -652.97 - Last mean reward per episode: -651.85 - Last mean moves per episode: 37.56\n",
      "Saving new best model\n",
      "7899972 timesteps\n",
      "Best mean reward: -651.85 - Last mean reward per episode: -651.15 - Last mean moves per episode: 37.54\n",
      "Saving new best model\n",
      "7909995 timesteps\n",
      "Best mean reward: -651.15 - Last mean reward per episode: -650.37 - Last mean moves per episode: 37.51\n",
      "Saving new best model\n",
      "7919987 timesteps\n",
      "Best mean reward: -650.37 - Last mean reward per episode: -647.67 - Last mean moves per episode: 37.42\n",
      "Saving new best model\n",
      "7929995 timesteps\n",
      "Best mean reward: -647.67 - Last mean reward per episode: -645.42 - Last mean moves per episode: 37.35\n",
      "Saving new best model\n",
      "7939988 timesteps\n",
      "Best mean reward: -645.42 - Last mean reward per episode: -643.32 - Last mean moves per episode: 37.29\n",
      "Saving new best model\n",
      "7949978 timesteps\n",
      "Best mean reward: -643.32 - Last mean reward per episode: -642.76 - Last mean moves per episode: 37.24\n",
      "Saving new best model\n",
      "7959974 timesteps\n",
      "Best mean reward: -642.76 - Last mean reward per episode: -641.91 - Last mean moves per episode: 37.21\n",
      "Saving new best model\n",
      "7969976 timesteps\n",
      "Best mean reward: -641.91 - Last mean reward per episode: -639.46 - Last mean moves per episode: 37.14\n",
      "Saving new best model\n",
      "7979969 timesteps\n",
      "Best mean reward: -639.46 - Last mean reward per episode: -637.24 - Last mean moves per episode: 37.07\n",
      "Saving new best model\n",
      "7989993 timesteps\n",
      "Best mean reward: -637.24 - Last mean reward per episode: -635.14 - Last mean moves per episode: 36.97\n",
      "Saving new best model\n",
      "7999980 timesteps\n",
      "Best mean reward: -635.14 - Last mean reward per episode: -635.12 - Last mean moves per episode: 36.96\n",
      "Saving new best model\n",
      "8009971 timesteps\n",
      "Best mean reward: -635.12 - Last mean reward per episode: -636.41 - Last mean moves per episode: 36.98\n",
      "8019950 timesteps\n",
      "Best mean reward: -635.12 - Last mean reward per episode: -636.01 - Last mean moves per episode: 36.96\n",
      "8030000 timesteps\n",
      "Best mean reward: -635.12 - Last mean reward per episode: -636.26 - Last mean moves per episode: 36.95\n",
      "8039988 timesteps\n",
      "Best mean reward: -635.12 - Last mean reward per episode: -634.94 - Last mean moves per episode: 36.91\n",
      "Saving new best model\n",
      "8049928 timesteps\n",
      "Best mean reward: -634.94 - Last mean reward per episode: -633.45 - Last mean moves per episode: 36.86\n",
      "Saving new best model\n",
      "8059988 timesteps\n",
      "Best mean reward: -633.45 - Last mean reward per episode: -633.08 - Last mean moves per episode: 36.84\n",
      "Saving new best model\n",
      "8069970 timesteps\n",
      "Best mean reward: -633.08 - Last mean reward per episode: -633.23 - Last mean moves per episode: 36.83\n",
      "8079988 timesteps\n",
      "Best mean reward: -633.08 - Last mean reward per episode: -632.74 - Last mean moves per episode: 36.82\n",
      "Saving new best model\n",
      "8089979 timesteps\n",
      "Best mean reward: -632.74 - Last mean reward per episode: -631.12 - Last mean moves per episode: 36.77\n",
      "Saving new best model\n",
      "8099989 timesteps\n",
      "Best mean reward: -631.12 - Last mean reward per episode: -629.50 - Last mean moves per episode: 36.71\n",
      "Saving new best model\n",
      "8109974 timesteps\n",
      "Best mean reward: -629.50 - Last mean reward per episode: -630.22 - Last mean moves per episode: 36.74\n",
      "8120000 timesteps\n",
      "Best mean reward: -629.50 - Last mean reward per episode: -627.01 - Last mean moves per episode: 36.64\n",
      "Saving new best model\n",
      "8129965 timesteps\n",
      "Best mean reward: -627.01 - Last mean reward per episode: -626.99 - Last mean moves per episode: 36.63\n",
      "Saving new best model\n",
      "8139980 timesteps\n",
      "Best mean reward: -626.99 - Last mean reward per episode: -625.16 - Last mean moves per episode: 36.58\n",
      "Saving new best model\n",
      "8149953 timesteps\n",
      "Best mean reward: -625.16 - Last mean reward per episode: -624.63 - Last mean moves per episode: 36.56\n",
      "Saving new best model\n",
      "8159979 timesteps\n",
      "Best mean reward: -624.63 - Last mean reward per episode: -622.91 - Last mean moves per episode: 36.51\n",
      "Saving new best model\n",
      "8169981 timesteps\n",
      "Best mean reward: -622.91 - Last mean reward per episode: -619.77 - Last mean moves per episode: 36.40\n",
      "Saving new best model\n",
      "8179906 timesteps\n",
      "Best mean reward: -619.77 - Last mean reward per episode: -617.52 - Last mean moves per episode: 36.33\n",
      "Saving new best model\n",
      "8189975 timesteps\n",
      "Best mean reward: -617.52 - Last mean reward per episode: -618.78 - Last mean moves per episode: 36.36\n",
      "8199968 timesteps\n",
      "Best mean reward: -617.52 - Last mean reward per episode: -615.60 - Last mean moves per episode: 36.27\n",
      "Saving new best model\n",
      "8209991 timesteps\n",
      "Best mean reward: -615.60 - Last mean reward per episode: -615.50 - Last mean moves per episode: 36.26\n",
      "Saving new best model\n",
      "8219993 timesteps\n",
      "Best mean reward: -615.50 - Last mean reward per episode: -614.04 - Last mean moves per episode: 36.20\n",
      "Saving new best model\n",
      "8229937 timesteps\n",
      "Best mean reward: -614.04 - Last mean reward per episode: -612.12 - Last mean moves per episode: 36.14\n",
      "Saving new best model\n",
      "8239954 timesteps\n",
      "Best mean reward: -612.12 - Last mean reward per episode: -609.40 - Last mean moves per episode: 36.06\n",
      "Saving new best model\n",
      "8249984 timesteps\n",
      "Best mean reward: -609.40 - Last mean reward per episode: -610.63 - Last mean moves per episode: 36.10\n",
      "8259988 timesteps\n",
      "Best mean reward: -609.40 - Last mean reward per episode: -609.79 - Last mean moves per episode: 36.06\n",
      "8269995 timesteps\n",
      "Best mean reward: -609.40 - Last mean reward per episode: -608.62 - Last mean moves per episode: 36.04\n",
      "Saving new best model\n",
      "8279989 timesteps\n",
      "Best mean reward: -608.62 - Last mean reward per episode: -608.88 - Last mean moves per episode: 36.05\n",
      "8289986 timesteps\n",
      "Best mean reward: -608.62 - Last mean reward per episode: -606.51 - Last mean moves per episode: 35.99\n",
      "Saving new best model\n",
      "8299949 timesteps\n",
      "Best mean reward: -606.51 - Last mean reward per episode: -601.98 - Last mean moves per episode: 35.85\n",
      "Saving new best model\n",
      "8309974 timesteps\n",
      "Best mean reward: -601.98 - Last mean reward per episode: -600.96 - Last mean moves per episode: 35.83\n",
      "Saving new best model\n",
      "8319992 timesteps\n",
      "Best mean reward: -600.96 - Last mean reward per episode: -599.22 - Last mean moves per episode: 35.78\n",
      "Saving new best model\n",
      "8329992 timesteps\n",
      "Best mean reward: -599.22 - Last mean reward per episode: -598.73 - Last mean moves per episode: 35.76\n",
      "Saving new best model\n",
      "8339981 timesteps\n",
      "Best mean reward: -598.73 - Last mean reward per episode: -599.79 - Last mean moves per episode: 35.79\n",
      "8349965 timesteps\n",
      "Best mean reward: -598.73 - Last mean reward per episode: -598.38 - Last mean moves per episode: 35.76\n",
      "Saving new best model\n",
      "8359977 timesteps\n",
      "Best mean reward: -598.38 - Last mean reward per episode: -597.99 - Last mean moves per episode: 35.75\n",
      "Saving new best model\n",
      "8370000 timesteps\n",
      "Best mean reward: -597.99 - Last mean reward per episode: -596.95 - Last mean moves per episode: 35.73\n",
      "Saving new best model\n",
      "8379983 timesteps\n",
      "Best mean reward: -596.95 - Last mean reward per episode: -595.35 - Last mean moves per episode: 35.68\n",
      "Saving new best model\n",
      "8389968 timesteps\n",
      "Best mean reward: -595.35 - Last mean reward per episode: -593.27 - Last mean moves per episode: 35.60\n",
      "Saving new best model\n",
      "8399991 timesteps\n",
      "Best mean reward: -593.27 - Last mean reward per episode: -592.50 - Last mean moves per episode: 35.57\n",
      "Saving new best model\n",
      "8409991 timesteps\n",
      "Best mean reward: -592.50 - Last mean reward per episode: -590.99 - Last mean moves per episode: 35.54\n",
      "Saving new best model\n",
      "8419999 timesteps\n",
      "Best mean reward: -590.99 - Last mean reward per episode: -589.01 - Last mean moves per episode: 35.50\n",
      "Saving new best model\n",
      "8429978 timesteps\n",
      "Best mean reward: -589.01 - Last mean reward per episode: -588.40 - Last mean moves per episode: 35.49\n",
      "Saving new best model\n",
      "8439959 timesteps\n",
      "Best mean reward: -588.40 - Last mean reward per episode: -586.18 - Last mean moves per episode: 35.42\n",
      "Saving new best model\n",
      "8449980 timesteps\n",
      "Best mean reward: -586.18 - Last mean reward per episode: -586.24 - Last mean moves per episode: 35.41\n",
      "8459999 timesteps\n",
      "Best mean reward: -586.18 - Last mean reward per episode: -584.97 - Last mean moves per episode: 35.37\n",
      "Saving new best model\n",
      "8469993 timesteps\n",
      "Best mean reward: -584.97 - Last mean reward per episode: -583.74 - Last mean moves per episode: 35.33\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8479961 timesteps\n",
      "Best mean reward: -583.74 - Last mean reward per episode: -580.39 - Last mean moves per episode: 35.24\n",
      "Saving new best model\n",
      "8489987 timesteps\n",
      "Best mean reward: -580.39 - Last mean reward per episode: -576.42 - Last mean moves per episode: 35.13\n",
      "Saving new best model\n",
      "8499986 timesteps\n",
      "Best mean reward: -576.42 - Last mean reward per episode: -573.97 - Last mean moves per episode: 35.05\n",
      "Saving new best model\n",
      "8509988 timesteps\n",
      "Best mean reward: -573.97 - Last mean reward per episode: -575.31 - Last mean moves per episode: 35.09\n",
      "8519970 timesteps\n",
      "Best mean reward: -573.97 - Last mean reward per episode: -573.99 - Last mean moves per episode: 35.05\n",
      "8529995 timesteps\n",
      "Best mean reward: -573.97 - Last mean reward per episode: -573.40 - Last mean moves per episode: 35.03\n",
      "Saving new best model\n",
      "8539994 timesteps\n",
      "Best mean reward: -573.40 - Last mean reward per episode: -570.29 - Last mean moves per episode: 34.94\n",
      "Saving new best model\n",
      "8549992 timesteps\n",
      "Best mean reward: -570.29 - Last mean reward per episode: -571.15 - Last mean moves per episode: 34.95\n",
      "8559968 timesteps\n",
      "Best mean reward: -570.29 - Last mean reward per episode: -568.23 - Last mean moves per episode: 34.87\n",
      "Saving new best model\n",
      "8569996 timesteps\n",
      "Best mean reward: -568.23 - Last mean reward per episode: -567.81 - Last mean moves per episode: 34.87\n",
      "Saving new best model\n",
      "8579984 timesteps\n",
      "Best mean reward: -567.81 - Last mean reward per episode: -565.82 - Last mean moves per episode: 34.81\n",
      "Saving new best model\n",
      "8589976 timesteps\n",
      "Best mean reward: -565.82 - Last mean reward per episode: -563.23 - Last mean moves per episode: 34.73\n",
      "Saving new best model\n",
      "8599989 timesteps\n",
      "Best mean reward: -563.23 - Last mean reward per episode: -561.24 - Last mean moves per episode: 34.66\n",
      "Saving new best model\n",
      "8609976 timesteps\n",
      "Best mean reward: -561.24 - Last mean reward per episode: -560.74 - Last mean moves per episode: 34.65\n",
      "Saving new best model\n",
      "8619975 timesteps\n",
      "Best mean reward: -560.74 - Last mean reward per episode: -561.00 - Last mean moves per episode: 34.65\n",
      "8630000 timesteps\n",
      "Best mean reward: -560.74 - Last mean reward per episode: -561.08 - Last mean moves per episode: 34.64\n",
      "8639997 timesteps\n",
      "Best mean reward: -560.74 - Last mean reward per episode: -561.92 - Last mean moves per episode: 34.66\n",
      "8649996 timesteps\n",
      "Best mean reward: -560.74 - Last mean reward per episode: -561.73 - Last mean moves per episode: 34.66\n",
      "8659982 timesteps\n",
      "Best mean reward: -560.74 - Last mean reward per episode: -560.55 - Last mean moves per episode: 34.62\n",
      "Saving new best model\n",
      "8669972 timesteps\n",
      "Best mean reward: -560.55 - Last mean reward per episode: -559.48 - Last mean moves per episode: 34.60\n",
      "Saving new best model\n",
      "8679977 timesteps\n",
      "Best mean reward: -559.48 - Last mean reward per episode: -555.97 - Last mean moves per episode: 34.51\n",
      "Saving new best model\n",
      "8689998 timesteps\n",
      "Best mean reward: -555.97 - Last mean reward per episode: -556.91 - Last mean moves per episode: 34.54\n",
      "8699973 timesteps\n",
      "Best mean reward: -555.97 - Last mean reward per episode: -553.46 - Last mean moves per episode: 34.43\n",
      "Saving new best model\n",
      "8709973 timesteps\n",
      "Best mean reward: -553.46 - Last mean reward per episode: -552.03 - Last mean moves per episode: 34.38\n",
      "Saving new best model\n",
      "8719969 timesteps\n",
      "Best mean reward: -552.03 - Last mean reward per episode: -552.37 - Last mean moves per episode: 34.40\n",
      "8729979 timesteps\n",
      "Best mean reward: -552.03 - Last mean reward per episode: -552.10 - Last mean moves per episode: 34.40\n",
      "8740000 timesteps\n",
      "Best mean reward: -552.03 - Last mean reward per episode: -550.04 - Last mean moves per episode: 34.35\n",
      "Saving new best model\n",
      "8749987 timesteps\n",
      "Best mean reward: -550.04 - Last mean reward per episode: -546.91 - Last mean moves per episode: 34.26\n",
      "Saving new best model\n",
      "8759981 timesteps\n",
      "Best mean reward: -546.91 - Last mean reward per episode: -545.68 - Last mean moves per episode: 34.21\n",
      "Saving new best model\n",
      "8769998 timesteps\n",
      "Best mean reward: -545.68 - Last mean reward per episode: -544.84 - Last mean moves per episode: 34.17\n",
      "Saving new best model\n",
      "8779970 timesteps\n",
      "Best mean reward: -544.84 - Last mean reward per episode: -543.69 - Last mean moves per episode: 34.13\n",
      "Saving new best model\n",
      "8789950 timesteps\n",
      "Best mean reward: -543.69 - Last mean reward per episode: -540.51 - Last mean moves per episode: 34.04\n",
      "Saving new best model\n",
      "8799988 timesteps\n",
      "Best mean reward: -540.51 - Last mean reward per episode: -540.04 - Last mean moves per episode: 34.02\n",
      "Saving new best model\n",
      "8809968 timesteps\n",
      "Best mean reward: -540.04 - Last mean reward per episode: -540.31 - Last mean moves per episode: 34.03\n",
      "8819982 timesteps\n",
      "Best mean reward: -540.04 - Last mean reward per episode: -540.14 - Last mean moves per episode: 34.04\n",
      "8829993 timesteps\n",
      "Best mean reward: -540.04 - Last mean reward per episode: -540.95 - Last mean moves per episode: 34.05\n",
      "8839985 timesteps\n",
      "Best mean reward: -540.04 - Last mean reward per episode: -542.15 - Last mean moves per episode: 34.08\n",
      "8849973 timesteps\n",
      "Best mean reward: -540.04 - Last mean reward per episode: -541.13 - Last mean moves per episode: 34.04\n",
      "8859991 timesteps\n",
      "Best mean reward: -540.04 - Last mean reward per episode: -539.45 - Last mean moves per episode: 33.99\n",
      "Saving new best model\n",
      "8869980 timesteps\n",
      "Best mean reward: -539.45 - Last mean reward per episode: -538.93 - Last mean moves per episode: 33.98\n",
      "Saving new best model\n",
      "8879981 timesteps\n",
      "Best mean reward: -538.93 - Last mean reward per episode: -536.70 - Last mean moves per episode: 33.90\n",
      "Saving new best model\n",
      "8889980 timesteps\n",
      "Best mean reward: -536.70 - Last mean reward per episode: -537.21 - Last mean moves per episode: 33.91\n",
      "8900000 timesteps\n",
      "Best mean reward: -536.70 - Last mean reward per episode: -534.92 - Last mean moves per episode: 33.83\n",
      "Saving new best model\n",
      "8909958 timesteps\n",
      "Best mean reward: -534.92 - Last mean reward per episode: -534.39 - Last mean moves per episode: 33.81\n",
      "Saving new best model\n",
      "8919996 timesteps\n",
      "Best mean reward: -534.39 - Last mean reward per episode: -534.57 - Last mean moves per episode: 33.82\n",
      "8929979 timesteps\n",
      "Best mean reward: -534.39 - Last mean reward per episode: -532.98 - Last mean moves per episode: 33.76\n",
      "Saving new best model\n",
      "8939979 timesteps\n",
      "Best mean reward: -532.98 - Last mean reward per episode: -532.33 - Last mean moves per episode: 33.74\n",
      "Saving new best model\n",
      "8949948 timesteps\n",
      "Best mean reward: -532.33 - Last mean reward per episode: -529.35 - Last mean moves per episode: 33.65\n",
      "Saving new best model\n",
      "8959992 timesteps\n",
      "Best mean reward: -529.35 - Last mean reward per episode: -525.82 - Last mean moves per episode: 33.55\n",
      "Saving new best model\n",
      "8969986 timesteps\n",
      "Best mean reward: -525.82 - Last mean reward per episode: -524.42 - Last mean moves per episode: 33.54\n",
      "Saving new best model\n",
      "8979992 timesteps\n",
      "Best mean reward: -524.42 - Last mean reward per episode: -523.68 - Last mean moves per episode: 33.52\n",
      "Saving new best model\n",
      "8989990 timesteps\n",
      "Best mean reward: -523.68 - Last mean reward per episode: -521.92 - Last mean moves per episode: 33.47\n",
      "Saving new best model\n",
      "8999973 timesteps\n",
      "Best mean reward: -521.92 - Last mean reward per episode: -522.08 - Last mean moves per episode: 33.46\n",
      "9009995 timesteps\n",
      "Best mean reward: -521.92 - Last mean reward per episode: -521.16 - Last mean moves per episode: 33.43\n",
      "Saving new best model\n",
      "9019922 timesteps\n",
      "Best mean reward: -521.16 - Last mean reward per episode: -519.05 - Last mean moves per episode: 33.35\n",
      "Saving new best model\n",
      "9029998 timesteps\n",
      "Best mean reward: -519.05 - Last mean reward per episode: -519.79 - Last mean moves per episode: 33.36\n",
      "9039998 timesteps\n",
      "Best mean reward: -519.05 - Last mean reward per episode: -518.52 - Last mean moves per episode: 33.33\n",
      "Saving new best model\n",
      "9049999 timesteps\n",
      "Best mean reward: -518.52 - Last mean reward per episode: -517.44 - Last mean moves per episode: 33.27\n",
      "Saving new best model\n",
      "9059972 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -518.12 - Last mean moves per episode: 33.28\n",
      "9069978 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -517.55 - Last mean moves per episode: 33.25\n",
      "9079994 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -517.81 - Last mean moves per episode: 33.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9090000 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -518.80 - Last mean moves per episode: 33.29\n",
      "9099988 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -517.53 - Last mean moves per episode: 33.24\n",
      "9109966 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -518.12 - Last mean moves per episode: 33.25\n",
      "9119984 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -517.86 - Last mean moves per episode: 33.23\n",
      "9129982 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -517.78 - Last mean moves per episode: 33.22\n",
      "9139973 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -517.86 - Last mean moves per episode: 33.22\n",
      "9149940 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -517.80 - Last mean moves per episode: 33.21\n",
      "9159999 timesteps\n",
      "Best mean reward: -517.44 - Last mean reward per episode: -516.14 - Last mean moves per episode: 33.15\n",
      "Saving new best model\n",
      "9169926 timesteps\n",
      "Best mean reward: -516.14 - Last mean reward per episode: -515.37 - Last mean moves per episode: 33.13\n",
      "Saving new best model\n",
      "9179992 timesteps\n",
      "Best mean reward: -515.37 - Last mean reward per episode: -515.81 - Last mean moves per episode: 33.14\n",
      "9189971 timesteps\n",
      "Best mean reward: -515.37 - Last mean reward per episode: -515.86 - Last mean moves per episode: 33.15\n",
      "9199982 timesteps\n",
      "Best mean reward: -515.37 - Last mean reward per episode: -514.80 - Last mean moves per episode: 33.11\n",
      "Saving new best model\n",
      "9209950 timesteps\n",
      "Best mean reward: -514.80 - Last mean reward per episode: -514.39 - Last mean moves per episode: 33.10\n",
      "Saving new best model\n",
      "9219978 timesteps\n",
      "Best mean reward: -514.39 - Last mean reward per episode: -513.50 - Last mean moves per episode: 33.08\n",
      "Saving new best model\n",
      "9229989 timesteps\n",
      "Best mean reward: -513.50 - Last mean reward per episode: -512.43 - Last mean moves per episode: 33.04\n",
      "Saving new best model\n",
      "9239984 timesteps\n",
      "Best mean reward: -512.43 - Last mean reward per episode: -511.63 - Last mean moves per episode: 33.03\n",
      "Saving new best model\n",
      "9249989 timesteps\n",
      "Best mean reward: -511.63 - Last mean reward per episode: -511.67 - Last mean moves per episode: 33.04\n",
      "9259967 timesteps\n",
      "Best mean reward: -511.63 - Last mean reward per episode: -511.49 - Last mean moves per episode: 33.03\n",
      "Saving new best model\n",
      "9269975 timesteps\n",
      "Best mean reward: -511.49 - Last mean reward per episode: -511.24 - Last mean moves per episode: 33.04\n",
      "Saving new best model\n",
      "9279985 timesteps\n",
      "Best mean reward: -511.24 - Last mean reward per episode: -511.89 - Last mean moves per episode: 33.06\n",
      "9289972 timesteps\n",
      "Best mean reward: -511.24 - Last mean reward per episode: -512.82 - Last mean moves per episode: 33.08\n",
      "9299991 timesteps\n",
      "Best mean reward: -511.24 - Last mean reward per episode: -510.84 - Last mean moves per episode: 33.02\n",
      "Saving new best model\n",
      "9309988 timesteps\n",
      "Best mean reward: -510.84 - Last mean reward per episode: -510.76 - Last mean moves per episode: 33.00\n",
      "Saving new best model\n",
      "9319988 timesteps\n",
      "Best mean reward: -510.76 - Last mean reward per episode: -512.93 - Last mean moves per episode: 33.07\n",
      "9329992 timesteps\n",
      "Best mean reward: -510.76 - Last mean reward per episode: -512.49 - Last mean moves per episode: 33.05\n",
      "9339991 timesteps\n",
      "Best mean reward: -510.76 - Last mean reward per episode: -511.28 - Last mean moves per episode: 33.01\n",
      "9349978 timesteps\n",
      "Best mean reward: -510.76 - Last mean reward per episode: -511.68 - Last mean moves per episode: 33.02\n",
      "9359972 timesteps\n",
      "Best mean reward: -510.76 - Last mean reward per episode: -511.68 - Last mean moves per episode: 33.02\n",
      "9369980 timesteps\n",
      "Best mean reward: -510.76 - Last mean reward per episode: -512.17 - Last mean moves per episode: 33.03\n",
      "9379981 timesteps\n",
      "Best mean reward: -510.76 - Last mean reward per episode: -510.12 - Last mean moves per episode: 32.99\n",
      "Saving new best model\n",
      "9389953 timesteps\n",
      "Best mean reward: -510.12 - Last mean reward per episode: -508.40 - Last mean moves per episode: 32.96\n",
      "Saving new best model\n",
      "9399981 timesteps\n",
      "Best mean reward: -508.40 - Last mean reward per episode: -507.21 - Last mean moves per episode: 32.91\n",
      "Saving new best model\n",
      "9409977 timesteps\n",
      "Best mean reward: -507.21 - Last mean reward per episode: -504.91 - Last mean moves per episode: 32.83\n",
      "Saving new best model\n",
      "9419965 timesteps\n",
      "Best mean reward: -504.91 - Last mean reward per episode: -504.48 - Last mean moves per episode: 32.82\n",
      "Saving new best model\n",
      "9430000 timesteps\n",
      "Best mean reward: -504.48 - Last mean reward per episode: -503.78 - Last mean moves per episode: 32.80\n",
      "Saving new best model\n",
      "9439977 timesteps\n",
      "Best mean reward: -503.78 - Last mean reward per episode: -502.77 - Last mean moves per episode: 32.79\n",
      "Saving new best model\n",
      "9449974 timesteps\n",
      "Best mean reward: -502.77 - Last mean reward per episode: -501.97 - Last mean moves per episode: 32.77\n",
      "Saving new best model\n",
      "9459998 timesteps\n",
      "Best mean reward: -501.97 - Last mean reward per episode: -497.57 - Last mean moves per episode: 32.64\n",
      "Saving new best model\n",
      "9469983 timesteps\n",
      "Best mean reward: -497.57 - Last mean reward per episode: -496.66 - Last mean moves per episode: 32.61\n",
      "Saving new best model\n",
      "9479998 timesteps\n",
      "Best mean reward: -496.66 - Last mean reward per episode: -496.77 - Last mean moves per episode: 32.61\n",
      "9489983 timesteps\n",
      "Best mean reward: -496.66 - Last mean reward per episode: -495.80 - Last mean moves per episode: 32.57\n",
      "Saving new best model\n",
      "9499994 timesteps\n",
      "Best mean reward: -495.80 - Last mean reward per episode: -493.55 - Last mean moves per episode: 32.51\n",
      "Saving new best model\n",
      "9509970 timesteps\n",
      "Best mean reward: -493.55 - Last mean reward per episode: -491.23 - Last mean moves per episode: 32.43\n",
      "Saving new best model\n",
      "9520000 timesteps\n",
      "Best mean reward: -491.23 - Last mean reward per episode: -488.83 - Last mean moves per episode: 32.36\n",
      "Saving new best model\n",
      "9529975 timesteps\n",
      "Best mean reward: -488.83 - Last mean reward per episode: -486.33 - Last mean moves per episode: 32.27\n",
      "Saving new best model\n",
      "9539979 timesteps\n",
      "Best mean reward: -486.33 - Last mean reward per episode: -485.86 - Last mean moves per episode: 32.26\n",
      "Saving new best model\n",
      "9549982 timesteps\n",
      "Best mean reward: -485.86 - Last mean reward per episode: -485.63 - Last mean moves per episode: 32.25\n",
      "Saving new best model\n",
      "9559976 timesteps\n",
      "Best mean reward: -485.63 - Last mean reward per episode: -484.08 - Last mean moves per episode: 32.21\n",
      "Saving new best model\n",
      "9569996 timesteps\n",
      "Best mean reward: -484.08 - Last mean reward per episode: -481.77 - Last mean moves per episode: 32.12\n",
      "Saving new best model\n",
      "9579908 timesteps\n",
      "Best mean reward: -481.77 - Last mean reward per episode: -482.24 - Last mean moves per episode: 32.13\n",
      "9589989 timesteps\n",
      "Best mean reward: -481.77 - Last mean reward per episode: -482.47 - Last mean moves per episode: 32.13\n",
      "9599998 timesteps\n",
      "Best mean reward: -481.77 - Last mean reward per episode: -481.61 - Last mean moves per episode: 32.10\n",
      "Saving new best model\n",
      "9609975 timesteps\n",
      "Best mean reward: -481.61 - Last mean reward per episode: -482.14 - Last mean moves per episode: 32.09\n",
      "9619996 timesteps\n",
      "Best mean reward: -481.61 - Last mean reward per episode: -482.83 - Last mean moves per episode: 32.09\n",
      "9629975 timesteps\n",
      "Best mean reward: -481.61 - Last mean reward per episode: -479.95 - Last mean moves per episode: 32.00\n",
      "Saving new best model\n",
      "9639999 timesteps\n",
      "Best mean reward: -479.95 - Last mean reward per episode: -477.93 - Last mean moves per episode: 31.93\n",
      "Saving new best model\n",
      "9649970 timesteps\n",
      "Best mean reward: -477.93 - Last mean reward per episode: -476.15 - Last mean moves per episode: 31.87\n",
      "Saving new best model\n",
      "9659976 timesteps\n",
      "Best mean reward: -476.15 - Last mean reward per episode: -476.15 - Last mean moves per episode: 31.88\n",
      "Saving new best model\n",
      "9669966 timesteps\n",
      "Best mean reward: -476.15 - Last mean reward per episode: -474.21 - Last mean moves per episode: 31.82\n",
      "Saving new best model\n",
      "9679993 timesteps\n",
      "Best mean reward: -474.21 - Last mean reward per episode: -473.98 - Last mean moves per episode: 31.81\n",
      "Saving new best model\n",
      "9689982 timesteps\n",
      "Best mean reward: -473.98 - Last mean reward per episode: -471.92 - Last mean moves per episode: 31.76\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9699995 timesteps\n",
      "Best mean reward: -471.92 - Last mean reward per episode: -472.93 - Last mean moves per episode: 31.78\n",
      "9709979 timesteps\n",
      "Best mean reward: -471.92 - Last mean reward per episode: -471.78 - Last mean moves per episode: 31.74\n",
      "Saving new best model\n",
      "9719997 timesteps\n",
      "Best mean reward: -471.78 - Last mean reward per episode: -471.84 - Last mean moves per episode: 31.74\n",
      "9729997 timesteps\n",
      "Best mean reward: -471.78 - Last mean reward per episode: -471.11 - Last mean moves per episode: 31.72\n",
      "Saving new best model\n",
      "9739987 timesteps\n",
      "Best mean reward: -471.11 - Last mean reward per episode: -466.96 - Last mean moves per episode: 31.60\n",
      "Saving new best model\n",
      "9749950 timesteps\n",
      "Best mean reward: -466.96 - Last mean reward per episode: -466.67 - Last mean moves per episode: 31.59\n",
      "Saving new best model\n",
      "9759989 timesteps\n",
      "Best mean reward: -466.67 - Last mean reward per episode: -465.18 - Last mean moves per episode: 31.55\n",
      "Saving new best model\n",
      "9769969 timesteps\n",
      "Best mean reward: -465.18 - Last mean reward per episode: -466.87 - Last mean moves per episode: 31.59\n",
      "9779992 timesteps\n",
      "Best mean reward: -465.18 - Last mean reward per episode: -465.60 - Last mean moves per episode: 31.57\n",
      "9789990 timesteps\n",
      "Best mean reward: -465.18 - Last mean reward per episode: -465.35 - Last mean moves per episode: 31.56\n",
      "9799980 timesteps\n",
      "Best mean reward: -465.18 - Last mean reward per episode: -464.05 - Last mean moves per episode: 31.52\n",
      "Saving new best model\n",
      "9809989 timesteps\n",
      "Best mean reward: -464.05 - Last mean reward per episode: -461.98 - Last mean moves per episode: 31.46\n",
      "Saving new best model\n",
      "9819979 timesteps\n",
      "Best mean reward: -461.98 - Last mean reward per episode: -461.63 - Last mean moves per episode: 31.44\n",
      "Saving new best model\n",
      "9829943 timesteps\n",
      "Best mean reward: -461.63 - Last mean reward per episode: -462.59 - Last mean moves per episode: 31.46\n",
      "9839987 timesteps\n",
      "Best mean reward: -461.63 - Last mean reward per episode: -462.95 - Last mean moves per episode: 31.47\n",
      "9849948 timesteps\n",
      "Best mean reward: -461.63 - Last mean reward per episode: -462.20 - Last mean moves per episode: 31.43\n",
      "9860000 timesteps\n",
      "Best mean reward: -461.63 - Last mean reward per episode: -460.77 - Last mean moves per episode: 31.38\n",
      "Saving new best model\n",
      "9869991 timesteps\n",
      "Best mean reward: -460.77 - Last mean reward per episode: -460.51 - Last mean moves per episode: 31.37\n",
      "Saving new best model\n",
      "9879988 timesteps\n",
      "Best mean reward: -460.51 - Last mean reward per episode: -461.28 - Last mean moves per episode: 31.39\n",
      "9889999 timesteps\n",
      "Best mean reward: -460.51 - Last mean reward per episode: -459.80 - Last mean moves per episode: 31.34\n",
      "Saving new best model\n",
      "9899992 timesteps\n",
      "Best mean reward: -459.80 - Last mean reward per episode: -457.18 - Last mean moves per episode: 31.28\n",
      "Saving new best model\n",
      "9909977 timesteps\n",
      "Best mean reward: -457.18 - Last mean reward per episode: -455.86 - Last mean moves per episode: 31.23\n",
      "Saving new best model\n",
      "9919995 timesteps\n",
      "Best mean reward: -455.86 - Last mean reward per episode: -451.89 - Last mean moves per episode: 31.11\n",
      "Saving new best model\n",
      "9929963 timesteps\n",
      "Best mean reward: -451.89 - Last mean reward per episode: -451.32 - Last mean moves per episode: 31.10\n",
      "Saving new best model\n",
      "9939980 timesteps\n",
      "Best mean reward: -451.32 - Last mean reward per episode: -449.67 - Last mean moves per episode: 31.06\n",
      "Saving new best model\n",
      "9949993 timesteps\n",
      "Best mean reward: -449.67 - Last mean reward per episode: -449.50 - Last mean moves per episode: 31.05\n",
      "Saving new best model\n",
      "9959983 timesteps\n",
      "Best mean reward: -449.50 - Last mean reward per episode: -450.15 - Last mean moves per episode: 31.06\n",
      "9969993 timesteps\n",
      "Best mean reward: -449.50 - Last mean reward per episode: -448.94 - Last mean moves per episode: 31.02\n",
      "Saving new best model\n",
      "9979990 timesteps\n",
      "Best mean reward: -448.94 - Last mean reward per episode: -446.65 - Last mean moves per episode: 30.94\n",
      "Saving new best model\n",
      "9989992 timesteps\n",
      "Best mean reward: -446.65 - Last mean reward per episode: -445.25 - Last mean moves per episode: 30.89\n",
      "Saving new best model\n",
      "9999986 timesteps\n",
      "Best mean reward: -445.25 - Last mean reward per episode: -447.60 - Last mean moves per episode: 30.95\n",
      "10009995 timesteps\n",
      "Best mean reward: -445.25 - Last mean reward per episode: -445.77 - Last mean moves per episode: 30.89\n",
      "10019997 timesteps\n",
      "Best mean reward: -445.25 - Last mean reward per episode: -445.85 - Last mean moves per episode: 30.89\n",
      "10029997 timesteps\n",
      "Best mean reward: -445.25 - Last mean reward per episode: -445.72 - Last mean moves per episode: 30.89\n",
      "10039995 timesteps\n",
      "Best mean reward: -445.25 - Last mean reward per episode: -445.98 - Last mean moves per episode: 30.91\n",
      "10049994 timesteps\n",
      "Best mean reward: -445.25 - Last mean reward per episode: -447.40 - Last mean moves per episode: 30.93\n",
      "10059961 timesteps\n",
      "Best mean reward: -445.25 - Last mean reward per episode: -445.78 - Last mean moves per episode: 30.87\n",
      "10069990 timesteps\n",
      "Best mean reward: -445.25 - Last mean reward per episode: -446.73 - Last mean moves per episode: 30.88\n",
      "10079976 timesteps\n",
      "Best mean reward: -445.25 - Last mean reward per episode: -443.68 - Last mean moves per episode: 30.78\n",
      "Saving new best model\n",
      "10089981 timesteps\n",
      "Best mean reward: -443.68 - Last mean reward per episode: -442.36 - Last mean moves per episode: 30.73\n",
      "Saving new best model\n",
      "10099964 timesteps\n",
      "Best mean reward: -442.36 - Last mean reward per episode: -441.55 - Last mean moves per episode: 30.71\n",
      "Saving new best model\n",
      "10109989 timesteps\n",
      "Best mean reward: -441.55 - Last mean reward per episode: -442.79 - Last mean moves per episode: 30.74\n",
      "10119921 timesteps\n",
      "Best mean reward: -441.55 - Last mean reward per episode: -443.54 - Last mean moves per episode: 30.78\n",
      "10129989 timesteps\n",
      "Best mean reward: -441.55 - Last mean reward per episode: -442.03 - Last mean moves per episode: 30.74\n",
      "10139966 timesteps\n",
      "Best mean reward: -441.55 - Last mean reward per episode: -441.17 - Last mean moves per episode: 30.72\n",
      "Saving new best model\n",
      "10149998 timesteps\n",
      "Best mean reward: -441.17 - Last mean reward per episode: -441.48 - Last mean moves per episode: 30.73\n",
      "10159978 timesteps\n",
      "Best mean reward: -441.17 - Last mean reward per episode: -442.79 - Last mean moves per episode: 30.79\n",
      "10169990 timesteps\n",
      "Best mean reward: -441.17 - Last mean reward per episode: -442.01 - Last mean moves per episode: 30.77\n",
      "10179951 timesteps\n",
      "Best mean reward: -441.17 - Last mean reward per episode: -441.11 - Last mean moves per episode: 30.74\n",
      "Saving new best model\n",
      "10189984 timesteps\n",
      "Best mean reward: -441.11 - Last mean reward per episode: -441.62 - Last mean moves per episode: 30.76\n",
      "10199978 timesteps\n",
      "Best mean reward: -441.11 - Last mean reward per episode: -441.57 - Last mean moves per episode: 30.75\n",
      "10209988 timesteps\n",
      "Best mean reward: -441.11 - Last mean reward per episode: -442.50 - Last mean moves per episode: 30.78\n",
      "10219986 timesteps\n",
      "Best mean reward: -441.11 - Last mean reward per episode: -441.20 - Last mean moves per episode: 30.73\n",
      "10229980 timesteps\n",
      "Best mean reward: -441.11 - Last mean reward per episode: -441.92 - Last mean moves per episode: 30.75\n",
      "10239992 timesteps\n",
      "Best mean reward: -441.11 - Last mean reward per episode: -440.83 - Last mean moves per episode: 30.71\n",
      "Saving new best model\n",
      "10249985 timesteps\n",
      "Best mean reward: -440.83 - Last mean reward per episode: -443.20 - Last mean moves per episode: 30.78\n",
      "10259983 timesteps\n",
      "Best mean reward: -440.83 - Last mean reward per episode: -439.79 - Last mean moves per episode: 30.68\n",
      "Saving new best model\n",
      "10269976 timesteps\n",
      "Best mean reward: -439.79 - Last mean reward per episode: -440.84 - Last mean moves per episode: 30.72\n",
      "10279978 timesteps\n",
      "Best mean reward: -439.79 - Last mean reward per episode: -439.35 - Last mean moves per episode: 30.68\n",
      "Saving new best model\n",
      "10289965 timesteps\n",
      "Best mean reward: -439.35 - Last mean reward per episode: -439.49 - Last mean moves per episode: 30.69\n",
      "10299996 timesteps\n",
      "Best mean reward: -439.35 - Last mean reward per episode: -438.97 - Last mean moves per episode: 30.67\n",
      "Saving new best model\n",
      "10309965 timesteps\n",
      "Best mean reward: -438.97 - Last mean reward per episode: -436.17 - Last mean moves per episode: 30.60\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10319977 timesteps\n",
      "Best mean reward: -436.17 - Last mean reward per episode: -435.54 - Last mean moves per episode: 30.59\n",
      "Saving new best model\n",
      "10329988 timesteps\n",
      "Best mean reward: -435.54 - Last mean reward per episode: -433.49 - Last mean moves per episode: 30.53\n",
      "Saving new best model\n",
      "10339994 timesteps\n",
      "Best mean reward: -433.49 - Last mean reward per episode: -433.22 - Last mean moves per episode: 30.51\n",
      "Saving new best model\n",
      "10350000 timesteps\n",
      "Best mean reward: -433.22 - Last mean reward per episode: -433.49 - Last mean moves per episode: 30.53\n",
      "10359985 timesteps\n",
      "Best mean reward: -433.22 - Last mean reward per episode: -432.12 - Last mean moves per episode: 30.49\n",
      "Saving new best model\n",
      "10369994 timesteps\n",
      "Best mean reward: -432.12 - Last mean reward per episode: -431.65 - Last mean moves per episode: 30.49\n",
      "Saving new best model\n",
      "10379988 timesteps\n",
      "Best mean reward: -431.65 - Last mean reward per episode: -431.01 - Last mean moves per episode: 30.47\n",
      "Saving new best model\n",
      "10389976 timesteps\n",
      "Best mean reward: -431.01 - Last mean reward per episode: -431.03 - Last mean moves per episode: 30.47\n",
      "10399993 timesteps\n",
      "Best mean reward: -431.01 - Last mean reward per episode: -431.56 - Last mean moves per episode: 30.49\n",
      "10409983 timesteps\n",
      "Best mean reward: -431.01 - Last mean reward per episode: -429.49 - Last mean moves per episode: 30.43\n",
      "Saving new best model\n",
      "10419978 timesteps\n",
      "Best mean reward: -429.49 - Last mean reward per episode: -428.01 - Last mean moves per episode: 30.38\n",
      "Saving new best model\n",
      "10429978 timesteps\n",
      "Best mean reward: -428.01 - Last mean reward per episode: -428.34 - Last mean moves per episode: 30.37\n",
      "10439984 timesteps\n",
      "Best mean reward: -428.01 - Last mean reward per episode: -428.85 - Last mean moves per episode: 30.37\n",
      "10449992 timesteps\n",
      "Best mean reward: -428.01 - Last mean reward per episode: -429.22 - Last mean moves per episode: 30.37\n",
      "10459993 timesteps\n",
      "Best mean reward: -428.01 - Last mean reward per episode: -428.16 - Last mean moves per episode: 30.32\n",
      "10469987 timesteps\n",
      "Best mean reward: -428.01 - Last mean reward per episode: -427.33 - Last mean moves per episode: 30.28\n",
      "Saving new best model\n",
      "10479975 timesteps\n",
      "Best mean reward: -427.33 - Last mean reward per episode: -427.99 - Last mean moves per episode: 30.29\n",
      "10489990 timesteps\n",
      "Best mean reward: -427.33 - Last mean reward per episode: -425.53 - Last mean moves per episode: 30.20\n",
      "Saving new best model\n",
      "10499921 timesteps\n",
      "Best mean reward: -425.53 - Last mean reward per episode: -425.57 - Last mean moves per episode: 30.20\n",
      "10509973 timesteps\n",
      "Best mean reward: -425.53 - Last mean reward per episode: -423.43 - Last mean moves per episode: 30.13\n",
      "Saving new best model\n",
      "10519977 timesteps\n",
      "Best mean reward: -423.43 - Last mean reward per episode: -424.36 - Last mean moves per episode: 30.17\n",
      "10529999 timesteps\n",
      "Best mean reward: -423.43 - Last mean reward per episode: -423.57 - Last mean moves per episode: 30.14\n",
      "10539988 timesteps\n",
      "Best mean reward: -423.43 - Last mean reward per episode: -420.89 - Last mean moves per episode: 30.04\n",
      "Saving new best model\n",
      "10549987 timesteps\n",
      "Best mean reward: -420.89 - Last mean reward per episode: -420.62 - Last mean moves per episode: 30.03\n",
      "Saving new best model\n",
      "10559982 timesteps\n",
      "Best mean reward: -420.62 - Last mean reward per episode: -423.32 - Last mean moves per episode: 30.10\n",
      "10569982 timesteps\n",
      "Best mean reward: -420.62 - Last mean reward per episode: -421.15 - Last mean moves per episode: 30.04\n",
      "10579980 timesteps\n",
      "Best mean reward: -420.62 - Last mean reward per episode: -422.89 - Last mean moves per episode: 30.08\n",
      "10589979 timesteps\n",
      "Best mean reward: -420.62 - Last mean reward per episode: -422.83 - Last mean moves per episode: 30.08\n",
      "10599980 timesteps\n",
      "Best mean reward: -420.62 - Last mean reward per episode: -420.96 - Last mean moves per episode: 30.02\n",
      "10609995 timesteps\n",
      "Best mean reward: -420.62 - Last mean reward per episode: -421.42 - Last mean moves per episode: 30.03\n",
      "10619993 timesteps\n",
      "Best mean reward: -420.62 - Last mean reward per episode: -422.23 - Last mean moves per episode: 30.04\n",
      "10629975 timesteps\n",
      "Best mean reward: -420.62 - Last mean reward per episode: -422.42 - Last mean moves per episode: 30.03\n",
      "10639988 timesteps\n",
      "Best mean reward: -420.62 - Last mean reward per episode: -421.49 - Last mean moves per episode: 30.00\n",
      "10649995 timesteps\n",
      "Best mean reward: -420.62 - Last mean reward per episode: -418.69 - Last mean moves per episode: 29.91\n",
      "Saving new best model\n",
      "10659986 timesteps\n",
      "Best mean reward: -418.69 - Last mean reward per episode: -418.14 - Last mean moves per episode: 29.89\n",
      "Saving new best model\n",
      "10669988 timesteps\n",
      "Best mean reward: -418.14 - Last mean reward per episode: -417.70 - Last mean moves per episode: 29.88\n",
      "Saving new best model\n",
      "10679982 timesteps\n",
      "Best mean reward: -417.70 - Last mean reward per episode: -417.45 - Last mean moves per episode: 29.87\n",
      "Saving new best model\n",
      "10689993 timesteps\n",
      "Best mean reward: -417.45 - Last mean reward per episode: -418.92 - Last mean moves per episode: 29.91\n",
      "10699973 timesteps\n",
      "Best mean reward: -417.45 - Last mean reward per episode: -418.70 - Last mean moves per episode: 29.89\n",
      "10709976 timesteps\n",
      "Best mean reward: -417.45 - Last mean reward per episode: -421.78 - Last mean moves per episode: 29.99\n",
      "10719986 timesteps\n",
      "Best mean reward: -417.45 - Last mean reward per episode: -421.19 - Last mean moves per episode: 29.98\n",
      "10729979 timesteps\n",
      "Best mean reward: -417.45 - Last mean reward per episode: -417.96 - Last mean moves per episode: 29.91\n",
      "10739986 timesteps\n",
      "Best mean reward: -417.45 - Last mean reward per episode: -416.70 - Last mean moves per episode: 29.85\n",
      "Saving new best model\n",
      "10749978 timesteps\n",
      "Best mean reward: -416.70 - Last mean reward per episode: -416.97 - Last mean moves per episode: 29.86\n",
      "10759975 timesteps\n",
      "Best mean reward: -416.70 - Last mean reward per episode: -415.21 - Last mean moves per episode: 29.83\n",
      "Saving new best model\n",
      "10769960 timesteps\n",
      "Best mean reward: -415.21 - Last mean reward per episode: -413.21 - Last mean moves per episode: 29.78\n",
      "Saving new best model\n",
      "10779995 timesteps\n",
      "Best mean reward: -413.21 - Last mean reward per episode: -413.28 - Last mean moves per episode: 29.78\n",
      "10789982 timesteps\n",
      "Best mean reward: -413.21 - Last mean reward per episode: -412.18 - Last mean moves per episode: 29.76\n",
      "Saving new best model\n",
      "10799998 timesteps\n",
      "Best mean reward: -412.18 - Last mean reward per episode: -409.47 - Last mean moves per episode: 29.68\n",
      "Saving new best model\n",
      "10809995 timesteps\n",
      "Best mean reward: -409.47 - Last mean reward per episode: -410.05 - Last mean moves per episode: 29.68\n",
      "10819999 timesteps\n",
      "Best mean reward: -409.47 - Last mean reward per episode: -407.56 - Last mean moves per episode: 29.61\n",
      "Saving new best model\n",
      "10829987 timesteps\n",
      "Best mean reward: -407.56 - Last mean reward per episode: -406.48 - Last mean moves per episode: 29.58\n",
      "Saving new best model\n",
      "10839982 timesteps\n",
      "Best mean reward: -406.48 - Last mean reward per episode: -407.12 - Last mean moves per episode: 29.60\n",
      "10849958 timesteps\n",
      "Best mean reward: -406.48 - Last mean reward per episode: -404.56 - Last mean moves per episode: 29.53\n",
      "Saving new best model\n",
      "10859987 timesteps\n",
      "Best mean reward: -404.56 - Last mean reward per episode: -401.88 - Last mean moves per episode: 29.47\n",
      "Saving new best model\n",
      "10869975 timesteps\n",
      "Best mean reward: -401.88 - Last mean reward per episode: -399.10 - Last mean moves per episode: 29.39\n",
      "Saving new best model\n",
      "10879993 timesteps\n",
      "Best mean reward: -399.10 - Last mean reward per episode: -398.59 - Last mean moves per episode: 29.37\n",
      "Saving new best model\n",
      "10889985 timesteps\n",
      "Best mean reward: -398.59 - Last mean reward per episode: -398.05 - Last mean moves per episode: 29.35\n",
      "Saving new best model\n",
      "10899978 timesteps\n",
      "Best mean reward: -398.05 - Last mean reward per episode: -398.28 - Last mean moves per episode: 29.36\n",
      "10909975 timesteps\n",
      "Best mean reward: -398.05 - Last mean reward per episode: -397.70 - Last mean moves per episode: 29.32\n",
      "Saving new best model\n",
      "10919987 timesteps\n",
      "Best mean reward: -397.70 - Last mean reward per episode: -396.56 - Last mean moves per episode: 29.29\n",
      "Saving new best model\n",
      "10929963 timesteps\n",
      "Best mean reward: -396.56 - Last mean reward per episode: -394.50 - Last mean moves per episode: 29.24\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10939987 timesteps\n",
      "Best mean reward: -394.50 - Last mean reward per episode: -394.28 - Last mean moves per episode: 29.23\n",
      "Saving new best model\n",
      "10949989 timesteps\n",
      "Best mean reward: -394.28 - Last mean reward per episode: -395.96 - Last mean moves per episode: 29.27\n",
      "10959981 timesteps\n",
      "Best mean reward: -394.28 - Last mean reward per episode: -394.01 - Last mean moves per episode: 29.20\n",
      "Saving new best model\n",
      "10969992 timesteps\n",
      "Best mean reward: -394.01 - Last mean reward per episode: -394.52 - Last mean moves per episode: 29.22\n",
      "10979975 timesteps\n",
      "Best mean reward: -394.01 - Last mean reward per episode: -394.65 - Last mean moves per episode: 29.21\n",
      "10989969 timesteps\n",
      "Best mean reward: -394.01 - Last mean reward per episode: -392.93 - Last mean moves per episode: 29.16\n",
      "Saving new best model\n",
      "10999991 timesteps\n",
      "Best mean reward: -392.93 - Last mean reward per episode: -389.60 - Last mean moves per episode: 29.05\n",
      "Saving new best model\n",
      "11009982 timesteps\n",
      "Best mean reward: -389.60 - Last mean reward per episode: -389.05 - Last mean moves per episode: 29.03\n",
      "Saving new best model\n",
      "11020000 timesteps\n",
      "Best mean reward: -389.05 - Last mean reward per episode: -390.80 - Last mean moves per episode: 29.08\n",
      "11029996 timesteps\n",
      "Best mean reward: -389.05 - Last mean reward per episode: -391.07 - Last mean moves per episode: 29.09\n",
      "11039992 timesteps\n",
      "Best mean reward: -389.05 - Last mean reward per episode: -386.36 - Last mean moves per episode: 28.95\n",
      "Saving new best model\n",
      "11049998 timesteps\n",
      "Best mean reward: -386.36 - Last mean reward per episode: -386.13 - Last mean moves per episode: 28.93\n",
      "Saving new best model\n",
      "11059995 timesteps\n",
      "Best mean reward: -386.13 - Last mean reward per episode: -386.58 - Last mean moves per episode: 28.94\n",
      "11069995 timesteps\n",
      "Best mean reward: -386.13 - Last mean reward per episode: -385.17 - Last mean moves per episode: 28.90\n",
      "Saving new best model\n",
      "11079983 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -386.19 - Last mean moves per episode: 28.93\n",
      "11089977 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -388.24 - Last mean moves per episode: 28.97\n",
      "11099989 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -388.39 - Last mean moves per episode: 28.98\n",
      "11109977 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -388.62 - Last mean moves per episode: 28.98\n",
      "11119979 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -389.87 - Last mean moves per episode: 29.03\n",
      "11129999 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -388.97 - Last mean moves per episode: 28.99\n",
      "11139974 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -391.25 - Last mean moves per episode: 29.04\n",
      "11149993 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -393.01 - Last mean moves per episode: 29.07\n",
      "11159968 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -395.26 - Last mean moves per episode: 29.11\n",
      "11169981 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -395.22 - Last mean moves per episode: 29.10\n",
      "11179975 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -395.77 - Last mean moves per episode: 29.11\n",
      "11189985 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -393.48 - Last mean moves per episode: 29.05\n",
      "11199993 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -392.74 - Last mean moves per episode: 29.04\n",
      "11209983 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -391.56 - Last mean moves per episode: 28.99\n",
      "11219972 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -392.91 - Last mean moves per episode: 29.01\n",
      "11229958 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -393.27 - Last mean moves per episode: 29.01\n",
      "11239985 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -390.56 - Last mean moves per episode: 28.93\n",
      "11249990 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -388.95 - Last mean moves per episode: 28.89\n",
      "11259952 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -388.13 - Last mean moves per episode: 28.88\n",
      "11269997 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -388.80 - Last mean moves per episode: 28.89\n",
      "11279986 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -389.40 - Last mean moves per episode: 28.90\n",
      "11289945 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -388.46 - Last mean moves per episode: 28.86\n",
      "11299944 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -389.08 - Last mean moves per episode: 28.88\n",
      "11309996 timesteps\n",
      "Best mean reward: -385.17 - Last mean reward per episode: -385.02 - Last mean moves per episode: 28.76\n",
      "Saving new best model\n",
      "11319973 timesteps\n",
      "Best mean reward: -385.02 - Last mean reward per episode: -384.16 - Last mean moves per episode: 28.74\n",
      "Saving new best model\n",
      "11329957 timesteps\n",
      "Best mean reward: -384.16 - Last mean reward per episode: -384.61 - Last mean moves per episode: 28.73\n",
      "11339995 timesteps\n",
      "Best mean reward: -384.16 - Last mean reward per episode: -385.27 - Last mean moves per episode: 28.74\n",
      "11349998 timesteps\n",
      "Best mean reward: -384.16 - Last mean reward per episode: -384.72 - Last mean moves per episode: 28.71\n",
      "11359999 timesteps\n",
      "Best mean reward: -384.16 - Last mean reward per episode: -386.39 - Last mean moves per episode: 28.75\n",
      "11369988 timesteps\n",
      "Best mean reward: -384.16 - Last mean reward per episode: -384.89 - Last mean moves per episode: 28.72\n",
      "11379999 timesteps\n",
      "Best mean reward: -384.16 - Last mean reward per episode: -381.67 - Last mean moves per episode: 28.63\n",
      "Saving new best model\n",
      "11389990 timesteps\n",
      "Best mean reward: -381.67 - Last mean reward per episode: -379.58 - Last mean moves per episode: 28.57\n",
      "Saving new best model\n",
      "11399943 timesteps\n",
      "Best mean reward: -379.58 - Last mean reward per episode: -378.64 - Last mean moves per episode: 28.54\n",
      "Saving new best model\n",
      "11409987 timesteps\n",
      "Best mean reward: -378.64 - Last mean reward per episode: -376.42 - Last mean moves per episode: 28.49\n",
      "Saving new best model\n",
      "11419968 timesteps\n",
      "Best mean reward: -376.42 - Last mean reward per episode: -375.46 - Last mean moves per episode: 28.49\n",
      "Saving new best model\n",
      "11429974 timesteps\n",
      "Best mean reward: -375.46 - Last mean reward per episode: -373.57 - Last mean moves per episode: 28.44\n",
      "Saving new best model\n",
      "11439998 timesteps\n",
      "Best mean reward: -373.57 - Last mean reward per episode: -373.84 - Last mean moves per episode: 28.45\n",
      "11449988 timesteps\n",
      "Best mean reward: -373.57 - Last mean reward per episode: -371.72 - Last mean moves per episode: 28.39\n",
      "Saving new best model\n",
      "11459997 timesteps\n",
      "Best mean reward: -371.72 - Last mean reward per episode: -367.95 - Last mean moves per episode: 28.29\n",
      "Saving new best model\n",
      "11469951 timesteps\n",
      "Best mean reward: -367.95 - Last mean reward per episode: -366.68 - Last mean moves per episode: 28.24\n",
      "Saving new best model\n",
      "11479977 timesteps\n",
      "Best mean reward: -366.68 - Last mean reward per episode: -366.93 - Last mean moves per episode: 28.23\n",
      "11490000 timesteps\n",
      "Best mean reward: -366.68 - Last mean reward per episode: -366.22 - Last mean moves per episode: 28.22\n",
      "Saving new best model\n",
      "11499974 timesteps\n",
      "Best mean reward: -366.22 - Last mean reward per episode: -366.23 - Last mean moves per episode: 28.23\n",
      "11509998 timesteps\n",
      "Best mean reward: -366.22 - Last mean reward per episode: -367.13 - Last mean moves per episode: 28.26\n",
      "11519992 timesteps\n",
      "Best mean reward: -366.22 - Last mean reward per episode: -366.40 - Last mean moves per episode: 28.25\n",
      "11529982 timesteps\n",
      "Best mean reward: -366.22 - Last mean reward per episode: -367.32 - Last mean moves per episode: 28.27\n",
      "11539999 timesteps\n",
      "Best mean reward: -366.22 - Last mean reward per episode: -367.47 - Last mean moves per episode: 28.26\n",
      "11549992 timesteps\n",
      "Best mean reward: -366.22 - Last mean reward per episode: -364.36 - Last mean moves per episode: 28.15\n",
      "Saving new best model\n",
      "11559997 timesteps\n",
      "Best mean reward: -364.36 - Last mean reward per episode: -362.31 - Last mean moves per episode: 28.10\n",
      "Saving new best model\n",
      "11569998 timesteps\n",
      "Best mean reward: -362.31 - Last mean reward per episode: -362.36 - Last mean moves per episode: 28.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11579972 timesteps\n",
      "Best mean reward: -362.31 - Last mean reward per episode: -361.43 - Last mean moves per episode: 28.09\n",
      "Saving new best model\n",
      "11589993 timesteps\n",
      "Best mean reward: -361.43 - Last mean reward per episode: -361.61 - Last mean moves per episode: 28.09\n",
      "11599968 timesteps\n",
      "Best mean reward: -361.43 - Last mean reward per episode: -361.92 - Last mean moves per episode: 28.10\n",
      "11609975 timesteps\n",
      "Best mean reward: -361.43 - Last mean reward per episode: -360.71 - Last mean moves per episode: 28.09\n",
      "Saving new best model\n",
      "11619968 timesteps\n",
      "Best mean reward: -360.71 - Last mean reward per episode: -359.68 - Last mean moves per episode: 28.06\n",
      "Saving new best model\n",
      "11629979 timesteps\n",
      "Best mean reward: -359.68 - Last mean reward per episode: -359.45 - Last mean moves per episode: 28.05\n",
      "Saving new best model\n",
      "11639979 timesteps\n",
      "Best mean reward: -359.45 - Last mean reward per episode: -357.31 - Last mean moves per episode: 28.00\n",
      "Saving new best model\n",
      "11649992 timesteps\n",
      "Best mean reward: -357.31 - Last mean reward per episode: -356.80 - Last mean moves per episode: 27.98\n",
      "Saving new best model\n",
      "11659981 timesteps\n",
      "Best mean reward: -356.80 - Last mean reward per episode: -357.28 - Last mean moves per episode: 27.98\n",
      "11669971 timesteps\n",
      "Best mean reward: -356.80 - Last mean reward per episode: -356.77 - Last mean moves per episode: 27.94\n",
      "Saving new best model\n",
      "11679980 timesteps\n",
      "Best mean reward: -356.77 - Last mean reward per episode: -356.64 - Last mean moves per episode: 27.95\n",
      "Saving new best model\n",
      "11689975 timesteps\n",
      "Best mean reward: -356.64 - Last mean reward per episode: -356.50 - Last mean moves per episode: 27.93\n",
      "Saving new best model\n",
      "11699984 timesteps\n",
      "Best mean reward: -356.50 - Last mean reward per episode: -357.17 - Last mean moves per episode: 27.96\n",
      "11709989 timesteps\n",
      "Best mean reward: -356.50 - Last mean reward per episode: -358.43 - Last mean moves per episode: 27.99\n",
      "11719969 timesteps\n",
      "Best mean reward: -356.50 - Last mean reward per episode: -356.89 - Last mean moves per episode: 27.95\n",
      "11729983 timesteps\n",
      "Best mean reward: -356.50 - Last mean reward per episode: -357.13 - Last mean moves per episode: 27.96\n",
      "11739988 timesteps\n",
      "Best mean reward: -356.50 - Last mean reward per episode: -357.57 - Last mean moves per episode: 27.97\n",
      "11749996 timesteps\n",
      "Best mean reward: -356.50 - Last mean reward per episode: -357.56 - Last mean moves per episode: 27.98\n",
      "11759990 timesteps\n",
      "Best mean reward: -356.50 - Last mean reward per episode: -356.59 - Last mean moves per episode: 27.97\n",
      "11769980 timesteps\n",
      "Best mean reward: -356.50 - Last mean reward per episode: -356.39 - Last mean moves per episode: 27.97\n",
      "Saving new best model\n",
      "11779985 timesteps\n",
      "Best mean reward: -356.39 - Last mean reward per episode: -353.12 - Last mean moves per episode: 27.87\n",
      "Saving new best model\n",
      "11789986 timesteps\n",
      "Best mean reward: -353.12 - Last mean reward per episode: -352.78 - Last mean moves per episode: 27.85\n",
      "Saving new best model\n",
      "11799990 timesteps\n",
      "Best mean reward: -352.78 - Last mean reward per episode: -353.21 - Last mean moves per episode: 27.85\n",
      "11809984 timesteps\n",
      "Best mean reward: -352.78 - Last mean reward per episode: -351.48 - Last mean moves per episode: 27.79\n",
      "Saving new best model\n",
      "11819955 timesteps\n",
      "Best mean reward: -351.48 - Last mean reward per episode: -351.45 - Last mean moves per episode: 27.79\n",
      "Saving new best model\n",
      "11829987 timesteps\n",
      "Best mean reward: -351.45 - Last mean reward per episode: -351.09 - Last mean moves per episode: 27.79\n",
      "Saving new best model\n",
      "11839991 timesteps\n",
      "Best mean reward: -351.09 - Last mean reward per episode: -351.99 - Last mean moves per episode: 27.77\n",
      "11849989 timesteps\n",
      "Best mean reward: -351.09 - Last mean reward per episode: -349.83 - Last mean moves per episode: 27.72\n",
      "Saving new best model\n",
      "11859981 timesteps\n",
      "Best mean reward: -349.83 - Last mean reward per episode: -348.46 - Last mean moves per episode: 27.68\n",
      "Saving new best model\n",
      "11869983 timesteps\n",
      "Best mean reward: -348.46 - Last mean reward per episode: -348.60 - Last mean moves per episode: 27.67\n",
      "11879998 timesteps\n",
      "Best mean reward: -348.46 - Last mean reward per episode: -348.12 - Last mean moves per episode: 27.66\n",
      "Saving new best model\n",
      "11889974 timesteps\n",
      "Best mean reward: -348.12 - Last mean reward per episode: -348.02 - Last mean moves per episode: 27.64\n",
      "Saving new best model\n",
      "11899984 timesteps\n",
      "Best mean reward: -348.02 - Last mean reward per episode: -348.05 - Last mean moves per episode: 27.64\n",
      "11909993 timesteps\n",
      "Best mean reward: -348.02 - Last mean reward per episode: -345.92 - Last mean moves per episode: 27.57\n",
      "Saving new best model\n",
      "11919996 timesteps\n",
      "Best mean reward: -345.92 - Last mean reward per episode: -345.51 - Last mean moves per episode: 27.56\n",
      "Saving new best model\n",
      "11929994 timesteps\n",
      "Best mean reward: -345.51 - Last mean reward per episode: -346.74 - Last mean moves per episode: 27.58\n",
      "11939980 timesteps\n",
      "Best mean reward: -345.51 - Last mean reward per episode: -348.00 - Last mean moves per episode: 27.62\n",
      "11949980 timesteps\n",
      "Best mean reward: -345.51 - Last mean reward per episode: -347.21 - Last mean moves per episode: 27.60\n",
      "11959989 timesteps\n",
      "Best mean reward: -345.51 - Last mean reward per episode: -346.34 - Last mean moves per episode: 27.56\n",
      "11969979 timesteps\n",
      "Best mean reward: -345.51 - Last mean reward per episode: -344.59 - Last mean moves per episode: 27.52\n",
      "Saving new best model\n",
      "11979983 timesteps\n",
      "Best mean reward: -344.59 - Last mean reward per episode: -343.17 - Last mean moves per episode: 27.45\n",
      "Saving new best model\n",
      "11989967 timesteps\n",
      "Best mean reward: -343.17 - Last mean reward per episode: -340.72 - Last mean moves per episode: 27.37\n",
      "Saving new best model\n",
      "11999993 timesteps\n",
      "Best mean reward: -340.72 - Last mean reward per episode: -340.46 - Last mean moves per episode: 27.36\n",
      "Saving new best model\n",
      "12009987 timesteps\n",
      "Best mean reward: -340.46 - Last mean reward per episode: -340.65 - Last mean moves per episode: 27.37\n",
      "12019992 timesteps\n",
      "Best mean reward: -340.46 - Last mean reward per episode: -339.11 - Last mean moves per episode: 27.32\n",
      "Saving new best model\n",
      "12029987 timesteps\n",
      "Best mean reward: -339.11 - Last mean reward per episode: -339.18 - Last mean moves per episode: 27.31\n",
      "12039982 timesteps\n",
      "Best mean reward: -339.11 - Last mean reward per episode: -337.49 - Last mean moves per episode: 27.25\n",
      "Saving new best model\n",
      "12049997 timesteps\n",
      "Best mean reward: -337.49 - Last mean reward per episode: -337.58 - Last mean moves per episode: 27.24\n",
      "12059950 timesteps\n",
      "Best mean reward: -337.49 - Last mean reward per episode: -336.84 - Last mean moves per episode: 27.22\n",
      "Saving new best model\n",
      "12069994 timesteps\n",
      "Best mean reward: -336.84 - Last mean reward per episode: -336.37 - Last mean moves per episode: 27.21\n",
      "Saving new best model\n",
      "12079996 timesteps\n",
      "Best mean reward: -336.37 - Last mean reward per episode: -335.69 - Last mean moves per episode: 27.19\n",
      "Saving new best model\n",
      "12089990 timesteps\n",
      "Best mean reward: -335.69 - Last mean reward per episode: -335.40 - Last mean moves per episode: 27.20\n",
      "Saving new best model\n",
      "12099999 timesteps\n",
      "Best mean reward: -335.40 - Last mean reward per episode: -334.70 - Last mean moves per episode: 27.16\n",
      "Saving new best model\n",
      "12109941 timesteps\n",
      "Best mean reward: -334.70 - Last mean reward per episode: -334.45 - Last mean moves per episode: 27.15\n",
      "Saving new best model\n",
      "12119983 timesteps\n",
      "Best mean reward: -334.45 - Last mean reward per episode: -335.05 - Last mean moves per episode: 27.17\n",
      "12129998 timesteps\n",
      "Best mean reward: -334.45 - Last mean reward per episode: -334.95 - Last mean moves per episode: 27.16\n",
      "12139971 timesteps\n",
      "Best mean reward: -334.45 - Last mean reward per episode: -333.70 - Last mean moves per episode: 27.10\n",
      "Saving new best model\n",
      "12149997 timesteps\n",
      "Best mean reward: -333.70 - Last mean reward per episode: -335.04 - Last mean moves per episode: 27.14\n",
      "12159974 timesteps\n",
      "Best mean reward: -333.70 - Last mean reward per episode: -334.78 - Last mean moves per episode: 27.14\n",
      "12169996 timesteps\n",
      "Best mean reward: -333.70 - Last mean reward per episode: -333.57 - Last mean moves per episode: 27.10\n",
      "Saving new best model\n",
      "12179997 timesteps\n",
      "Best mean reward: -333.57 - Last mean reward per episode: -332.84 - Last mean moves per episode: 27.07\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12189995 timesteps\n",
      "Best mean reward: -332.84 - Last mean reward per episode: -331.80 - Last mean moves per episode: 27.03\n",
      "Saving new best model\n",
      "12199984 timesteps\n",
      "Best mean reward: -331.80 - Last mean reward per episode: -328.41 - Last mean moves per episode: 26.93\n",
      "Saving new best model\n",
      "12209987 timesteps\n",
      "Best mean reward: -328.41 - Last mean reward per episode: -324.78 - Last mean moves per episode: 26.84\n",
      "Saving new best model\n",
      "12219995 timesteps\n",
      "Best mean reward: -324.78 - Last mean reward per episode: -324.01 - Last mean moves per episode: 26.80\n",
      "Saving new best model\n",
      "12229957 timesteps\n",
      "Best mean reward: -324.01 - Last mean reward per episode: -325.55 - Last mean moves per episode: 26.83\n",
      "12239996 timesteps\n",
      "Best mean reward: -324.01 - Last mean reward per episode: -326.56 - Last mean moves per episode: 26.86\n",
      "12249992 timesteps\n",
      "Best mean reward: -324.01 - Last mean reward per episode: -327.08 - Last mean moves per episode: 26.87\n",
      "12259981 timesteps\n",
      "Best mean reward: -324.01 - Last mean reward per episode: -325.24 - Last mean moves per episode: 26.83\n",
      "12269986 timesteps\n",
      "Best mean reward: -324.01 - Last mean reward per episode: -324.30 - Last mean moves per episode: 26.80\n",
      "12279974 timesteps\n",
      "Best mean reward: -324.01 - Last mean reward per episode: -323.80 - Last mean moves per episode: 26.77\n",
      "Saving new best model\n",
      "12289980 timesteps\n",
      "Best mean reward: -323.80 - Last mean reward per episode: -324.05 - Last mean moves per episode: 26.78\n",
      "12299999 timesteps\n",
      "Best mean reward: -323.80 - Last mean reward per episode: -322.60 - Last mean moves per episode: 26.72\n",
      "Saving new best model\n",
      "12309986 timesteps\n",
      "Best mean reward: -322.60 - Last mean reward per episode: -321.22 - Last mean moves per episode: 26.69\n",
      "Saving new best model\n",
      "12320000 timesteps\n",
      "Best mean reward: -321.22 - Last mean reward per episode: -321.97 - Last mean moves per episode: 26.70\n",
      "12329999 timesteps\n",
      "Best mean reward: -321.22 - Last mean reward per episode: -321.31 - Last mean moves per episode: 26.67\n",
      "12339992 timesteps\n",
      "Best mean reward: -321.22 - Last mean reward per episode: -319.95 - Last mean moves per episode: 26.63\n",
      "Saving new best model\n",
      "12349991 timesteps\n",
      "Best mean reward: -319.95 - Last mean reward per episode: -318.87 - Last mean moves per episode: 26.60\n",
      "Saving new best model\n",
      "12359986 timesteps\n",
      "Best mean reward: -318.87 - Last mean reward per episode: -317.19 - Last mean moves per episode: 26.55\n",
      "Saving new best model\n",
      "12369954 timesteps\n",
      "Best mean reward: -317.19 - Last mean reward per episode: -315.84 - Last mean moves per episode: 26.52\n",
      "Saving new best model\n",
      "12379991 timesteps\n",
      "Best mean reward: -315.84 - Last mean reward per episode: -313.75 - Last mean moves per episode: 26.45\n",
      "Saving new best model\n",
      "12389994 timesteps\n",
      "Best mean reward: -313.75 - Last mean reward per episode: -312.56 - Last mean moves per episode: 26.40\n",
      "Saving new best model\n",
      "12399968 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -314.46 - Last mean moves per episode: 26.44\n",
      "12409984 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -313.61 - Last mean moves per episode: 26.43\n",
      "12419989 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -313.59 - Last mean moves per episode: 26.43\n",
      "12429980 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -313.73 - Last mean moves per episode: 26.43\n",
      "12439987 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -314.05 - Last mean moves per episode: 26.45\n",
      "12449979 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -313.63 - Last mean moves per episode: 26.43\n",
      "12459996 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -312.65 - Last mean moves per episode: 26.40\n",
      "12469994 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -315.78 - Last mean moves per episode: 26.51\n",
      "12479991 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -317.53 - Last mean moves per episode: 26.55\n",
      "12489986 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -316.45 - Last mean moves per episode: 26.52\n",
      "12499996 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -314.52 - Last mean moves per episode: 26.47\n",
      "12509962 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -313.28 - Last mean moves per episode: 26.44\n",
      "12519990 timesteps\n",
      "Best mean reward: -312.56 - Last mean reward per episode: -311.21 - Last mean moves per episode: 26.36\n",
      "Saving new best model\n",
      "12529989 timesteps\n",
      "Best mean reward: -311.21 - Last mean reward per episode: -312.38 - Last mean moves per episode: 26.38\n",
      "12539993 timesteps\n",
      "Best mean reward: -311.21 - Last mean reward per episode: -310.33 - Last mean moves per episode: 26.32\n",
      "Saving new best model\n",
      "12549995 timesteps\n",
      "Best mean reward: -310.33 - Last mean reward per episode: -310.47 - Last mean moves per episode: 26.32\n",
      "12559923 timesteps\n",
      "Best mean reward: -310.33 - Last mean reward per episode: -312.68 - Last mean moves per episode: 26.37\n",
      "12570000 timesteps\n",
      "Best mean reward: -310.33 - Last mean reward per episode: -313.13 - Last mean moves per episode: 26.37\n",
      "12579998 timesteps\n",
      "Best mean reward: -310.33 - Last mean reward per episode: -312.58 - Last mean moves per episode: 26.36\n",
      "12589999 timesteps\n",
      "Best mean reward: -310.33 - Last mean reward per episode: -310.62 - Last mean moves per episode: 26.31\n",
      "12599989 timesteps\n",
      "Best mean reward: -310.33 - Last mean reward per episode: -312.11 - Last mean moves per episode: 26.35\n",
      "12609990 timesteps\n",
      "Best mean reward: -310.33 - Last mean reward per episode: -311.53 - Last mean moves per episode: 26.33\n",
      "12619997 timesteps\n",
      "Best mean reward: -310.33 - Last mean reward per episode: -310.91 - Last mean moves per episode: 26.31\n",
      "12629850 timesteps\n",
      "Best mean reward: -310.33 - Last mean reward per episode: -309.20 - Last mean moves per episode: 26.27\n",
      "Saving new best model\n",
      "12639988 timesteps\n",
      "Best mean reward: -309.20 - Last mean reward per episode: -310.62 - Last mean moves per episode: 26.31\n",
      "12649984 timesteps\n",
      "Best mean reward: -309.20 - Last mean reward per episode: -310.65 - Last mean moves per episode: 26.31\n",
      "12660000 timesteps\n",
      "Best mean reward: -309.20 - Last mean reward per episode: -308.51 - Last mean moves per episode: 26.26\n",
      "Saving new best model\n",
      "12669977 timesteps\n",
      "Best mean reward: -308.51 - Last mean reward per episode: -308.66 - Last mean moves per episode: 26.26\n",
      "12679997 timesteps\n",
      "Best mean reward: -308.51 - Last mean reward per episode: -306.57 - Last mean moves per episode: 26.19\n",
      "Saving new best model\n",
      "12689966 timesteps\n",
      "Best mean reward: -306.57 - Last mean reward per episode: -305.80 - Last mean moves per episode: 26.16\n",
      "Saving new best model\n",
      "12699990 timesteps\n",
      "Best mean reward: -305.80 - Last mean reward per episode: -303.76 - Last mean moves per episode: 26.10\n",
      "Saving new best model\n",
      "12709984 timesteps\n",
      "Best mean reward: -303.76 - Last mean reward per episode: -302.84 - Last mean moves per episode: 26.07\n",
      "Saving new best model\n",
      "12719997 timesteps\n",
      "Best mean reward: -302.84 - Last mean reward per episode: -301.97 - Last mean moves per episode: 26.03\n",
      "Saving new best model\n",
      "12730000 timesteps\n",
      "Best mean reward: -301.97 - Last mean reward per episode: -300.18 - Last mean moves per episode: 25.96\n",
      "Saving new best model\n",
      "12739993 timesteps\n",
      "Best mean reward: -300.18 - Last mean reward per episode: -296.85 - Last mean moves per episode: 25.86\n",
      "Saving new best model\n",
      "12749993 timesteps\n",
      "Best mean reward: -296.85 - Last mean reward per episode: -295.98 - Last mean moves per episode: 25.83\n",
      "Saving new best model\n",
      "12759978 timesteps\n",
      "Best mean reward: -295.98 - Last mean reward per episode: -296.62 - Last mean moves per episode: 25.83\n",
      "12769984 timesteps\n",
      "Best mean reward: -295.98 - Last mean reward per episode: -294.16 - Last mean moves per episode: 25.75\n",
      "Saving new best model\n",
      "12779989 timesteps\n",
      "Best mean reward: -294.16 - Last mean reward per episode: -293.99 - Last mean moves per episode: 25.75\n",
      "Saving new best model\n",
      "12789979 timesteps\n",
      "Best mean reward: -293.99 - Last mean reward per episode: -294.32 - Last mean moves per episode: 25.75\n",
      "12799989 timesteps\n",
      "Best mean reward: -293.99 - Last mean reward per episode: -296.63 - Last mean moves per episode: 25.80\n",
      "12809998 timesteps\n",
      "Best mean reward: -293.99 - Last mean reward per episode: -294.15 - Last mean moves per episode: 25.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12819981 timesteps\n",
      "Best mean reward: -293.99 - Last mean reward per episode: -291.87 - Last mean moves per episode: 25.69\n",
      "Saving new best model\n",
      "12829992 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -293.30 - Last mean moves per episode: 25.71\n",
      "12839984 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -294.67 - Last mean moves per episode: 25.73\n",
      "12849985 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -295.67 - Last mean moves per episode: 25.74\n",
      "12859998 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -296.53 - Last mean moves per episode: 25.74\n",
      "12869975 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -297.49 - Last mean moves per episode: 25.75\n",
      "12879989 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -296.41 - Last mean moves per episode: 25.73\n",
      "12889983 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -294.52 - Last mean moves per episode: 25.68\n",
      "12900000 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -295.10 - Last mean moves per episode: 25.69\n",
      "12909996 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -294.32 - Last mean moves per episode: 25.66\n",
      "12919990 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -294.65 - Last mean moves per episode: 25.66\n",
      "12929997 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -296.46 - Last mean moves per episode: 25.71\n",
      "12939992 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -296.58 - Last mean moves per episode: 25.71\n",
      "12949991 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -298.25 - Last mean moves per episode: 25.75\n",
      "12959980 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -296.96 - Last mean moves per episode: 25.70\n",
      "12969990 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -297.44 - Last mean moves per episode: 25.72\n",
      "12979995 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -297.56 - Last mean moves per episode: 25.72\n",
      "12989991 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -297.46 - Last mean moves per episode: 25.72\n",
      "12999995 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -298.92 - Last mean moves per episode: 25.75\n",
      "13009989 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -298.57 - Last mean moves per episode: 25.75\n",
      "13019993 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -296.12 - Last mean moves per episode: 25.70\n",
      "13029991 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -296.90 - Last mean moves per episode: 25.73\n",
      "13039979 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -295.45 - Last mean moves per episode: 25.67\n",
      "13049994 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -295.15 - Last mean moves per episode: 25.67\n",
      "13059957 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -293.64 - Last mean moves per episode: 25.62\n",
      "13069994 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -293.58 - Last mean moves per episode: 25.62\n",
      "13079985 timesteps\n",
      "Best mean reward: -291.87 - Last mean reward per episode: -290.93 - Last mean moves per episode: 25.53\n",
      "Saving new best model\n",
      "13089993 timesteps\n",
      "Best mean reward: -290.93 - Last mean reward per episode: -289.11 - Last mean moves per episode: 25.50\n",
      "Saving new best model\n",
      "13099989 timesteps\n",
      "Best mean reward: -289.11 - Last mean reward per episode: -287.12 - Last mean moves per episode: 25.46\n",
      "Saving new best model\n",
      "13109986 timesteps\n",
      "Best mean reward: -287.12 - Last mean reward per episode: -285.28 - Last mean moves per episode: 25.42\n",
      "Saving new best model\n",
      "13119995 timesteps\n",
      "Best mean reward: -285.28 - Last mean reward per episode: -284.29 - Last mean moves per episode: 25.39\n",
      "Saving new best model\n",
      "13129983 timesteps\n",
      "Best mean reward: -284.29 - Last mean reward per episode: -284.09 - Last mean moves per episode: 25.39\n",
      "Saving new best model\n",
      "13139985 timesteps\n",
      "Best mean reward: -284.09 - Last mean reward per episode: -285.89 - Last mean moves per episode: 25.43\n",
      "13149981 timesteps\n",
      "Best mean reward: -284.09 - Last mean reward per episode: -283.85 - Last mean moves per episode: 25.37\n",
      "Saving new best model\n",
      "13159991 timesteps\n",
      "Best mean reward: -283.85 - Last mean reward per episode: -283.50 - Last mean moves per episode: 25.36\n",
      "Saving new best model\n",
      "13169993 timesteps\n",
      "Best mean reward: -283.50 - Last mean reward per episode: -280.49 - Last mean moves per episode: 25.28\n",
      "Saving new best model\n",
      "13179959 timesteps\n",
      "Best mean reward: -280.49 - Last mean reward per episode: -278.67 - Last mean moves per episode: 25.23\n",
      "Saving new best model\n",
      "13189989 timesteps\n",
      "Best mean reward: -278.67 - Last mean reward per episode: -278.42 - Last mean moves per episode: 25.23\n",
      "Saving new best model\n",
      "13199997 timesteps\n",
      "Best mean reward: -278.42 - Last mean reward per episode: -277.15 - Last mean moves per episode: 25.19\n",
      "Saving new best model\n",
      "13209981 timesteps\n",
      "Best mean reward: -277.15 - Last mean reward per episode: -278.03 - Last mean moves per episode: 25.22\n",
      "13219975 timesteps\n",
      "Best mean reward: -277.15 - Last mean reward per episode: -276.81 - Last mean moves per episode: 25.19\n",
      "Saving new best model\n",
      "13229996 timesteps\n",
      "Best mean reward: -276.81 - Last mean reward per episode: -275.86 - Last mean moves per episode: 25.17\n",
      "Saving new best model\n",
      "13239996 timesteps\n",
      "Best mean reward: -275.86 - Last mean reward per episode: -276.68 - Last mean moves per episode: 25.19\n",
      "13250000 timesteps\n",
      "Best mean reward: -275.86 - Last mean reward per episode: -275.52 - Last mean moves per episode: 25.18\n",
      "Saving new best model\n",
      "13259938 timesteps\n",
      "Best mean reward: -275.52 - Last mean reward per episode: -276.14 - Last mean moves per episode: 25.17\n",
      "13269990 timesteps\n",
      "Best mean reward: -275.52 - Last mean reward per episode: -275.12 - Last mean moves per episode: 25.13\n",
      "Saving new best model\n",
      "13279990 timesteps\n",
      "Best mean reward: -275.12 - Last mean reward per episode: -274.49 - Last mean moves per episode: 25.10\n",
      "Saving new best model\n",
      "13289987 timesteps\n",
      "Best mean reward: -274.49 - Last mean reward per episode: -276.47 - Last mean moves per episode: 25.15\n",
      "13299987 timesteps\n",
      "Best mean reward: -274.49 - Last mean reward per episode: -276.75 - Last mean moves per episode: 25.16\n",
      "13309991 timesteps\n",
      "Best mean reward: -274.49 - Last mean reward per episode: -275.69 - Last mean moves per episode: 25.14\n",
      "13319990 timesteps\n",
      "Best mean reward: -274.49 - Last mean reward per episode: -275.53 - Last mean moves per episode: 25.14\n",
      "13329979 timesteps\n",
      "Best mean reward: -274.49 - Last mean reward per episode: -272.94 - Last mean moves per episode: 25.07\n",
      "Saving new best model\n",
      "13339985 timesteps\n",
      "Best mean reward: -272.94 - Last mean reward per episode: -275.34 - Last mean moves per episode: 25.14\n",
      "13349991 timesteps\n",
      "Best mean reward: -272.94 - Last mean reward per episode: -274.43 - Last mean moves per episode: 25.12\n",
      "13359988 timesteps\n",
      "Best mean reward: -272.94 - Last mean reward per episode: -275.89 - Last mean moves per episode: 25.15\n",
      "13369996 timesteps\n",
      "Best mean reward: -272.94 - Last mean reward per episode: -277.17 - Last mean moves per episode: 25.18\n",
      "13379990 timesteps\n",
      "Best mean reward: -272.94 - Last mean reward per episode: -275.63 - Last mean moves per episode: 25.15\n",
      "13389959 timesteps\n",
      "Best mean reward: -272.94 - Last mean reward per episode: -274.46 - Last mean moves per episode: 25.12\n",
      "13399985 timesteps\n",
      "Best mean reward: -272.94 - Last mean reward per episode: -273.46 - Last mean moves per episode: 25.09\n",
      "13409981 timesteps\n",
      "Best mean reward: -272.94 - Last mean reward per episode: -272.25 - Last mean moves per episode: 25.06\n",
      "Saving new best model\n",
      "13419988 timesteps\n",
      "Best mean reward: -272.25 - Last mean reward per episode: -272.86 - Last mean moves per episode: 25.07\n",
      "13429981 timesteps\n",
      "Best mean reward: -272.25 - Last mean reward per episode: -271.79 - Last mean moves per episode: 25.04\n",
      "Saving new best model\n",
      "13439917 timesteps\n",
      "Best mean reward: -271.79 - Last mean reward per episode: -270.32 - Last mean moves per episode: 24.98\n",
      "Saving new best model\n",
      "13449904 timesteps\n",
      "Best mean reward: -270.32 - Last mean reward per episode: -271.07 - Last mean moves per episode: 24.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13459995 timesteps\n",
      "Best mean reward: -270.32 - Last mean reward per episode: -272.15 - Last mean moves per episode: 25.00\n",
      "13469962 timesteps\n",
      "Best mean reward: -270.32 - Last mean reward per episode: -273.97 - Last mean moves per episode: 25.03\n",
      "13479953 timesteps\n",
      "Best mean reward: -270.32 - Last mean reward per episode: -274.04 - Last mean moves per episode: 25.03\n",
      "13489992 timesteps\n",
      "Best mean reward: -270.32 - Last mean reward per episode: -273.68 - Last mean moves per episode: 25.02\n",
      "13499980 timesteps\n",
      "Best mean reward: -270.32 - Last mean reward per episode: -273.22 - Last mean moves per episode: 25.01\n",
      "13509998 timesteps\n",
      "Best mean reward: -270.32 - Last mean reward per episode: -270.28 - Last mean moves per episode: 24.95\n",
      "Saving new best model\n",
      "13519998 timesteps\n",
      "Best mean reward: -270.28 - Last mean reward per episode: -270.87 - Last mean moves per episode: 24.97\n",
      "13529971 timesteps\n",
      "Best mean reward: -270.28 - Last mean reward per episode: -272.27 - Last mean moves per episode: 25.02\n",
      "13539998 timesteps\n",
      "Best mean reward: -270.28 - Last mean reward per episode: -270.24 - Last mean moves per episode: 24.96\n",
      "Saving new best model\n",
      "13549991 timesteps\n",
      "Best mean reward: -270.24 - Last mean reward per episode: -268.97 - Last mean moves per episode: 24.92\n",
      "Saving new best model\n",
      "13559997 timesteps\n",
      "Best mean reward: -268.97 - Last mean reward per episode: -268.76 - Last mean moves per episode: 24.92\n",
      "Saving new best model\n",
      "13570000 timesteps\n",
      "Best mean reward: -268.76 - Last mean reward per episode: -267.88 - Last mean moves per episode: 24.89\n",
      "Saving new best model\n",
      "13579992 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -269.81 - Last mean moves per episode: 24.96\n",
      "13590000 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -269.30 - Last mean moves per episode: 24.93\n",
      "13599989 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -270.19 - Last mean moves per episode: 24.93\n",
      "13609995 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -269.51 - Last mean moves per episode: 24.91\n",
      "13619998 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -269.06 - Last mean moves per episode: 24.91\n",
      "13629997 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -270.28 - Last mean moves per episode: 24.92\n",
      "13639972 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -270.36 - Last mean moves per episode: 24.92\n",
      "13649967 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -273.33 - Last mean moves per episode: 24.99\n",
      "13659982 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -272.97 - Last mean moves per episode: 24.99\n",
      "13669992 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -272.22 - Last mean moves per episode: 24.96\n",
      "13679996 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -272.71 - Last mean moves per episode: 24.97\n",
      "13689983 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -272.36 - Last mean moves per episode: 24.96\n",
      "13699986 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -271.08 - Last mean moves per episode: 24.93\n",
      "13709991 timesteps\n",
      "Best mean reward: -267.88 - Last mean reward per episode: -267.33 - Last mean moves per episode: 24.83\n",
      "Saving new best model\n",
      "13719977 timesteps\n",
      "Best mean reward: -267.33 - Last mean reward per episode: -266.22 - Last mean moves per episode: 24.81\n",
      "Saving new best model\n",
      "13729993 timesteps\n",
      "Best mean reward: -266.22 - Last mean reward per episode: -264.38 - Last mean moves per episode: 24.75\n",
      "Saving new best model\n",
      "13739993 timesteps\n",
      "Best mean reward: -264.38 - Last mean reward per episode: -262.40 - Last mean moves per episode: 24.67\n",
      "Saving new best model\n",
      "13749994 timesteps\n",
      "Best mean reward: -262.40 - Last mean reward per episode: -262.79 - Last mean moves per episode: 24.67\n",
      "13759978 timesteps\n",
      "Best mean reward: -262.40 - Last mean reward per episode: -263.47 - Last mean moves per episode: 24.67\n",
      "13769993 timesteps\n",
      "Best mean reward: -262.40 - Last mean reward per episode: -262.03 - Last mean moves per episode: 24.63\n",
      "Saving new best model\n",
      "13779993 timesteps\n",
      "Best mean reward: -262.03 - Last mean reward per episode: -262.36 - Last mean moves per episode: 24.64\n",
      "13789986 timesteps\n",
      "Best mean reward: -262.03 - Last mean reward per episode: -263.03 - Last mean moves per episode: 24.67\n",
      "13799993 timesteps\n",
      "Best mean reward: -262.03 - Last mean reward per episode: -261.35 - Last mean moves per episode: 24.60\n",
      "Saving new best model\n",
      "13809987 timesteps\n",
      "Best mean reward: -261.35 - Last mean reward per episode: -261.15 - Last mean moves per episode: 24.59\n",
      "Saving new best model\n",
      "13819975 timesteps\n",
      "Best mean reward: -261.15 - Last mean reward per episode: -259.46 - Last mean moves per episode: 24.54\n",
      "Saving new best model\n",
      "13829975 timesteps\n",
      "Best mean reward: -259.46 - Last mean reward per episode: -262.43 - Last mean moves per episode: 24.59\n",
      "13840000 timesteps\n",
      "Best mean reward: -259.46 - Last mean reward per episode: -261.41 - Last mean moves per episode: 24.56\n",
      "13849995 timesteps\n",
      "Best mean reward: -259.46 - Last mean reward per episode: -259.99 - Last mean moves per episode: 24.52\n",
      "13859994 timesteps\n",
      "Best mean reward: -259.46 - Last mean reward per episode: -257.60 - Last mean moves per episode: 24.45\n",
      "Saving new best model\n",
      "13869993 timesteps\n",
      "Best mean reward: -257.60 - Last mean reward per episode: -256.77 - Last mean moves per episode: 24.43\n",
      "Saving new best model\n",
      "13879978 timesteps\n",
      "Best mean reward: -256.77 - Last mean reward per episode: -254.21 - Last mean moves per episode: 24.36\n",
      "Saving new best model\n",
      "13890000 timesteps\n",
      "Best mean reward: -254.21 - Last mean reward per episode: -252.56 - Last mean moves per episode: 24.31\n",
      "Saving new best model\n",
      "13899999 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -253.63 - Last mean moves per episode: 24.31\n",
      "13909995 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -252.64 - Last mean moves per episode: 24.28\n",
      "13919990 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -254.00 - Last mean moves per episode: 24.30\n",
      "13929991 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -254.08 - Last mean moves per episode: 24.32\n",
      "13939971 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -253.25 - Last mean moves per episode: 24.29\n",
      "13949989 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -255.42 - Last mean moves per episode: 24.35\n",
      "13959990 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -253.86 - Last mean moves per episode: 24.31\n",
      "13969994 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -254.74 - Last mean moves per episode: 24.34\n",
      "13979982 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -256.02 - Last mean moves per episode: 24.39\n",
      "13989985 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -254.68 - Last mean moves per episode: 24.33\n",
      "13999991 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -254.45 - Last mean moves per episode: 24.34\n",
      "14009996 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -255.49 - Last mean moves per episode: 24.36\n",
      "14019976 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -255.13 - Last mean moves per episode: 24.34\n",
      "14029979 timesteps\n",
      "Best mean reward: -252.56 - Last mean reward per episode: -252.45 - Last mean moves per episode: 24.24\n",
      "Saving new best model\n",
      "14039983 timesteps\n",
      "Best mean reward: -252.45 - Last mean reward per episode: -252.57 - Last mean moves per episode: 24.25\n",
      "14049997 timesteps\n",
      "Best mean reward: -252.45 - Last mean reward per episode: -253.56 - Last mean moves per episode: 24.28\n",
      "14059997 timesteps\n",
      "Best mean reward: -252.45 - Last mean reward per episode: -256.28 - Last mean moves per episode: 24.35\n",
      "14069989 timesteps\n",
      "Best mean reward: -252.45 - Last mean reward per episode: -258.36 - Last mean moves per episode: 24.41\n",
      "14079999 timesteps\n",
      "Best mean reward: -252.45 - Last mean reward per episode: -251.77 - Last mean moves per episode: 24.25\n",
      "Saving new best model\n",
      "14089984 timesteps\n",
      "Best mean reward: -251.77 - Last mean reward per episode: -251.94 - Last mean moves per episode: 24.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14099997 timesteps\n",
      "Best mean reward: -251.77 - Last mean reward per episode: -251.35 - Last mean moves per episode: 24.24\n",
      "Saving new best model\n",
      "14109997 timesteps\n",
      "Best mean reward: -251.35 - Last mean reward per episode: -249.02 - Last mean moves per episode: 24.18\n",
      "Saving new best model\n",
      "14119981 timesteps\n",
      "Best mean reward: -249.02 - Last mean reward per episode: -249.02 - Last mean moves per episode: 24.17\n",
      "Saving new best model\n",
      "14130000 timesteps\n",
      "Best mean reward: -249.02 - Last mean reward per episode: -252.55 - Last mean moves per episode: 24.25\n",
      "14139985 timesteps\n",
      "Best mean reward: -249.02 - Last mean reward per episode: -249.11 - Last mean moves per episode: 24.16\n",
      "14149965 timesteps\n",
      "Best mean reward: -249.02 - Last mean reward per episode: -247.94 - Last mean moves per episode: 24.14\n",
      "Saving new best model\n",
      "14159996 timesteps\n",
      "Best mean reward: -247.94 - Last mean reward per episode: -245.51 - Last mean moves per episode: 24.09\n",
      "Saving new best model\n",
      "14169984 timesteps\n",
      "Best mean reward: -245.51 - Last mean reward per episode: -244.38 - Last mean moves per episode: 24.04\n",
      "Saving new best model\n",
      "14179984 timesteps\n",
      "Best mean reward: -244.38 - Last mean reward per episode: -244.59 - Last mean moves per episode: 24.05\n",
      "14189997 timesteps\n",
      "Best mean reward: -244.38 - Last mean reward per episode: -244.07 - Last mean moves per episode: 24.04\n",
      "Saving new best model\n",
      "14199984 timesteps\n",
      "Best mean reward: -244.07 - Last mean reward per episode: -243.91 - Last mean moves per episode: 24.03\n",
      "Saving new best model\n",
      "14209983 timesteps\n",
      "Best mean reward: -243.91 - Last mean reward per episode: -243.11 - Last mean moves per episode: 24.00\n",
      "Saving new best model\n",
      "14219997 timesteps\n",
      "Best mean reward: -243.11 - Last mean reward per episode: -242.73 - Last mean moves per episode: 23.97\n",
      "Saving new best model\n",
      "14229995 timesteps\n",
      "Best mean reward: -242.73 - Last mean reward per episode: -242.81 - Last mean moves per episode: 23.97\n",
      "14239996 timesteps\n",
      "Best mean reward: -242.73 - Last mean reward per episode: -243.45 - Last mean moves per episode: 23.97\n",
      "14249983 timesteps\n",
      "Best mean reward: -242.73 - Last mean reward per episode: -241.66 - Last mean moves per episode: 23.91\n",
      "Saving new best model\n",
      "14259991 timesteps\n",
      "Best mean reward: -241.66 - Last mean reward per episode: -241.16 - Last mean moves per episode: 23.91\n",
      "Saving new best model\n",
      "14269985 timesteps\n",
      "Best mean reward: -241.16 - Last mean reward per episode: -242.50 - Last mean moves per episode: 23.95\n",
      "14279998 timesteps\n",
      "Best mean reward: -241.16 - Last mean reward per episode: -242.36 - Last mean moves per episode: 23.95\n",
      "14289992 timesteps\n",
      "Best mean reward: -241.16 - Last mean reward per episode: -240.28 - Last mean moves per episode: 23.90\n",
      "Saving new best model\n",
      "14299982 timesteps\n",
      "Best mean reward: -240.28 - Last mean reward per episode: -236.99 - Last mean moves per episode: 23.81\n",
      "Saving new best model\n",
      "14309992 timesteps\n",
      "Best mean reward: -236.99 - Last mean reward per episode: -237.95 - Last mean moves per episode: 23.82\n",
      "14319995 timesteps\n",
      "Best mean reward: -236.99 - Last mean reward per episode: -238.39 - Last mean moves per episode: 23.84\n",
      "14329982 timesteps\n",
      "Best mean reward: -236.99 - Last mean reward per episode: -236.57 - Last mean moves per episode: 23.78\n",
      "Saving new best model\n",
      "14339824 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -237.81 - Last mean moves per episode: 23.82\n",
      "14349983 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -240.02 - Last mean moves per episode: 23.86\n",
      "14359972 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -239.36 - Last mean moves per episode: 23.86\n",
      "14369986 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -236.68 - Last mean moves per episode: 23.80\n",
      "14379984 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -238.91 - Last mean moves per episode: 23.84\n",
      "14389984 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -238.62 - Last mean moves per episode: 23.82\n",
      "14399976 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -238.96 - Last mean moves per episode: 23.82\n",
      "14409996 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -239.02 - Last mean moves per episode: 23.81\n",
      "14419999 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -237.23 - Last mean moves per episode: 23.75\n",
      "14429960 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -237.23 - Last mean moves per episode: 23.75\n",
      "14439984 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -237.93 - Last mean moves per episode: 23.77\n",
      "14449998 timesteps\n",
      "Best mean reward: -236.57 - Last mean reward per episode: -236.37 - Last mean moves per episode: 23.73\n",
      "Saving new best model\n",
      "14459960 timesteps\n",
      "Best mean reward: -236.37 - Last mean reward per episode: -234.50 - Last mean moves per episode: 23.68\n",
      "Saving new best model\n",
      "14470000 timesteps\n",
      "Best mean reward: -234.50 - Last mean reward per episode: -236.06 - Last mean moves per episode: 23.73\n",
      "14479986 timesteps\n",
      "Best mean reward: -234.50 - Last mean reward per episode: -234.78 - Last mean moves per episode: 23.70\n",
      "14489991 timesteps\n",
      "Best mean reward: -234.50 - Last mean reward per episode: -236.29 - Last mean moves per episode: 23.75\n",
      "14499959 timesteps\n",
      "Best mean reward: -234.50 - Last mean reward per episode: -234.54 - Last mean moves per episode: 23.69\n",
      "14509997 timesteps\n",
      "Best mean reward: -234.50 - Last mean reward per episode: -233.25 - Last mean moves per episode: 23.65\n",
      "Saving new best model\n",
      "14519971 timesteps\n",
      "Best mean reward: -233.25 - Last mean reward per episode: -232.68 - Last mean moves per episode: 23.63\n",
      "Saving new best model\n",
      "14529996 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -235.62 - Last mean moves per episode: 23.71\n",
      "14539961 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -234.12 - Last mean moves per episode: 23.67\n",
      "14549984 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -233.39 - Last mean moves per episode: 23.64\n",
      "14559984 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -235.26 - Last mean moves per episode: 23.70\n",
      "14569997 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -239.66 - Last mean moves per episode: 23.79\n",
      "14579975 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -238.36 - Last mean moves per episode: 23.74\n",
      "14589978 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -238.91 - Last mean moves per episode: 23.76\n",
      "14599997 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -238.93 - Last mean moves per episode: 23.75\n",
      "14609987 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -237.98 - Last mean moves per episode: 23.71\n",
      "14620000 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -237.59 - Last mean moves per episode: 23.70\n",
      "14629988 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -236.95 - Last mean moves per episode: 23.69\n",
      "14639999 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -236.61 - Last mean moves per episode: 23.68\n",
      "14649999 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -236.59 - Last mean moves per episode: 23.68\n",
      "14659989 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -237.75 - Last mean moves per episode: 23.70\n",
      "14669989 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -236.71 - Last mean moves per episode: 23.66\n",
      "14679987 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -237.41 - Last mean moves per episode: 23.69\n",
      "14689986 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -237.73 - Last mean moves per episode: 23.69\n",
      "14699984 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -238.66 - Last mean moves per episode: 23.71\n",
      "14709995 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -238.97 - Last mean moves per episode: 23.71\n",
      "14719978 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -238.57 - Last mean moves per episode: 23.70\n",
      "14729990 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -239.18 - Last mean moves per episode: 23.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14739984 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -238.99 - Last mean moves per episode: 23.69\n",
      "14749993 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -241.99 - Last mean moves per episode: 23.76\n",
      "14759995 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -242.84 - Last mean moves per episode: 23.78\n",
      "14769994 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -239.00 - Last mean moves per episode: 23.69\n",
      "14779962 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -239.91 - Last mean moves per episode: 23.72\n",
      "14789986 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -239.45 - Last mean moves per episode: 23.69\n",
      "14799979 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -238.29 - Last mean moves per episode: 23.64\n",
      "14809997 timesteps\n",
      "Best mean reward: -232.68 - Last mean reward per episode: -232.63 - Last mean moves per episode: 23.52\n",
      "Saving new best model\n",
      "14819982 timesteps\n",
      "Best mean reward: -232.63 - Last mean reward per episode: -232.58 - Last mean moves per episode: 23.51\n",
      "Saving new best model\n",
      "14829985 timesteps\n",
      "Best mean reward: -232.58 - Last mean reward per episode: -231.89 - Last mean moves per episode: 23.48\n",
      "Saving new best model\n",
      "14839996 timesteps\n",
      "Best mean reward: -231.89 - Last mean reward per episode: -231.08 - Last mean moves per episode: 23.46\n",
      "Saving new best model\n",
      "14849936 timesteps\n",
      "Best mean reward: -231.08 - Last mean reward per episode: -228.49 - Last mean moves per episode: 23.40\n",
      "Saving new best model\n",
      "14859976 timesteps\n",
      "Best mean reward: -228.49 - Last mean reward per episode: -229.71 - Last mean moves per episode: 23.44\n",
      "14869993 timesteps\n",
      "Best mean reward: -228.49 - Last mean reward per episode: -229.68 - Last mean moves per episode: 23.43\n",
      "14879977 timesteps\n",
      "Best mean reward: -228.49 - Last mean reward per episode: -231.16 - Last mean moves per episode: 23.46\n",
      "14889859 timesteps\n",
      "Best mean reward: -228.49 - Last mean reward per episode: -230.06 - Last mean moves per episode: 23.43\n",
      "14899993 timesteps\n",
      "Best mean reward: -228.49 - Last mean reward per episode: -230.64 - Last mean moves per episode: 23.44\n",
      "14909990 timesteps\n",
      "Best mean reward: -228.49 - Last mean reward per episode: -230.00 - Last mean moves per episode: 23.42\n",
      "14919994 timesteps\n",
      "Best mean reward: -228.49 - Last mean reward per episode: -230.17 - Last mean moves per episode: 23.44\n",
      "14929965 timesteps\n",
      "Best mean reward: -228.49 - Last mean reward per episode: -229.81 - Last mean moves per episode: 23.42\n",
      "14939983 timesteps\n",
      "Best mean reward: -228.49 - Last mean reward per episode: -228.69 - Last mean moves per episode: 23.39\n",
      "14949987 timesteps\n",
      "Best mean reward: -228.49 - Last mean reward per episode: -227.79 - Last mean moves per episode: 23.40\n",
      "Saving new best model\n",
      "14959989 timesteps\n",
      "Best mean reward: -227.79 - Last mean reward per episode: -226.35 - Last mean moves per episode: 23.35\n",
      "Saving new best model\n",
      "14969975 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -228.88 - Last mean moves per episode: 23.41\n",
      "14979985 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -227.82 - Last mean moves per episode: 23.39\n",
      "14989881 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -227.63 - Last mean moves per episode: 23.39\n",
      "14999985 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -230.29 - Last mean moves per episode: 23.44\n",
      "15009990 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -230.32 - Last mean moves per episode: 23.44\n",
      "15019982 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -228.04 - Last mean moves per episode: 23.37\n",
      "15029998 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -227.33 - Last mean moves per episode: 23.36\n",
      "15039984 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -227.03 - Last mean moves per episode: 23.35\n",
      "15049992 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -226.91 - Last mean moves per episode: 23.34\n",
      "15059971 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -226.80 - Last mean moves per episode: 23.33\n",
      "15070000 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -227.51 - Last mean moves per episode: 23.36\n",
      "15079997 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -227.65 - Last mean moves per episode: 23.36\n",
      "15090000 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -231.35 - Last mean moves per episode: 23.45\n",
      "15099983 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -232.13 - Last mean moves per episode: 23.47\n",
      "15109991 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -230.80 - Last mean moves per episode: 23.43\n",
      "15119981 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -230.43 - Last mean moves per episode: 23.41\n",
      "15129992 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -228.57 - Last mean moves per episode: 23.37\n",
      "15139997 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -229.24 - Last mean moves per episode: 23.39\n",
      "15149995 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -228.84 - Last mean moves per episode: 23.38\n",
      "15159999 timesteps\n",
      "Best mean reward: -226.35 - Last mean reward per episode: -224.76 - Last mean moves per episode: 23.27\n",
      "Saving new best model\n",
      "15169993 timesteps\n",
      "Best mean reward: -224.76 - Last mean reward per episode: -225.76 - Last mean moves per episode: 23.30\n",
      "15179980 timesteps\n",
      "Best mean reward: -224.76 - Last mean reward per episode: -226.29 - Last mean moves per episode: 23.30\n",
      "15189997 timesteps\n",
      "Best mean reward: -224.76 - Last mean reward per episode: -227.04 - Last mean moves per episode: 23.33\n",
      "15199985 timesteps\n",
      "Best mean reward: -224.76 - Last mean reward per episode: -223.02 - Last mean moves per episode: 23.23\n",
      "Saving new best model\n",
      "15209987 timesteps\n",
      "Best mean reward: -223.02 - Last mean reward per episode: -220.97 - Last mean moves per episode: 23.17\n",
      "Saving new best model\n",
      "15219926 timesteps\n",
      "Best mean reward: -220.97 - Last mean reward per episode: -218.33 - Last mean moves per episode: 23.10\n",
      "Saving new best model\n",
      "15229993 timesteps\n",
      "Best mean reward: -218.33 - Last mean reward per episode: -215.70 - Last mean moves per episode: 23.04\n",
      "Saving new best model\n",
      "15239998 timesteps\n",
      "Best mean reward: -215.70 - Last mean reward per episode: -214.66 - Last mean moves per episode: 22.99\n",
      "Saving new best model\n",
      "15249992 timesteps\n",
      "Best mean reward: -214.66 - Last mean reward per episode: -215.17 - Last mean moves per episode: 22.99\n",
      "15259989 timesteps\n",
      "Best mean reward: -214.66 - Last mean reward per episode: -213.81 - Last mean moves per episode: 22.95\n",
      "Saving new best model\n",
      "15269988 timesteps\n",
      "Best mean reward: -213.81 - Last mean reward per episode: -213.87 - Last mean moves per episode: 22.95\n",
      "15279985 timesteps\n",
      "Best mean reward: -213.81 - Last mean reward per episode: -211.02 - Last mean moves per episode: 22.87\n",
      "Saving new best model\n",
      "15289994 timesteps\n",
      "Best mean reward: -211.02 - Last mean reward per episode: -211.38 - Last mean moves per episode: 22.89\n",
      "15299944 timesteps\n",
      "Best mean reward: -211.02 - Last mean reward per episode: -211.52 - Last mean moves per episode: 22.87\n",
      "15309981 timesteps\n",
      "Best mean reward: -211.02 - Last mean reward per episode: -212.10 - Last mean moves per episode: 22.89\n",
      "15319989 timesteps\n",
      "Best mean reward: -211.02 - Last mean reward per episode: -211.72 - Last mean moves per episode: 22.88\n",
      "15329998 timesteps\n",
      "Best mean reward: -211.02 - Last mean reward per episode: -210.80 - Last mean moves per episode: 22.85\n",
      "Saving new best model\n",
      "15339991 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -214.73 - Last mean moves per episode: 22.96\n",
      "15349998 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -213.67 - Last mean moves per episode: 22.92\n",
      "15359994 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -213.46 - Last mean moves per episode: 22.90\n",
      "15369982 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -212.78 - Last mean moves per episode: 22.86\n",
      "15379994 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -212.19 - Last mean moves per episode: 22.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15389999 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -212.06 - Last mean moves per episode: 22.85\n",
      "15399991 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -211.18 - Last mean moves per episode: 22.82\n",
      "15409993 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -212.14 - Last mean moves per episode: 22.83\n",
      "15419984 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -211.80 - Last mean moves per episode: 22.82\n",
      "15429980 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -212.07 - Last mean moves per episode: 22.83\n",
      "15439987 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -211.11 - Last mean moves per episode: 22.80\n",
      "15449978 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -212.72 - Last mean moves per episode: 22.84\n",
      "15459952 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -213.40 - Last mean moves per episode: 22.86\n",
      "15469993 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -212.62 - Last mean moves per episode: 22.84\n",
      "15479991 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -213.08 - Last mean moves per episode: 22.85\n",
      "15489998 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -213.98 - Last mean moves per episode: 22.88\n",
      "15499885 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -212.49 - Last mean moves per episode: 22.83\n",
      "15509992 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -215.99 - Last mean moves per episode: 22.92\n",
      "15519981 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -215.88 - Last mean moves per episode: 22.91\n",
      "15529996 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -213.36 - Last mean moves per episode: 22.84\n",
      "15539996 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -213.60 - Last mean moves per episode: 22.83\n",
      "15549975 timesteps\n",
      "Best mean reward: -210.80 - Last mean reward per episode: -209.83 - Last mean moves per episode: 22.73\n",
      "Saving new best model\n",
      "15559993 timesteps\n",
      "Best mean reward: -209.83 - Last mean reward per episode: -206.85 - Last mean moves per episode: 22.66\n",
      "Saving new best model\n",
      "15569978 timesteps\n",
      "Best mean reward: -206.85 - Last mean reward per episode: -206.76 - Last mean moves per episode: 22.66\n",
      "Saving new best model\n",
      "15579995 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -211.28 - Last mean moves per episode: 22.79\n",
      "15589994 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -212.42 - Last mean moves per episode: 22.83\n",
      "15599979 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -213.60 - Last mean moves per episode: 22.87\n",
      "15609993 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -216.15 - Last mean moves per episode: 22.90\n",
      "15619987 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -218.71 - Last mean moves per episode: 22.98\n",
      "15629984 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -217.26 - Last mean moves per episode: 22.95\n",
      "15639994 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -216.47 - Last mean moves per episode: 22.95\n",
      "15649995 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -218.39 - Last mean moves per episode: 22.99\n",
      "15659997 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -219.28 - Last mean moves per episode: 23.01\n",
      "15669999 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -221.68 - Last mean moves per episode: 23.07\n",
      "15679983 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -219.61 - Last mean moves per episode: 23.02\n",
      "15689995 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -219.55 - Last mean moves per episode: 23.00\n",
      "15699988 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -218.74 - Last mean moves per episode: 22.98\n",
      "15709996 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -218.45 - Last mean moves per episode: 22.98\n",
      "15719982 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -218.68 - Last mean moves per episode: 22.99\n",
      "15729991 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -219.93 - Last mean moves per episode: 23.04\n",
      "15739968 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -215.91 - Last mean moves per episode: 22.93\n",
      "15749993 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -215.66 - Last mean moves per episode: 22.92\n",
      "15759999 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -216.18 - Last mean moves per episode: 22.93\n",
      "15769981 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -216.40 - Last mean moves per episode: 22.92\n",
      "15779985 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -216.40 - Last mean moves per episode: 22.92\n",
      "15789986 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -217.30 - Last mean moves per episode: 22.93\n",
      "15799978 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -214.80 - Last mean moves per episode: 22.87\n",
      "15809967 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -209.50 - Last mean moves per episode: 22.73\n",
      "15819997 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -208.14 - Last mean moves per episode: 22.69\n",
      "15829989 timesteps\n",
      "Best mean reward: -206.76 - Last mean reward per episode: -204.69 - Last mean moves per episode: 22.59\n",
      "Saving new best model\n",
      "15840000 timesteps\n",
      "Best mean reward: -204.69 - Last mean reward per episode: -206.33 - Last mean moves per episode: 22.63\n",
      "15849996 timesteps\n",
      "Best mean reward: -204.69 - Last mean reward per episode: -207.13 - Last mean moves per episode: 22.62\n",
      "15859982 timesteps\n",
      "Best mean reward: -204.69 - Last mean reward per episode: -207.96 - Last mean moves per episode: 22.62\n",
      "15869985 timesteps\n",
      "Best mean reward: -204.69 - Last mean reward per episode: -206.84 - Last mean moves per episode: 22.58\n",
      "15879995 timesteps\n",
      "Best mean reward: -204.69 - Last mean reward per episode: -203.02 - Last mean moves per episode: 22.48\n",
      "Saving new best model\n",
      "15889987 timesteps\n",
      "Best mean reward: -203.02 - Last mean reward per episode: -202.06 - Last mean moves per episode: 22.45\n",
      "Saving new best model\n",
      "15899994 timesteps\n",
      "Best mean reward: -202.06 - Last mean reward per episode: -200.22 - Last mean moves per episode: 22.40\n",
      "Saving new best model\n",
      "15909996 timesteps\n",
      "Best mean reward: -200.22 - Last mean reward per episode: -201.23 - Last mean moves per episode: 22.41\n",
      "15919985 timesteps\n",
      "Best mean reward: -200.22 - Last mean reward per episode: -200.36 - Last mean moves per episode: 22.37\n",
      "15929980 timesteps\n",
      "Best mean reward: -200.22 - Last mean reward per episode: -198.66 - Last mean moves per episode: 22.32\n",
      "Saving new best model\n",
      "15939980 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -198.67 - Last mean moves per episode: 22.31\n",
      "15949989 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -198.98 - Last mean moves per episode: 22.32\n",
      "15959978 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -201.00 - Last mean moves per episode: 22.37\n",
      "15969995 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -202.50 - Last mean moves per episode: 22.40\n",
      "15979996 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -200.29 - Last mean moves per episode: 22.33\n",
      "15989987 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -202.23 - Last mean moves per episode: 22.37\n",
      "15999997 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -200.60 - Last mean moves per episode: 22.36\n",
      "16009993 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -198.93 - Last mean moves per episode: 22.33\n",
      "16019991 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -199.81 - Last mean moves per episode: 22.34\n",
      "16029998 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -201.41 - Last mean moves per episode: 22.36\n",
      "16039966 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -200.17 - Last mean moves per episode: 22.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16049984 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -202.89 - Last mean moves per episode: 22.41\n",
      "16059992 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -200.77 - Last mean moves per episode: 22.34\n",
      "16069993 timesteps\n",
      "Best mean reward: -198.66 - Last mean reward per episode: -195.97 - Last mean moves per episode: 22.23\n",
      "Saving new best model\n",
      "16079985 timesteps\n",
      "Best mean reward: -195.97 - Last mean reward per episode: -195.43 - Last mean moves per episode: 22.21\n",
      "Saving new best model\n",
      "16089997 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -196.30 - Last mean moves per episode: 22.23\n",
      "16100000 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -195.71 - Last mean moves per episode: 22.19\n",
      "16109994 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -199.85 - Last mean moves per episode: 22.30\n",
      "16119992 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -199.48 - Last mean moves per episode: 22.29\n",
      "16129977 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -198.15 - Last mean moves per episode: 22.27\n",
      "16139988 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -200.53 - Last mean moves per episode: 22.33\n",
      "16149981 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -202.31 - Last mean moves per episode: 22.38\n",
      "16159990 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -202.63 - Last mean moves per episode: 22.40\n",
      "16169994 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -202.35 - Last mean moves per episode: 22.37\n",
      "16179988 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -203.09 - Last mean moves per episode: 22.40\n",
      "16189999 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -197.33 - Last mean moves per episode: 22.27\n",
      "16199971 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -199.24 - Last mean moves per episode: 22.33\n",
      "16209976 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -198.91 - Last mean moves per episode: 22.33\n",
      "16219988 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -198.24 - Last mean moves per episode: 22.31\n",
      "16229995 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -197.46 - Last mean moves per episode: 22.29\n",
      "16239994 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -197.56 - Last mean moves per episode: 22.28\n",
      "16249982 timesteps\n",
      "Best mean reward: -195.43 - Last mean reward per episode: -192.61 - Last mean moves per episode: 22.16\n",
      "Saving new best model\n",
      "16259992 timesteps\n",
      "Best mean reward: -192.61 - Last mean reward per episode: -193.06 - Last mean moves per episode: 22.16\n",
      "16270000 timesteps\n",
      "Best mean reward: -192.61 - Last mean reward per episode: -190.12 - Last mean moves per episode: 22.09\n",
      "Saving new best model\n",
      "16279997 timesteps\n",
      "Best mean reward: -190.12 - Last mean reward per episode: -188.75 - Last mean moves per episode: 22.07\n",
      "Saving new best model\n",
      "16289989 timesteps\n",
      "Best mean reward: -188.75 - Last mean reward per episode: -190.22 - Last mean moves per episode: 22.10\n",
      "16299997 timesteps\n",
      "Best mean reward: -188.75 - Last mean reward per episode: -187.54 - Last mean moves per episode: 22.01\n",
      "Saving new best model\n",
      "16309997 timesteps\n",
      "Best mean reward: -187.54 - Last mean reward per episode: -186.70 - Last mean moves per episode: 21.98\n",
      "Saving new best model\n",
      "16319990 timesteps\n",
      "Best mean reward: -186.70 - Last mean reward per episode: -186.78 - Last mean moves per episode: 22.00\n",
      "16329989 timesteps\n",
      "Best mean reward: -186.70 - Last mean reward per episode: -182.66 - Last mean moves per episode: 21.90\n",
      "Saving new best model\n",
      "16339997 timesteps\n",
      "Best mean reward: -182.66 - Last mean reward per episode: -181.60 - Last mean moves per episode: 21.86\n",
      "Saving new best model\n",
      "16349975 timesteps\n",
      "Best mean reward: -181.60 - Last mean reward per episode: -178.90 - Last mean moves per episode: 21.78\n",
      "Saving new best model\n",
      "16359968 timesteps\n",
      "Best mean reward: -178.90 - Last mean reward per episode: -176.07 - Last mean moves per episode: 21.71\n",
      "Saving new best model\n",
      "16370000 timesteps\n",
      "Best mean reward: -176.07 - Last mean reward per episode: -177.40 - Last mean moves per episode: 21.73\n",
      "16379991 timesteps\n",
      "Best mean reward: -176.07 - Last mean reward per episode: -177.96 - Last mean moves per episode: 21.73\n",
      "16389987 timesteps\n",
      "Best mean reward: -176.07 - Last mean reward per episode: -174.47 - Last mean moves per episode: 21.65\n",
      "Saving new best model\n",
      "16399995 timesteps\n",
      "Best mean reward: -174.47 - Last mean reward per episode: -173.19 - Last mean moves per episode: 21.58\n",
      "Saving new best model\n",
      "16409991 timesteps\n",
      "Best mean reward: -173.19 - Last mean reward per episode: -172.33 - Last mean moves per episode: 21.54\n",
      "Saving new best model\n",
      "16419996 timesteps\n",
      "Best mean reward: -172.33 - Last mean reward per episode: -170.29 - Last mean moves per episode: 21.48\n",
      "Saving new best model\n",
      "16429991 timesteps\n",
      "Best mean reward: -170.29 - Last mean reward per episode: -170.37 - Last mean moves per episode: 21.47\n",
      "16439995 timesteps\n",
      "Best mean reward: -170.29 - Last mean reward per episode: -171.68 - Last mean moves per episode: 21.49\n",
      "16450000 timesteps\n",
      "Best mean reward: -170.29 - Last mean reward per episode: -171.94 - Last mean moves per episode: 21.49\n",
      "16459984 timesteps\n",
      "Best mean reward: -170.29 - Last mean reward per episode: -172.43 - Last mean moves per episode: 21.51\n",
      "16469994 timesteps\n",
      "Best mean reward: -170.29 - Last mean reward per episode: -173.57 - Last mean moves per episode: 21.53\n",
      "16479992 timesteps\n",
      "Best mean reward: -170.29 - Last mean reward per episode: -170.78 - Last mean moves per episode: 21.46\n",
      "16489992 timesteps\n",
      "Best mean reward: -170.29 - Last mean reward per episode: -172.09 - Last mean moves per episode: 21.50\n",
      "16499992 timesteps\n",
      "Best mean reward: -170.29 - Last mean reward per episode: -172.25 - Last mean moves per episode: 21.50\n",
      "16509999 timesteps\n",
      "Best mean reward: -170.29 - Last mean reward per episode: -171.09 - Last mean moves per episode: 21.48\n",
      "16519998 timesteps\n",
      "Best mean reward: -170.29 - Last mean reward per episode: -169.10 - Last mean moves per episode: 21.43\n",
      "Saving new best model\n",
      "16529990 timesteps\n",
      "Best mean reward: -169.10 - Last mean reward per episode: -168.86 - Last mean moves per episode: 21.43\n",
      "Saving new best model\n",
      "16539998 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -169.41 - Last mean moves per episode: 21.43\n",
      "16549994 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -171.58 - Last mean moves per episode: 21.48\n",
      "16559997 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -173.06 - Last mean moves per episode: 21.50\n",
      "16569976 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -174.27 - Last mean moves per episode: 21.52\n",
      "16579999 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -174.13 - Last mean moves per episode: 21.51\n",
      "16589987 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -172.49 - Last mean moves per episode: 21.45\n",
      "16599976 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -173.58 - Last mean moves per episode: 21.47\n",
      "16609979 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -174.78 - Last mean moves per episode: 21.49\n",
      "16619997 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -172.66 - Last mean moves per episode: 21.43\n",
      "16629986 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -175.10 - Last mean moves per episode: 21.50\n",
      "16639979 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -175.40 - Last mean moves per episode: 21.50\n",
      "16649993 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -175.12 - Last mean moves per episode: 21.50\n",
      "16659989 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -176.41 - Last mean moves per episode: 21.54\n",
      "16669980 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -173.78 - Last mean moves per episode: 21.46\n",
      "16679984 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -174.73 - Last mean moves per episode: 21.49\n",
      "16689985 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -177.60 - Last mean moves per episode: 21.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16699993 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -178.37 - Last mean moves per episode: 21.58\n",
      "16709983 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -177.57 - Last mean moves per episode: 21.54\n",
      "16719987 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -179.02 - Last mean moves per episode: 21.56\n",
      "16729988 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -180.32 - Last mean moves per episode: 21.58\n",
      "16739989 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -182.46 - Last mean moves per episode: 21.65\n",
      "16749999 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -180.95 - Last mean moves per episode: 21.60\n",
      "16759963 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -181.27 - Last mean moves per episode: 21.62\n",
      "16769982 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -179.97 - Last mean moves per episode: 21.58\n",
      "16779999 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -181.07 - Last mean moves per episode: 21.61\n",
      "16789942 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -182.51 - Last mean moves per episode: 21.64\n",
      "16799958 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -181.88 - Last mean moves per episode: 21.63\n",
      "16809989 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -182.65 - Last mean moves per episode: 21.66\n",
      "16820000 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -182.44 - Last mean moves per episode: 21.65\n",
      "16829994 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -185.19 - Last mean moves per episode: 21.72\n",
      "16839994 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -185.28 - Last mean moves per episode: 21.72\n",
      "16849989 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -183.95 - Last mean moves per episode: 21.69\n",
      "16859994 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -183.36 - Last mean moves per episode: 21.67\n",
      "16869992 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -182.12 - Last mean moves per episode: 21.62\n",
      "16879754 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -182.42 - Last mean moves per episode: 21.61\n",
      "16889998 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -187.57 - Last mean moves per episode: 21.74\n",
      "16899987 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -184.90 - Last mean moves per episode: 21.67\n",
      "16909989 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -184.35 - Last mean moves per episode: 21.63\n",
      "16919998 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -184.63 - Last mean moves per episode: 21.64\n",
      "16929983 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -183.71 - Last mean moves per episode: 21.60\n",
      "16939995 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -184.93 - Last mean moves per episode: 21.65\n",
      "16949971 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -181.96 - Last mean moves per episode: 21.56\n",
      "16959996 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -183.66 - Last mean moves per episode: 21.60\n",
      "16970000 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -184.68 - Last mean moves per episode: 21.62\n",
      "16979984 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -179.73 - Last mean moves per episode: 21.48\n",
      "16989065 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -178.85 - Last mean moves per episode: 21.46\n",
      "17000000 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -182.33 - Last mean moves per episode: 21.53\n",
      "17009993 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -180.73 - Last mean moves per episode: 21.46\n",
      "17019984 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -179.65 - Last mean moves per episode: 21.42\n",
      "17029983 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -179.98 - Last mean moves per episode: 21.41\n",
      "17039993 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -177.45 - Last mean moves per episode: 21.34\n",
      "17049982 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -174.96 - Last mean moves per episode: 21.29\n",
      "17059986 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -174.85 - Last mean moves per episode: 21.28\n",
      "17069967 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -174.04 - Last mean moves per episode: 21.24\n",
      "17079998 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -171.87 - Last mean moves per episode: 21.19\n",
      "17089972 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -174.27 - Last mean moves per episode: 21.24\n",
      "17099992 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -170.90 - Last mean moves per episode: 21.14\n",
      "17109999 timesteps\n",
      "Best mean reward: -168.86 - Last mean reward per episode: -167.27 - Last mean moves per episode: 21.03\n",
      "Saving new best model\n",
      "17119976 timesteps\n",
      "Best mean reward: -167.27 - Last mean reward per episode: -166.44 - Last mean moves per episode: 21.03\n",
      "Saving new best model\n",
      "17129986 timesteps\n",
      "Best mean reward: -166.44 - Last mean reward per episode: -165.89 - Last mean moves per episode: 21.02\n",
      "Saving new best model\n",
      "17140000 timesteps\n",
      "Best mean reward: -165.89 - Last mean reward per episode: -164.46 - Last mean moves per episode: 20.99\n",
      "Saving new best model\n",
      "17149988 timesteps\n",
      "Best mean reward: -164.46 - Last mean reward per episode: -166.75 - Last mean moves per episode: 21.04\n",
      "17159994 timesteps\n",
      "Best mean reward: -164.46 - Last mean reward per episode: -169.67 - Last mean moves per episode: 21.11\n",
      "17169989 timesteps\n",
      "Best mean reward: -164.46 - Last mean reward per episode: -165.65 - Last mean moves per episode: 20.98\n",
      "17179997 timesteps\n",
      "Best mean reward: -164.46 - Last mean reward per episode: -163.00 - Last mean moves per episode: 20.89\n",
      "Saving new best model\n",
      "17189993 timesteps\n",
      "Best mean reward: -163.00 - Last mean reward per episode: -163.62 - Last mean moves per episode: 20.92\n",
      "17200000 timesteps\n",
      "Best mean reward: -163.00 - Last mean reward per episode: -155.00 - Last mean moves per episode: 20.71\n",
      "Saving new best model\n",
      "17209997 timesteps\n",
      "Best mean reward: -155.00 - Last mean reward per episode: -156.53 - Last mean moves per episode: 20.76\n",
      "17219972 timesteps\n",
      "Best mean reward: -155.00 - Last mean reward per episode: -154.91 - Last mean moves per episode: 20.73\n",
      "Saving new best model\n",
      "17229985 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -161.06 - Last mean moves per episode: 20.87\n",
      "17239988 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -162.16 - Last mean moves per episode: 20.91\n",
      "17249995 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -163.07 - Last mean moves per episode: 20.93\n",
      "17259980 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -163.55 - Last mean moves per episode: 20.94\n",
      "17269998 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -162.66 - Last mean moves per episode: 20.92\n",
      "17279993 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -162.89 - Last mean moves per episode: 20.90\n",
      "17289967 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -161.98 - Last mean moves per episode: 20.87\n",
      "17299980 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -159.95 - Last mean moves per episode: 20.82\n",
      "17309993 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -160.43 - Last mean moves per episode: 20.81\n",
      "17319984 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -161.54 - Last mean moves per episode: 20.85\n",
      "17329989 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -161.92 - Last mean moves per episode: 20.84\n",
      "17339987 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -162.04 - Last mean moves per episode: 20.85\n",
      "17349995 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -162.62 - Last mean moves per episode: 20.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17359991 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -157.58 - Last mean moves per episode: 20.73\n",
      "17369998 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -157.70 - Last mean moves per episode: 20.74\n",
      "17379989 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -158.21 - Last mean moves per episode: 20.76\n",
      "17389997 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -163.12 - Last mean moves per episode: 20.89\n",
      "17399987 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -162.79 - Last mean moves per episode: 20.89\n",
      "17409996 timesteps\n",
      "Best mean reward: -154.91 - Last mean reward per episode: -163.86 - Last mean moves per episode: 20.92\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-54443aead261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\caleb\\anaconda3\\envs\\deep36\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         )\n",
      "\u001b[1;32mc:\\users\\caleb\\anaconda3\\envs\\deep36\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\caleb\\anaconda3\\envs\\deep36\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m             \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\caleb\\anaconda3\\envs\\deep36\\lib\\site-packages\\stable_baselines3\\common\\policies.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;31m# Evaluate the values for the given observations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_vf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_sde\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlatent_sde\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\caleb\\anaconda3\\envs\\deep36\\lib\\site-packages\\stable_baselines3\\common\\policies.py\u001b[0m in \u001b[0;36m_get_action_dist_from_latent\u001b[1;34m(self, latent_pi, latent_sde)\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCategoricalDistribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m             \u001b[1;31m# Here mean_actions are the logits before the softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiCategoricalDistribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m             \u001b[1;31m# Here mean_actions are the flattened logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\caleb\\anaconda3\\envs\\deep36\\lib\\site-packages\\stable_baselines3\\common\\distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[1;34m(self, action_logits)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mproba_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_logits\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"CategoricalDistribution\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\caleb\\anaconda3\\envs\\deep36\\lib\\site-packages\\torch\\distributions\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`logits` parameter must be at least one-dimensional.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;31m# Normalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dim = 6\n",
    "ships = [3,3]\n",
    "rewards = [-45, 6, 6, -2]\n",
    "lr = 0.00007\n",
    "num_timesteps = 50000000 # this is number of moves and not number of episodes\n",
    "log_dir = \"./gym_full_6_new/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "policy_kwargs = dict(\n",
    "                     activation_fn=th.nn.Tanh, \n",
    "#                     net_arch=[100,120,100]\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "battleship = Battleship(dim, ships, True)\n",
    "\n",
    "env = BattleshipEnv(battleship, rewards)\n",
    "env = Monitor(\n",
    "    env, \n",
    "    filename=log_dir, \n",
    "    allow_early_resets=True\n",
    ")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "best_mean_reward, n_steps, step_interval, episode_interval = -np.inf, 0, 10000, 10000\n",
    "\n",
    "model = A2C(\n",
    "    'MlpPolicy', \n",
    "    env,\n",
    "    learning_rate=lr,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0, \n",
    "    gamma=1\n",
    ").learn(\n",
    "    total_timesteps=num_timesteps, \n",
    "    callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with Three Ships {2,3,3} : 7x7 board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_results(log_dir, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with One Ship on a Bigger Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim = 10\n",
    "ships = [2,3,3,4,5]\n",
    "rewards = [-10, 6, 6, -0.5]\n",
    "lr = 0.00005\n",
    "num_timesteps = 50000000 # this is number of moves and not number of episodes\n",
    "log_dir = \"./gym_full_6/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "policy_kwargs = dict(\n",
    "                     activation_fn=th.nn.Tanh, \n",
    "#                     net_arch=[100,120,100]\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "battleship = Battleship(dim, ships, True)\n",
    "\n",
    "env = BattleshipEnv(battleship, rewards)\n",
    "env = Monitor(\n",
    "    env, \n",
    "    filename=log_dir, \n",
    "    allow_early_resets=True\n",
    ")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "best_mean_reward, n_steps, step_interval, episode_interval = -np.inf, 0, 10000, 10000\n",
    "\n",
    "model = A2C(\n",
    "    'MlpPolicy', \n",
    "    env,\n",
    "    learning_rate=lr,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0, \n",
    "    gamma=1\n",
    ").learn(\n",
    "    total_timesteps=num_timesteps, \n",
    "    callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save manually \n",
    "#model.save(log_dir + 'best_model_cruiser_10x10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(log_dir,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10x10 Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "ships = [2,3,3,4,5]\n",
    "rewards = [-150, 100, 10, -0.75]\n",
    "lr = 0.000125\n",
    "num_timesteps = 500000000 # this is number of moves and not number of episodes\n",
    "log_dir = \"./gym_full_10/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "policy_kwargs = dict(\n",
    "                     activation_fn=th.nn.Tanh, \n",
    "                     net_arch=[200,dict(pi= [300,200,100], vf= [300,200,100])]\n",
    ")\n",
    "\n",
    "\n",
    "battleship = Battleship(dim, ships, True)\n",
    "\n",
    "env = BattleshipEnv(battleship, rewards)\n",
    "env = Monitor(\n",
    "    env, \n",
    "    filename=log_dir, \n",
    "    allow_early_resets=True\n",
    ")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "best_mean_reward, n_steps, step_interval, episode_interval = -np.inf, 0, 10000, 10000\n",
    "\n",
    "model = A2C(\n",
    "    'MlpPolicy', \n",
    "    env,\n",
    "    learning_rate=lr,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0, \n",
    "    gamma=1\n",
    ").learn(\n",
    "    total_timesteps=num_timesteps, \n",
    "    callback=callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing How the Agent Plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_best = A2C.load('./gym/best_model_cruiser_5x5.pkl')\n",
    "#model_best = A2C.load('./gym/best_model_cruiser_6x6.pkl')\n",
    "model_best = A2C.load('./gym/best_model_cruiser_7x7.pkl')\n",
    "#model_best = A2C.load('./gym/best_model_cruiser_10x10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# brew install ffmpeg\n",
    "# brew install gifsicle\n",
    "# Shift + Command + 5 for recording. This saves .mov file\n",
    "# right-click on mov file, get info for video size to use here below\n",
    "# ffmpeg -i in.mov -s 448x790 -pix_fmt rgb24 -r 10 -f gif - | gifsicle --optimize=3 --delay=3 > out.gif\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "ships = {}\n",
    "ships['cruiser'] = 3\n",
    "\n",
    "grid_size=7\n",
    "enemy_board = 0*np.ones((grid_size, grid_size), dtype='int')\n",
    "#enemy_board[3,5] = 1\n",
    "#enemy_board[4,5] = 1\n",
    "#enemy_board[5,5] = 1\n",
    "env = BattleshipEnv(enemy_board=None, ship_locs={}, grid_size=grid_size, ships=ships)\n",
    "# give me time to setup recording\n",
    "time.sleep(5)\n",
    "for ep in range(10):\n",
    "    obs = env.reset()\n",
    "    ## 2 empty boards\n",
    "    done = False\n",
    "    nmoves = 0\n",
    "    print('episode no.', ep, '# moves:', nmoves)\n",
    "    env.render()\n",
    "    env.render()\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)        \n",
    "    while not done:\n",
    "        action, obs = model_best.predict(obs, deterministic=True)\n",
    "        obs, _, done , _ = env.step(action)\n",
    "        nmoves += 1\n",
    "        print('episode no.', ep, '# moves:', nmoves)\n",
    "        env.render()\n",
    "        board_rendering(grid_size, env.enemy_board)\n",
    "        time.sleep(np.random.uniform(1,3))\n",
    "        clear_output(wait=True)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing The Algorithm Parameters with Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To optimize a RL model, see https://github.com/araffin/rl-baselines-zoo/tree/master/hyperparams or\n",
    "## in general https://github.com/araffin/rl-baselines-zoo. This package uses optuna optimization\n",
    "## but it works for the trained agents there. You can modify this package to include your case\n",
    "## or just use the yml file to see what parameters to tune\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import DQN, PPO2, A2C, ACKTR\n",
    "from stable_baselines3.bench import Monitor\n",
    "\n",
    "# Agent hyperparameter optimization\n",
    "def objective(space):\n",
    "    \n",
    "\n",
    "    env_copies = space['env_copies']    \n",
    "    num_timesteps = space['num_timesteps']\n",
    "    gamma = space['gamma']\n",
    "    n_steps = space['n_steps']\n",
    "    vf_coef = space['vf_coef']\n",
    "    ent_coef = space['ent_coef']\n",
    "    max_grad_norm = space['max_grad_norm']\n",
    "    learning_rate = space['learning_rate']\n",
    "    alpha = space['alpha']\n",
    "    epsilon = space['epsilon']\n",
    "    lr_schedule = space['lr_schedule']\n",
    "    \n",
    "    print('space:', space)\n",
    "    \n",
    "    # ships\n",
    "    ships = {}\n",
    "    ships['cruiser'] = 3\n",
    "\n",
    "    grid_size = 7\n",
    "\n",
    "    # Instantiate the env\n",
    "    env = BattleshipEnv(enemy_board=None, ship_locs={}, grid_size=grid_size, ships=ships)\n",
    "\n",
    "    env = DummyVecEnv([lambda: env]*env_copies)\n",
    "      \n",
    "    model = A2C('MlpPolicy', env, verbose=0, \n",
    "                 gamma=gamma,\n",
    "                 n_steps=n_steps,\n",
    "                 ent_coef=ent_coef,\n",
    "                 learning_rate=learning_rate,\n",
    "                 vf_coef=vf_coef,\n",
    "                 max_grad_norm=max_grad_norm,\n",
    "                 alpha=alpha,\n",
    "                 epsilon=epsilon,\n",
    "                 lr_schedule=lr_schedule\n",
    "               ).learn(total_timesteps=num_timesteps)\n",
    "        \n",
    "    rewards_mean = []\n",
    "    moves_mean = []\n",
    "    n_episodes = 100\n",
    "    for ep in range(n_episodes):\n",
    "        reward_env = []\n",
    "        moves_env = []\n",
    "        for env_i in env.envs:\n",
    "            obs = env_i.reset()\n",
    "            done = False\n",
    "            rewards_sum = 0\n",
    "            moves = 0\n",
    "            while not done:\n",
    "                action, obs = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done , _ = env_i.step(action)\n",
    "                rewards_sum += reward # total reward for this episode\n",
    "                moves += 1\n",
    "            reward_env.append(rewards_sum)\n",
    "            moves_env.append(moves)\n",
    "        rewards_mean.append(np.min(reward_env)) # avg environment reward \n",
    "        moves_mean.append(np.mean(moves_env)) # avg environment reward \n",
    "    rewards_mean = np.mean(rewards_mean)\n",
    "    moves_mean = np.mean(moves_mean)\n",
    "\n",
    "    print('reward', rewards_mean, 'moves', moves_mean)\n",
    "    \n",
    "    # hyperopt will minimize objective, number of moves in this case\n",
    "    return{'loss': moves_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "    'env_copies': hp.choice('env_copies', [10]),\n",
    "    'num_timesteps': hp.choice('num_timesteps', [1000000]), #np.arange(1000000, 1000001, 1000000, dtype=int)\n",
    "    'gamma': hp.choice('gamma', [0.99, 0.95, 0.9]),\n",
    "    'n_steps': hp.choice('n_steps', [5, 1, 10]),\n",
    "    'vf_coef': hp.choice('vf_coef', [0.25, 0.1, 0.5]),\n",
    "    'ent_coef': hp.choice('ent_coef', [0.01, 0.1]), \n",
    "    'learning_rate': hp.choice('learning_rate', [0.0007]),\n",
    "    'max_grad_norm': hp.choice('max_grad_norm', [0.5, 0.2, 0.7]), \n",
    "    'alpha': hp.choice('lam', [0.99, 0.95, 0.9]), \n",
    "    'epsilon': hp.choice('epsilon', [1e-5, 1e-3, 1e-4]), \n",
    "    'lr_schedule': hp.choice('lr_schedule', ['constant', 'linear'])\n",
    "}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=30, \n",
    "            trials=trials, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = space_eval(space, best)\n",
    "param_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward scheme\n",
    "\n",
    "For any action: \n",
    "$$r=-1,$$ \n",
    "but if an action is illegal (moving to a non-empty cell), a random action is drawn from the action space. \n",
    "\n",
    "This action is penalized assigning:\n",
    "\n",
    "$$r=-2*S.$$\n",
    "\n",
    "where $S$ is the grid side length.\n",
    "\n",
    "If an action results into a hit:\n",
    "$$\n",
    "r = S.\n",
    "$$\n",
    "If all ship cells are hit (game is completed)\n",
    "$$R = S*S.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skeleton Battleship Environmnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BattleshipEnv(gym.Env):\n",
    "    \n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    \"\"\"see https://github.com/openai/gym/blob/master/gym/core.py\"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human']} \n",
    "\n",
    "    def __init__(self, enemy_board, ship_locs, grid_size, ships):\n",
    "        \n",
    "        super(BattleshipEnv, self).__init__()\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # In our case the action space is discrete: index of action\n",
    "        self.action_space = spaces.Discrete(self.grid_size * self.grid_size)\n",
    "        # The observation will be the state or configuration of the board\n",
    "        self.observation_space = spaces.Box(low=-1, high=1,shape=(self.grid_size, self.grid_size), \n",
    "                                            dtype=np.int)\n",
    "        \n",
    "        pass\n",
    "            \n",
    "    # an action will be an index of action_space either from epsilon-greedy\n",
    "    # or from model prediction\n",
    "    def step(self, action):\n",
    "            \n",
    "        \"\"\"\n",
    "        Rewards for action and sets next state\n",
    "        Also, checks if game is completed (done)\n",
    "        :return: next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment to an initial state\n",
    "        :return: (np.array) state\n",
    "        \"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Human readable state. In this case the scoring board\n",
    "        \"\"\"\n",
    "        \n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
